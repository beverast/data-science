{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install BigQuery\n",
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/ec2-user/SageMaker/CommitAnalysis/LamdaLabsProject-03b6fddae3e7.json\"\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n",
    "table_ref = dataset_ref.table(\"commits\")\n",
    "table = client.get_table(table_ref)  # API call\n",
    "\n",
    "# Load 50 rows from a table\n",
    "rows = client.list_rows(table)\n",
    "50 == table.num_rows\n",
    "\n",
    "# Load the first 10 rows\n",
    "rows = client.list_rows(table, max_results=10)\n",
    "assert len(list(rows)) == 10\n",
    "\n",
    "# Use the start index to load an arbitrary portion of the table\n",
    "rows = client.list_rows(table, start_index=10, max_results=10)\n",
    "\n",
    "#Save query as DataFrame\n",
    "commits_df = rows.to_dataframe()\n",
    "commits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Let boto know you want to connect to S3\n",
    "s3 = boto3.Session().resource('s3')\n",
    "# S3 bucket name\n",
    "bucket = 'github-analysis-test' \n",
    "# filepath in s3 bucket\n",
    "key = 'DataSets/test.csv' \n",
    "# download dataframe as csv locally \n",
    "doc = commits_df.to_csv('test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to s3 bucket \n",
    "# Must be a string\n",
    "s3.Bucket(bucket).Object(key).upload_file('test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
