{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate-Limited Querying of Github's GraphQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"/home/joseph/graphql_token.txt\", \"r\")\n",
    "api_token = file.read().strip()\n",
    "\n",
    "url = \"https://api.github.com/graphql\"\n",
    "headers = {\"Authorization\": \"token %s\" % api_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(json: str) -> dict:\n",
    "    r = requests.post(url=url, json=json, headers=headers)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Query\n",
    "\n",
    "The data is returned in a JSON structure, as a `str` type object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'viewer': {'repositories': {'totalCount': 38,\n",
       "    'pageInfo': {'hasNextPage': True, 'endCursor': 'Y3Vyc29yOnYyOpHOAvjC2Q=='},\n",
       "    'edges': [{'node': {'name': 'Yendors-Analysis'}}]}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json = {\n",
    "    \"query\": \"{ viewer { repositories(first: 1) { totalCount pageInfo { hasNextPage endCursor } edges { node { name } } } } }\"\n",
    "}\n",
    "\n",
    "query(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query returns the remaining number of nodes queries, as well as the when the limit will be reset. This data can be passed into a pandas `DataFrame` using `pd.read_json()`, and from there the remaining limit and reset time can be parsed to allow for rate-limited programmatic scraping of Github's GraphQL API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'viewer': {'login': 'beverast'},\n",
       "  'rateLimit': {'limit': 5000,\n",
       "   'cost': 1,\n",
       "   'remaining': 4998,\n",
       "   'resetAt': '2019-08-28T17:54:18Z'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = \"\"\"query {\n",
    "  viewer {\n",
    "    login\n",
    "  }\n",
    "  rateLimit {\n",
    "    limit\n",
    "    cost\n",
    "    remaining\n",
    "    resetAt\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "json = {\"query\": query_text}\n",
    "query(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest GraphQL Responses Into a DataFrame\n",
    "**1. Query the endpoint, ingest as a DataFrame from JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_df = pd.DataFrame(query(json))\n",
    "limit_df = limit_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rateLimit</td>\n",
       "      <td>{'limit': 5000, 'cost': 1, 'remaining': 4997, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viewer</td>\n",
       "      <td>{'login': 'beverast'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               data\n",
       "0  rateLimit  {'limit': 5000, 'cost': 1, 'remaining': 4997, ...\n",
       "1     viewer                              {'login': 'beverast'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create columns for the necessary data: `remaining` and `resetAt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_df[\"remaining\"] = limit_df.iloc[0][1]['remaining']\n",
    "limit_df[\"resetAt\"] = pd.Timestamp(limit_df.iloc[0][1]['resetAt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Drop unnecessary `viewer` data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_df = limit_df.drop(axis=1, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data</th>\n",
       "      <th>remaining</th>\n",
       "      <th>resetAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rateLimit</td>\n",
       "      <td>{'limit': 5000, 'cost': 1, 'remaining': 4997, ...</td>\n",
       "      <td>4997</td>\n",
       "      <td>2019-08-28 17:54:18+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               data  remaining  \\\n",
       "0  rateLimit  {'limit': 5000, 'cost': 1, 'remaining': 4997, ...       4997   \n",
       "\n",
       "                    resetAt  \n",
       "0 2019-08-28 17:54:18+00:00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_limit() -> pd.DataFrame:\n",
    "    \"\"\" Returns a DataFrame of current GraphQL API query limits.\"\"\"\n",
    "    query_text = \"\"\"query {\n",
    "                      viewer {\n",
    "                        login\n",
    "                      }\n",
    "                      rateLimit {\n",
    "                        limit\n",
    "                        cost\n",
    "                        remaining\n",
    "                        resetAt\n",
    "                      }\n",
    "                    }\"\"\"\n",
    "\n",
    "    json = {\"query\": query_text}\n",
    "    query(json)\n",
    "    df = pd.DataFrame(query(json)).reset_index()\n",
    "    \n",
    "    df[\"remaining\"] = df.iloc[0][1]['remaining']\n",
    "    df[\"resetAt\"] = pd.Timestamp(df.iloc[0][1]['resetAt'])\n",
    "    df = df.drop(axis=1, index=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_remaining(df: pd.DataFrame) -> int:\n",
    "    \"\"\"Returns the remaining count of node queries.\"\"\"\n",
    "    return df['remaining'][0].astype(int)\n",
    "\n",
    "\n",
    "def get_resetAt(df: pd.DataFrame) -> pd.Timestamp:\n",
    "    \"\"\"Retruns the time (UTC) at which the remaining queries will be reset to 5000.\"\"\"\n",
    "    return pd.Timestamp(df['resetAt'][0])\n",
    "\n",
    "\n",
    "def is_resetAt_reached(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Returns True or False if the resetAt time has been reached.\"\"\"    \n",
    "    resetAt = get_resetAt(df)\n",
    "    return pd.Timestamp.now(tz='UTC') > resetAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4995, Timestamp('2019-08-28 17:54:18+0000', tz='UTC'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df = update_limit()\n",
    "remaining = get_remaining(limit_df)\n",
    "resetAt = get_resetAt(limit_df)\n",
    "\n",
    "remaining, resetAt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following two code cells are from [this notebook](https://github.com/labs15-github-commit/data-science/blob/patrick/toDataFrameFunction.ipynb) by Patrick Wilky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_df(x):\n",
    "    \"\"\"\n",
    "    This function will take the raw query results and turn it into a pretty dataframe.\n",
    "    \"\"\"\n",
    "    data = x.get('data').get('search').get('nodes')\n",
    "    \n",
    "    copy = pd.DataFrame(data)\n",
    "    \n",
    "    templist = []\n",
    "    templist2 = []\n",
    "    \n",
    "    for i in copy.comments:\n",
    "        templist.append(i.get('totalCount'))\n",
    "    copy['commentCount'] = templist\n",
    "    \n",
    "    templist.clear()\n",
    "    for i in copy.comments:\n",
    "        templist.append(i.get('edges'))\n",
    "    copy['comments'] = templist\n",
    "    \n",
    "    templist.clear()\n",
    "    for i in range(len(copy.comments)):\n",
    "        templist2 = []\n",
    "        if (copy.commentCount[i]==0):\n",
    "            templist.append(templist2)\n",
    "        else:\n",
    "            for o in copy.comments[i]:\n",
    "                templist2.append(o.get('node'))\n",
    "                if (len(templist2)==copy.commentCount[i]):\n",
    "                    templist.append(templist2)                   \n",
    "    copy['comments'] = templist\n",
    "    \n",
    "    templist.clear()\n",
    "    for i in copy.author:\n",
    "        templist.append(i.get('company'))\n",
    "    copy['company'] = templist \n",
    "    \n",
    "    templist.clear()\n",
    "    for i in copy.author:\n",
    "        templist.append(i.get('login'))\n",
    "    copy['author'] = templist   \n",
    "    \n",
    "    templist.clear()\n",
    "    for i in copy.files:\n",
    "        templist.append(i.get('totalCount'))\n",
    "    copy['filesCommited'] = templist\n",
    "    \n",
    "    \n",
    "    templist.clear()\n",
    "    for i in copy.mergedBy:\n",
    "        if (i == None):\n",
    "            templist.append(None)\n",
    "        else:\n",
    "            templist.append(i.get('login'))\n",
    "    copy['mergedBy'] = templist\n",
    "    \n",
    "    copy = copy.drop(columns='files')\n",
    "    \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_request_query = \"\"\"\n",
    "{\n",
    "  search(first:100, query:\"repo:pandas-dev/pandas created:2018-08-01..2018-08-12 type:pr\", type:ISSUE) {\n",
    "    nodes {\n",
    "      ... on PullRequest {\n",
    "        createdAt\n",
    "        updatedAt\n",
    "        title\n",
    "        mergedBy {\n",
    "          login\n",
    "        }\n",
    "        authorAssociation\n",
    "        author {\n",
    "          login\n",
    "          ... on User {\n",
    "            company\n",
    "          }\n",
    "        }\n",
    "        files {\n",
    "          totalCount\n",
    "        }\n",
    "        state\n",
    "        resourcePath\n",
    "        bodyText\n",
    "        comments(first: 50) {\n",
    "          totalCount\n",
    "          edges {\n",
    "            node {\n",
    "              authorAssociation\n",
    "              author{\n",
    "                login\n",
    "              }\n",
    "              bodyText\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"query\": pull_request_query}\n",
    "response = query(json)\n",
    "pull_request_df = better_df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>authorAssociation</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>comments</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>mergedBy</th>\n",
       "      <th>resourcePath</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>company</th>\n",
       "      <th>filesCommited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jseabold</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>2 issues for DataFrame.to_csv\\n1 ) If header a...</td>\n",
       "      <td>[{'authorAssociation': 'MEMBER', 'author': {'l...</td>\n",
       "      <td>2011-09-17T17:56:07Z</td>\n",
       "      <td>None</td>\n",
       "      <td>/pandas-dev/pandas/pull/151</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>ENH: improve DataFrame read_csv / to_csv for I...</td>\n",
       "      <td>2014-06-19T05:29:22Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Civis Analytics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLnick</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Hi Wes\\nFirstly, congrats on such an amazing p...</td>\n",
       "      <td>[{'authorAssociation': 'MEMBER', 'author': {'l...</td>\n",
       "      <td>2011-09-16T09:32:28Z</td>\n",
       "      <td>None</td>\n",
       "      <td>/pandas-dev/pandas/pull/146</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Minor change to csv reading</td>\n",
       "      <td>2014-06-16T01:22:51Z</td>\n",
       "      <td>1</td>\n",
       "      <td>IBM @CODAIT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jseabold</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>More low-hanging fruit. Anywhere the indices h...</td>\n",
       "      <td>[{'authorAssociation': 'MEMBER', 'author': {'l...</td>\n",
       "      <td>2011-09-14T23:59:36Z</td>\n",
       "      <td>None</td>\n",
       "      <td>/pandas-dev/pandas/pull/142</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>ENH: Allow unstacking by level name</td>\n",
       "      <td>2014-06-14T02:28:14Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Civis Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jseabold</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>Trying to implement the easy items on my wishl...</td>\n",
       "      <td>[{'authorAssociation': 'MEMBER', 'author': {'l...</td>\n",
       "      <td>2011-09-14T23:17:01Z</td>\n",
       "      <td>None</td>\n",
       "      <td>/pandas-dev/pandas/pull/141</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>ENH: Allow to sort on index level by name</td>\n",
       "      <td>2014-06-16T22:03:18Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Civis Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jseabold</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>The convenience function I've been using. Don'...</td>\n",
       "      <td>[{'authorAssociation': 'MEMBER', 'author': {'l...</td>\n",
       "      <td>2011-09-14T21:43:18Z</td>\n",
       "      <td>None</td>\n",
       "      <td>/pandas-dev/pandas/pull/140</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>ENH: Add panel_index convenience function</td>\n",
       "      <td>2014-08-02T09:06:16Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Civis Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author authorAssociation  \\\n",
       "0  jseabold       CONTRIBUTOR   \n",
       "1    MLnick              NONE   \n",
       "2  jseabold       CONTRIBUTOR   \n",
       "3  jseabold       CONTRIBUTOR   \n",
       "4  jseabold       CONTRIBUTOR   \n",
       "\n",
       "                                            bodyText  \\\n",
       "0  2 issues for DataFrame.to_csv\\n1 ) If header a...   \n",
       "1  Hi Wes\\nFirstly, congrats on such an amazing p...   \n",
       "2  More low-hanging fruit. Anywhere the indices h...   \n",
       "3  Trying to implement the easy items on my wishl...   \n",
       "4  The convenience function I've been using. Don'...   \n",
       "\n",
       "                                            comments             createdAt  \\\n",
       "0  [{'authorAssociation': 'MEMBER', 'author': {'l...  2011-09-17T17:56:07Z   \n",
       "1  [{'authorAssociation': 'MEMBER', 'author': {'l...  2011-09-16T09:32:28Z   \n",
       "2  [{'authorAssociation': 'MEMBER', 'author': {'l...  2011-09-14T23:59:36Z   \n",
       "3  [{'authorAssociation': 'MEMBER', 'author': {'l...  2011-09-14T23:17:01Z   \n",
       "4  [{'authorAssociation': 'MEMBER', 'author': {'l...  2011-09-14T21:43:18Z   \n",
       "\n",
       "  mergedBy                 resourcePath   state  \\\n",
       "0     None  /pandas-dev/pandas/pull/151  CLOSED   \n",
       "1     None  /pandas-dev/pandas/pull/146  CLOSED   \n",
       "2     None  /pandas-dev/pandas/pull/142  CLOSED   \n",
       "3     None  /pandas-dev/pandas/pull/141  CLOSED   \n",
       "4     None  /pandas-dev/pandas/pull/140  CLOSED   \n",
       "\n",
       "                                               title             updatedAt  \\\n",
       "0  ENH: improve DataFrame read_csv / to_csv for I...  2014-06-19T05:29:22Z   \n",
       "1                        Minor change to csv reading  2014-06-16T01:22:51Z   \n",
       "2                ENH: Allow unstacking by level name  2014-06-14T02:28:14Z   \n",
       "3          ENH: Allow to sort on index level by name  2014-06-16T22:03:18Z   \n",
       "4          ENH: Add panel_index convenience function  2014-08-02T09:06:16Z   \n",
       "\n",
       "   commentCount          company  filesCommited  \n",
       "0             1  Civis Analytics              3  \n",
       "1             1     IBM @CODAIT               1  \n",
       "2             1  Civis Analytics              1  \n",
       "3             1  Civis Analytics              1  \n",
       "4             1  Civis Analytics              1  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull_request_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create GraphQL query to get a repo's `createdAt` and `updatedAt` info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Dustin Yang's graphql.ipynb\n",
    "repo_query = \"\"\"\n",
    "{\n",
    "  repositoryOwner(login: \"pandas-dev\") {\n",
    "    id\n",
    "    login\n",
    "    repository(name: \"pandas\") {\n",
    "      id\n",
    "      name\n",
    "      createdAt\n",
    "      updatedAt\n",
    "      description\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"query\": repo_query}\n",
    "query_results = query(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Ingest as a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk4NTgxMjc=</td>\n",
       "      <td>pandas</td>\n",
       "      <td>2010-08-24T01:37:33Z</td>\n",
       "      <td>2019-08-28T16:52:02Z</td>\n",
       "      <td>Flexible and powerful data analysis / manipula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id    name             createdAt  \\\n",
       "0  MDEwOlJlcG9zaXRvcnk4NTgxMjc=  pandas  2010-08-24T01:37:33Z   \n",
       "\n",
       "              updatedAt                                        description  \n",
       "0  2019-08-28T16:52:02Z  Flexible and powerful data analysis / manipula...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dict = query_results['data']['repositoryOwner']['repository']\n",
    "repo_df = pd.DataFrame.from_dict(query_dict, orient='index').T\n",
    "repo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Convert to `pd.Timestamp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 5 columns):\n",
      "id             1 non-null object\n",
      "name           1 non-null object\n",
      "createdAt      1 non-null datetime64[ns, UTC]\n",
      "updatedAt      1 non-null datetime64[ns, UTC]\n",
      "description    1 non-null object\n",
      "dtypes: datetime64[ns, UTC](2), object(3)\n",
      "memory usage: 120.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "repo_df['createdAt'] = pd.Timestamp(repo_df['createdAt'][0])\n",
    "repo_df['updatedAt'] = pd.Timestamp(repo_df['updatedAt'][0])\n",
    "\n",
    "repo_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate date intervals for querying**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('3291 days 15:14:29')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_df['updatedAt'][0] - repo_df['createdAt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2010-08-31 01:37:33+00:00', '2010-09-30 01:37:33+00:00',\n",
       "               '2010-10-31 01:37:33+00:00', '2010-11-30 01:37:33+00:00',\n",
       "               '2010-12-31 01:37:33+00:00', '2011-01-31 01:37:33+00:00',\n",
       "               '2011-02-28 01:37:33+00:00', '2011-03-31 01:37:33+00:00',\n",
       "               '2011-04-30 01:37:33+00:00', '2011-05-31 01:37:33+00:00',\n",
       "               ...\n",
       "               '2018-10-31 01:37:33+00:00', '2018-11-30 01:37:33+00:00',\n",
       "               '2018-12-31 01:37:33+00:00', '2019-01-31 01:37:33+00:00',\n",
       "               '2019-02-28 01:37:33+00:00', '2019-03-31 01:37:33+00:00',\n",
       "               '2019-04-30 01:37:33+00:00', '2019-05-31 01:37:33+00:00',\n",
       "               '2019-06-30 01:37:33+00:00', '2019-07-31 01:37:33+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', length=108, freq='M')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_months = pd.date_range(start=repo_df['createdAt'][0], end=repo_df['updatedAt'][0], freq='M', tz='UTC')\n",
    "repo_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Insert these `DatetimeIndex` values into queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "pr_search_token = Template(\"\"\"\n",
    "{\n",
    "  search(first:100, query:\"repo:pandas-dev/pandas created:$date1..$date2 type:pr\", type:ISSUE) {\"\"\")\n",
    "\n",
    "pr_query_wo_search = \"\"\"\n",
    "    nodes {\n",
    "      ... on PullRequest {\n",
    "        createdAt\n",
    "        updatedAt\n",
    "        title\n",
    "        mergedBy {\n",
    "          login\n",
    "        }\n",
    "        authorAssociation\n",
    "        author {\n",
    "          login\n",
    "          ... on User {\n",
    "            company\n",
    "          }\n",
    "        }\n",
    "        files {\n",
    "          totalCount\n",
    "        }\n",
    "        state\n",
    "        resourcePath\n",
    "        bodyText\n",
    "        comments(first: 50) {\n",
    "          totalCount\n",
    "          edges {\n",
    "            node {\n",
    "              authorAssociation\n",
    "              author{\n",
    "                login\n",
    "              }\n",
    "              bodyText\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2010-08-31', '2010-09-30')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(repo_months[0].date()), str(repo_months[1].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2010-08-31 01:37:33+0000', tz='UTC', freq='M'),\n",
       "  Timestamp('2010-09-30 01:37:33+0000', tz='UTC', freq='M')),\n",
       " (Timestamp('2010-09-30 01:37:33+0000', tz='UTC', freq='M'),\n",
       "  Timestamp('2010-10-31 01:37:33+0000', tz='UTC', freq='M')),\n",
       " (Timestamp('2010-10-31 01:37:33+0000', tz='UTC', freq='M'),\n",
       "  Timestamp('2010-11-30 01:37:33+0000', tz='UTC', freq='M')),\n",
       " (Timestamp('2010-11-30 01:37:33+0000', tz='UTC', freq='M'),\n",
       "  Timestamp('2010-12-31 01:37:33+0000', tz='UTC', freq='M')),\n",
       " (Timestamp('2010-12-31 01:37:33+0000', tz='UTC', freq='M'),\n",
       "  Timestamp('2011-01-31 01:37:33+0000', tz='UTC', freq='M'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_pairs = list(zip(repo_months,repo_months[1:]))\n",
    "month_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2010-08-31..2010-09-30 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2010-09-30..2010-10-31 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2010-10-31..2010-11-30 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2010-11-30..2010-12-31 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2010-12-31..2011-01-31 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2011-01-31..2011-02-28 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2011-02-28..2011-03-31 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2011-03-31..2011-04-30 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2011-04-30..2011-05-31 type:pr\", type:ISSUE) {',\n",
       " '\\n{\\n  search(first:100, query:\"repo:pandas-dev/pandas created:2011-05-31..2011-06-30 type:pr\", type:ISSUE) {']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_query_list = []\n",
    "for pair in month_pairs:\n",
    "    date1 = str(pair[0].date())\n",
    "    date2 = str(pair[1].date())\n",
    "    pr_query = pr_search_token.substitute(date1=date1, date2=date2)\n",
    "    pr_query_list.append(pr_query)\n",
    "\n",
    "pr_query_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  search(first:100, query:\"repo:pandas-dev/pandas created:2018-11-30..2018-12-31 type:pr\", type:ISSUE) {\n",
      "    nodes {\n",
      "      ... on PullRequest {\n",
      "        createdAt\n",
      "        updatedAt\n",
      "        title\n",
      "        mergedBy {\n",
      "          login\n",
      "        }\n",
      "        authorAssociation\n",
      "        author {\n",
      "          login\n",
      "          ... on User {\n",
      "            company\n",
      "          }\n",
      "        }\n",
      "        files {\n",
      "          totalCount\n",
      "        }\n",
      "        state\n",
      "        resourcePath\n",
      "        bodyText\n",
      "        comments(first: 50) {\n",
      "          totalCount\n",
      "          edges {\n",
      "            node {\n",
      "              authorAssociation\n",
      "              author{\n",
      "                login\n",
      "              }\n",
      "              bodyText\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = pr_query_list[99]+pr_query_wo_search\n",
    "print(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"query\": test_query}\n",
    "response = query(json)\n",
    "pr_test_query = better_df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>authorAssociation</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>comments</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>mergedBy</th>\n",
       "      <th>resourcePath</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>company</th>\n",
       "      <th>filesCommited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TomAugspurger</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td></td>\n",
       "      <td>[{'authorAssociation': 'NONE', 'author': {'log...</td>\n",
       "      <td>2018-12-31T21:06:34Z</td>\n",
       "      <td>jreback</td>\n",
       "      <td>/pandas-dev/pandas/pull/24524</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>Fixed PeriodArray._time_shift positional argument</td>\n",
       "      <td>2019-01-02T20:18:03Z</td>\n",
       "      <td>3</td>\n",
       "      <td>@ContinuumIO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datapythonista</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>xref #22743\\nWe already had an option --warnin...</td>\n",
       "      <td>[{'authorAssociation': 'NONE', 'author': {'log...</td>\n",
       "      <td>2018-12-31T20:49:21Z</td>\n",
       "      <td>jreback</td>\n",
       "      <td>/pandas-dev/pandas/pull/24523</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>DOC: Make sphinx fail the build when --warning...</td>\n",
       "      <td>2018-12-31T23:17:51Z</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simonjayhawkins</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>xref #22715 (comment)</td>\n",
       "      <td>[{'authorAssociation': 'NONE', 'author': {'log...</td>\n",
       "      <td>2018-12-31T20:47:18Z</td>\n",
       "      <td>jreback</td>\n",
       "      <td>/pandas-dev/pandas/pull/24522</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>REF/TST: use monkeypatch in mock clipboard fix...</td>\n",
       "      <td>2019-01-01T21:01:59Z</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reidy-p</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>… tests\\n\\n Progress towards #24471\\n tests ad...</td>\n",
       "      <td>[{'authorAssociation': 'NONE', 'author': {'log...</td>\n",
       "      <td>2018-12-31T20:09:54Z</td>\n",
       "      <td>jreback</td>\n",
       "      <td>/pandas-dev/pandas/pull/24521</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>ENH: Add sort parameter to set operations for ...</td>\n",
       "      <td>2019-01-27T01:19:41Z</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gfyoung</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>Removes the DeprecationWarning raised by dateu...</td>\n",
       "      <td>[{'authorAssociation': 'NONE', 'author': {'log...</td>\n",
       "      <td>2018-12-31T19:41:22Z</td>\n",
       "      <td>jreback</td>\n",
       "      <td>/pandas-dev/pandas/pull/24520</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>MAINT: Port _timelex in codebase</td>\n",
       "      <td>2019-01-01T18:41:52Z</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author authorAssociation  \\\n",
       "0    TomAugspurger       CONTRIBUTOR   \n",
       "1   datapythonista            MEMBER   \n",
       "2  simonjayhawkins            MEMBER   \n",
       "3          reidy-p       CONTRIBUTOR   \n",
       "4          gfyoung            MEMBER   \n",
       "\n",
       "                                            bodyText  \\\n",
       "0                                                      \n",
       "1  xref #22743\\nWe already had an option --warnin...   \n",
       "2                              xref #22715 (comment)   \n",
       "3  … tests\\n\\n Progress towards #24471\\n tests ad...   \n",
       "4  Removes the DeprecationWarning raised by dateu...   \n",
       "\n",
       "                                            comments             createdAt  \\\n",
       "0  [{'authorAssociation': 'NONE', 'author': {'log...  2018-12-31T21:06:34Z   \n",
       "1  [{'authorAssociation': 'NONE', 'author': {'log...  2018-12-31T20:49:21Z   \n",
       "2  [{'authorAssociation': 'NONE', 'author': {'log...  2018-12-31T20:47:18Z   \n",
       "3  [{'authorAssociation': 'NONE', 'author': {'log...  2018-12-31T20:09:54Z   \n",
       "4  [{'authorAssociation': 'NONE', 'author': {'log...  2018-12-31T19:41:22Z   \n",
       "\n",
       "  mergedBy                   resourcePath   state  \\\n",
       "0  jreback  /pandas-dev/pandas/pull/24524  MERGED   \n",
       "1  jreback  /pandas-dev/pandas/pull/24523  MERGED   \n",
       "2  jreback  /pandas-dev/pandas/pull/24522  MERGED   \n",
       "3  jreback  /pandas-dev/pandas/pull/24521  MERGED   \n",
       "4  jreback  /pandas-dev/pandas/pull/24520  MERGED   \n",
       "\n",
       "                                               title             updatedAt  \\\n",
       "0  Fixed PeriodArray._time_shift positional argument  2019-01-02T20:18:03Z   \n",
       "1  DOC: Make sphinx fail the build when --warning...  2018-12-31T23:17:51Z   \n",
       "2  REF/TST: use monkeypatch in mock clipboard fix...  2019-01-01T21:01:59Z   \n",
       "3  ENH: Add sort parameter to set operations for ...  2019-01-27T01:19:41Z   \n",
       "4                   MAINT: Port _timelex in codebase  2019-01-01T18:41:52Z   \n",
       "\n",
       "   commentCount       company  filesCommited  \n",
       "0             3  @ContinuumIO              1  \n",
       "1             3          None              2  \n",
       "2             3          None              1  \n",
       "3            11          None             15  \n",
       "4             5          None              4  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_test_query.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalize, Write as a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_query(repository_owner: str, repository_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a repository_owner and a repository name, return a DataFrame of pull request data\n",
    "    for the entire history of the repo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get API query limits\n",
    "    limit_df = update_limit()\n",
    "    remaining = get_remaining(limit_df)\n",
    "    resetAt = get_resetAt(limit_df)\n",
    "\n",
    "    # Query template for getting repo info\n",
    "    repo_query = Template(\"\"\"\n",
    "    {\n",
    "      repositoryOwner(login: \"$repo_owner\") {\n",
    "        id\n",
    "        login\n",
    "        repository(name: \"$repo_name\") {\n",
    "          id\n",
    "          name\n",
    "          createdAt\n",
    "          updatedAt\n",
    "          description\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    # Customize template for given arguments\n",
    "    repo_query = repo_query.substitute(repo_owner=repository_owner, repo_name=repository_name)\n",
    "\n",
    "    # Perform the GraphQL query\n",
    "    json = {\"query\": repo_query}\n",
    "    query_results = query(json)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    query_dict = query_results['data']['repositoryOwner']['repository']\n",
    "    repo_df = pd.DataFrame.from_dict(query_dict, orient='index').T\n",
    "    \n",
    "    # Type conversion\n",
    "    repo_df['createdAt'] = pd.Timestamp(repo_df['createdAt'][0])\n",
    "    repo_df['updatedAt'] = pd.Timestamp(repo_df['updatedAt'][0])\n",
    "\n",
    "    # Get dates for generated queries\n",
    "    repo_months = pd.date_range(start=repo_df['createdAt'][0], end=repo_df['updatedAt'][0], freq='M', tz='UTC')\n",
    "    month_pairs = list(zip(repo_months,repo_months[1:]))\n",
    "\n",
    "    # Generate list of first half of formatted queries\n",
    "    pr_query_list = []\n",
    "    for pair in month_pairs[60:]:\n",
    "        date1 = str(pair[0].date())\n",
    "        date2 = str(pair[1].date())\n",
    "        pr_query = pr_search_token.substitute(date1=date1, date2=date2)\n",
    "        pr_query_list.append(pr_query)\n",
    "\n",
    "    # Generate DataFrame for all queries\n",
    "    total_repo_issues_df = pd.DataFrame()\n",
    "    \n",
    "    for search_token in pr_query_list:\n",
    "        concat_query = search_token+pr_query_wo_search\n",
    "        print(concat_query)    # DEBUG STATEMENT\n",
    "        json = {\"query\": concat_query}\n",
    "        query_response = query(json)\n",
    "        print(query_response)    # DEBUG STATEMENT\n",
    "        pr_query = better_df(query_response)\n",
    "        \n",
    "        # Check query limit\n",
    "        limit_df = update_limit()\n",
    "        remaining = get_remaining(limit_df)\n",
    "        resetAt = get_resetAt(limit_df)\n",
    "        \n",
    "        print(total_repo_issues_df.tail())    # DEBUG STATEMENT\n",
    "        \n",
    "        if remaining > 20:\n",
    "            print('Query limit remaining: ', remaining)\n",
    "            total_repo_issues_df = pd.concat([total_repo_issues_df, pr_query])\n",
    "            continue\n",
    "        else:\n",
    "            print('Query limit reached. Query limit resets at: ', resetAt)\n",
    "            # Make loop wait until Query limit is reset: time.sleep(secs)\n",
    "            while resetAt.time() > datetime.datetime.utcnow().time():\n",
    "                time.wait(600)\n",
    "                limit_df = update_limit()\n",
    "                resetAt = get_resetAt(limit_df)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return total_repo_issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  search(first:100, query:\"repo:pandas-dev/pandas created:2015-08-31..2015-09-30 type:pr\", type:ISSUE) {\n",
      "    nodes {\n",
      "      ... on PullRequest {\n",
      "        createdAt\n",
      "        updatedAt\n",
      "        title\n",
      "        mergedBy {\n",
      "          login\n",
      "        }\n",
      "        authorAssociation\n",
      "        author {\n",
      "          login\n",
      "          ... on User {\n",
      "            company\n",
      "          }\n",
      "        }\n",
      "        files {\n",
      "          totalCount\n",
      "        }\n",
      "        state\n",
      "        resourcePath\n",
      "        bodyText\n",
      "        comments(first: 50) {\n",
      "          totalCount\n",
      "          edges {\n",
      "            node {\n",
      "              authorAssociation\n",
      "              author{\n",
      "                login\n",
      "              }\n",
      "              bodyText\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "{'data': {'search': {'nodes': [{'createdAt': '2015-09-30T23:23:51Z', 'updatedAt': '2015-11-20T14:51:30Z', 'title': 'BUG: GH11206 where pd.isnull did not consider numpy NaT null', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen', 'company': None}, 'files': {'totalCount': 8}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11212', 'bodyText': 'closes #11206', 'comments': {'totalCount': 21, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm.\\ncan you run a perf check on the null benchmarks. just confirm nothing has substantially changed.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'Updated. Timing for frame_methods.frame_isnull.time_frame_isnull (where all data is non-nan floats) increased by less than 5%.\\nAdded a benchmark where lots of the data is various types of null.\\nBENCHMARK                                    BEFORE      AFTER     FACTOR\\n...ame_isnull_has_null.time_frame_isnull    38.18ms   138.88ms   3.63779915x\\n\\nI also changed the code slightly so that the logic is further short-circuited short-circuited.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'the benchmark u are showing is 3x increase?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"Yes.That's the with lots of nulls. I will dig a little to see if that can be avoided.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"these routines are hit everywhere, so i am for special casing certain things. IOW, that is why isnullobj exists. But it may make sense to have 2 versions of _checknull_with_nat, because they are called on different array kinds. E.g if you have an int array then it obviously doesn't need to check for None,NaN,NaT at all. most of these cases are already handled, but IIRC, this is hotspotted. So just want to try to nail that down.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'Updated.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'udpated'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"ok. I'm traveling and will get to this next week.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@kawochen can you update (move whatsnew to 0.17.1)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'Will do (still on vacation)\\n\\nOn Oct 9, 2015, at 8:35 AM, Jeff Reback notifications@github.com wrote:\\n@kawochen can you update (move whatsnew to 0.17.1)\\n—\\nReply to this email directly or view it on GitHub.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'updated'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can u rebase and run a perf check'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'Saw some horrifying perf hit (30X).  I will look at the compiled code later.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok gr8!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"so using timeit and this code\\nfrom pandas import *\\nimport pandas as pd\\nimport numpy as np\\n\\nnp.random.seed(1234)\\nsample = np.array([NaT, np.nan, None, np.datetime64('NaT'),\\n    np.timedelta64('NaT'), 0, 1, 2.0, '', 'abcd'])\\ndata = np.random.choice(sample, (1000, 1000))\\ndf = DataFrame(data)\\n\\n%timeit isnull(df)\\n\\nthe perf hit is 4X.  But if I that put into an asv benchmark, it shows 24X. I seem to be having problems with asv lately -- not sure what's wrong.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'are you using profile=True in the cython modules?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'pv'}, 'bodyText': 'The perf hit is different for numpy 1.10 vs 1.9 (and asv probably uses the newest version)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@kawochen pushing, but if you finish in the next few days, pls ping'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': '@pv thanks! 😄\\n@jreback\\nbranch\\n[ 32.58%] ··· Running ...s.frame_isnull_obj.time_frame_isnull    46.07ms\\nmaster\\n[ 82.58%] ··· Running ...s.frame_isnull_obj.time_frame_isnull    38.91ms\\n`'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"Replacing wouldn't work in all cases, because sometimes you don't want timedelta64 to be accepted, e.g. datetime_to_datetime64(), so it would become some sort of manual inlining for _checknull_with_nat to deal with those cases (which might not be a bad thing, depending on how often this is needed).\\nBut I see that tslib.pyx needs some cleaning, as code like if val is np_NaT or val.view('i8') == iNaT is everywhere (np_NaT is not a singleton, so the first half of the expression is almost certainly False, and the val.view('i8') is slower than get_datetime64_val (especially given the regression in numpy 1.10, but that will be fixed soon).\\nSo maybe when others or I clean up this module, we can see how to organize or deal with all the lower level scalar check null functions\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@kawochen thanks!\\nalways nice PR's from you! keep it up!\"}}]}}, {'createdAt': '2015-09-30T22:59:02Z', 'updatedAt': '2017-01-18T13:36:08Z', 'title': 'err#8038: Index.shift() gives confusing error message when no Datetim…', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn', 'company': 'Onfido'}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11211', 'bodyText': '…eIndex\\ncloses #8038', 'comments': {'totalCount': 29, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"Is this the behavior you want though, vs. a clearer error message? This doesn't seem like it's shifting the index; it seems like it's adding 1...\\n        result = pd.Index(range(0,10,2)).shift(1)\\n        expected = pd.Index([1, 3, 5, 7, 9], dtype='int64')\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@hamedhsn I don't this should even work on a non-datetimelike index as it doesn't make any sense. So we should prob raise a TypeError.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a test and a whatsnew note in 0.17.1'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback\\n1)why do I need to change tseries/base/DatetimeIndexOpsMixIn ?\\n2)where should I add tests?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I indicated above. The .shfit incore/index.pyapplies toIndex/Int64Indexetc. These are not valid for shifting and so should raiseNotImplementedError. Then make the actual change intseries/base/DatetimeIndexOpsMixInwhich is the base class for the datetimelikes (`DatetimeIndex,TimeDeltaIndex,PeriodIndex``) where shifting makes sense'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': 'sure, but there is no issue with DatetimeIndex using shift() with or without freq. that was my question.\\nthe error happens for Index/Int64Index which i changed it to raise NotImplementedError.\\nbtw where do i have to add the related tests?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@hamedhsn you can add tests in test_index (though the timeseries ones are prob in tseries/test_base)'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback this also should be fine'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"@jreback @hamedhsn I think this behavior is confusing. I think there are some reasons to shift a non-DateTime-like index, but it shouldn't be done with a freq.\\nDo you guys think this is clear and expected behavior?\\n        idx = Index(range(5))   \\n        result = idx.shift(periods=2, freq=2)\\n        expected = pd.Index(range(4, 9))\\n        self.assert_index_equal(result, expected)\\nA version I could see being helpful is to shift by a number of items. i.e. this:\\n        idx = Index(range(5,10))   \\n        result = idx.shift(1)\\n        expected = pd.Index(range(6, 11))\\n        self.assert_index_equal(result, expected)\\n...although I'm more against the current implementation than I am fighting for this specific implementation.\\nAs ever, open to being wrong though.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@MaximilianR that would be a major change to add a freq to a non-timeseries Index. So wouldn't want to support that (here).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"@jreback That's exactly what I'm saying. This PR enables freq for shifting non-timeseries index (or am I woefully mistaken)? This is a copy paste from the new test code:\\n        idx = Index(range(5))   \\n        result = idx.shift(periods=2, freq=2)\\n        expected = pd.Index(range(4, 9))\\n        self.assert_index_equal(result, expected)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"that's out of scope for this PR\\nwould be unexpected as freq is not a property of an Index\\nyou are free to propose it but make an issue\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"I don't think I'm being clear. Your comment here would fix the concern, but that hasn't been added.\\nOr I'm likely missing something, so I'll leave it for now\"}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback is this implementation correct? let me know if I need to anything'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'a lot closer....still a couple of comments'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback applied your comments'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'one more comment. pls squash as well.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback did that. let me know if I need to do anything else. Thanks'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'comments; needs a rebase'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback applied the comments'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback should be fine now hopefully'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@hamedhsn if you would like to update according to comments would be great'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback I think I did, used self.create_index(), put that in DateTimelike. and for the other one moved that to based class in test_index.py and used self.create_index(). and rebased that.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback added the other two tests under their own test classes'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'xref #11452'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@hamedhsn ok, tests look good (you just need to remove an error line and they would pass). pls add the other requested tests and good 2 go.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback applied your comments.rebased it'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via a05eceb\\nthanks!'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'danger89'}, 'bodyText': \"It seems I still have this issue see my comment in: #11452. I'm using pandas v0.19.2 via pip.\"}}]}}, {'createdAt': '2015-09-30T21:35:26Z', 'updatedAt': '2015-10-02T15:25:42Z', 'title': 'Add ability to set the allowLargeResults option in BigQuery #10474', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11209', 'bodyText': \"Modify read_gbq() to allow users to redirect the query results to a destination table via the destination_table parameter\\nModify read_gbq() to allow users to allow users to set the 'allowLargeResults' option in the BigQuery job configuration via the allow_large_results parameter\\n\\ncc @aaront\", 'comments': {'totalCount': 8, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this would be confusing to a user as read_gbq should return s frame\\ndoes it here?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"Yes, read_gbq() still returns a DataFrame with the query results regardless if these additional parameters are set. I've just tried the following scenarios:\\n\\nIn the first test, I set the destination_table and confirmed a destination table was created and DataFrame was returned.\\nIn the second test, I set the destination_table and allow_large_results and confirmed a destination table was created and DataFrame was returned.\\n\\nI will add a unit tests now for the above mentioned scenarios (I missed it the first time around)\\nAll tests pass locally. Could this make it into the 0.17.0 release? I think it is a very useful feature.\\ntony@tonypc:~/pandas-parthea/pandas/io/tests$ nosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_redirect_query_results_to_destination_table_dataset_does_not_exist (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_redirect_query_results_to_destination_table_default (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_redirect_query_results_to_destination_table_if_table_exists_append (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_redirect_query_results_to_destination_table_if_table_exists_fail (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_redirect_query_results_to_destination_table_if_table_exists_replace (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\npandas.io.tests.test_gbq.test_generate_bq_schema_deprecated ... ok\\n\\n----------------------------------------------------------------------\\nRan 59 tests in 379.762s\\n\\nOK\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this is bloating the API\\nif u r returning a frame then simply use to_gbq and push it back up'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"I agree that it doesn't make sense to return the data in a DataFrame when a destination table is specified (since you can use to_gbq to push it back up). My preference would be to return an empty DataFrame when a destination table is specified in order to avoid the unnecessary download and upload of data when users want to create smaller datasets from larger ones. The ability to run queries and send the query results directly to a table (in an efficient manner) could be useful.\\nRegarding the allow_large_results parameter:\\nFrom https://cloud.google.com/bigquery/quota-policy#queries, query results > 128 MB compressed require the  'allowLargeResults' option to be set in the job configuration. One of the requirements for allowing large results is that you must specify a destination table.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Another potential solution, is to create a new function gbq.query_to_table() which does not return a DataFrame.  gbq.query_to_table() would require a destination table to be specified and would support a parameter allow_large_results.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea I am not averse to these changes. But would like 0.17.0 to release and settle before considering api change.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"further crafting a nice useful, non-duplicative api is actually tricky. You want to have the limited set of things that one could 'do' in an intuitve way. So one of the big issues is how to pass in options (.e.g like allow_large_result, which is really a 'user' option.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Do you think it would be better to close this pull request, and request that we support this feature in the odo project instead (assuming that odo will support gbq) since the odo project is aimed at data migration?\\nThe functionality in this pull request could be similar to the following pull request in odo which adds ability to append query results to a table : blaze/odo#37'}}]}}, {'createdAt': '2015-09-30T17:11:45Z', 'updatedAt': '2015-11-01T16:35:57Z', 'title': 'Copy on Write via weakrefs', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank', 'company': 'Academia'}, 'files': {'totalCount': 5}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11207', 'bodyText': \"Aims to eventually close #10954 , alternative to #10973\\nIn the interest of advancing the discussion on this, here's a skeleton of a different copy on write implementation.\\nCurrently\\nWhen setting on a view, converts to copy.\\n df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})\\n intermediate = df.loc[1:1,]\\n intermediate['col1'] = -99\\n\\n\\nintermediate\\n\\nOut[8]: \\n   col1  col2\\n1   -99     4\\n\\ndf\\n\\nOut[9]: \\n   col1  col2\\n0     1     3\\n1     2     4\\n\\nChained-indexing will ALWAYS fail.\\ndf = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})\\ndf.loc[1:1,]['col1'] = -99\\ndf\\n\\nOut[11]: \\n   col1  col2\\n1     2     4\\n\\nSettingWithCopy warning disabled, thought not fully removed (figure this is too early stage to really chase that down).\\nForward Protection for loc setting (proof of concept, added oct 2):\\n df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})\\n intermediate = df.loc[1:1,]\\n\\n df.loc[1,'col1'] = -99\\n intermediate\\n\\n  Out[11]: \\n   col1  col2\\n0     1     3\\n1     2     4\\n\\nGoals / Places I'd love help\\n\\nWe've discussed keeps full-column slices as views, since we can guarantee these will always be views. I created an attribute called _is_column_view that can be set to True when a slice is a full column (e.g. new = df['col1'], but I'm not sure how to actually figure out if a slice is a full column, so right now it's initialized as False and just stays that way.\\nThis has no protection against forward propagation -- situations where one sets values on a Frame and they propagate to views of that Frame. There IS a framework to address it, but it's incomplete. As it stands:\\n\\nFrames now have a _children list attribute to keep references to their child views.\\nBefore setting, there's a call to a function to try and convert the children from views to copies.\\nBut as it stand, I don't know how to really manage those references (i.e. how to put references to children in that _children list).\\n\\n\\n\\n@jreback\\n@shoyer\\n@JanSchulz\\n@TomAugspurger\", 'comments': {'totalCount': 15, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"u need to have tests\\ndoesn't matter if it's a demo or not\\nthese are the first things u should write to demonstrate the API\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': '@jreback ok, added!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': 'Added an example of forward protection. NOT generalizable or at all robust, just illustrative for one setting.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': 'This might work in a reasonably efficient way if you make _children a WeakValueDictionary (keyed by object id?). Certainly seems better than using garbage collection in totally ad-hoc ways...'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': \"@shoyer would you mind explaining a little more? You mean just a floating global dictionary as opposed to an object attribute?\\nMain problem I'm having now (love suggestions @jreback @shoyer ): where to place _convert_children() call. Is there a single function that's always called when data is changed where I can put it? Or a short list of functions where it should be put?\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"The problem with making _children a list is that you can't efficiently remove items from a list. Putting children in the values of dictionary solves that problem, and WeakValueDictionary even solves the cleanup for you automatically, because it removes items when they are garbage collected.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': 'oh, oh, I see. Great!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"this list of _children would only be a very small number of items, so it won't make much difference\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"I'm still a fan of using WeakValuesDictionary rather than rolling our own\\nsolution. Either way weak refs are a more viable approach than hijacking\\nthe garbage collector.\\nOn Fri, Oct 2, 2015 at 2:52 PM, Jeff Reback notifications@github.com\\nwrote:\\n\\nthis list of _children would only be a very small number of items, so it\\nwon't make much difference\\n—\\nReply to this email directly or view it on GitHub\\n#11207 (comment).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"well if you get it to work, all ears.\\nif you do\\na = df['foo']\\n\\nyou don't have a ref, so not really sure what you are trying to track\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': \"That's the one case we wanted to stay as a view (full column slices) right?\\nSo that might be a feature not a bug...\\nOn Fri, Oct 2, 2015 at 3:06 PM Jeff Reback notifications@github.com wrote:\\n\\nwell if you get it to work, all ears.\\nif you do\\na = df['foo']\\nyou don't have a ref, so not really sure what you are trying to track\\n—\\nReply to this email directly or view it on GitHub\\n#11207 (comment).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': \"Also: I don't yet understand the internals well enough to know where all I\\nneed to put self._children.append() and _convert_children() -- this is just\\nan attempt to figure out the machinery in one specific case of subsetting\\nand setting. Guidance on bottlenecks for where to put those calls to cover\\nall cases would be much appreciated.\\nOn Fri, Oct 2, 2015 at 3:10 PM Nick Eubank nickeubank@gmail.com wrote:\\n\\nThat's the one case we wanted to stay as a view (full column slices)\\nright? So that might be a feature not a bug...\\nOn Fri, Oct 2, 2015 at 3:06 PM Jeff Reback notifications@github.com\\nwrote:\\n\\nwell if you get it to work, all ears.\\nif you do\\na = df['foo']\\nyou don't have a ref, so not really sure what you are trying to track\\n—\\nReply to this email directly or view it on GitHub\\n#11207 (comment).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': \"@jreback @shoyer OK, I think this is getting close to working... Will keep working on this, but if anyone has any input or sees problems, input appreciated!\\nStill trying to figure out columns... (i.e. to keep df['col1'] as a view).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': \"@shoyer @jreback OK, now keeps dictionary-like column slices (v = df['col1']) as views as our one special case.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'nickeubank'}, 'bodyText': 'Moving to new thread:'}}]}}, {'createdAt': '2015-09-30T00:03:30Z', 'updatedAt': '2015-10-04T17:01:51Z', 'title': 'PERF: vectorized DateOffset with months', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 5}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11205', 'bodyText': \"This is a follow-up to #10744.  In that, vectorized versions of some offsets were implemented, mostly by changing to periods and back.\\nThe case of shifting by years/months (which is actually most useful to me) required some extra hoops and had poorer performance - this PR implements a special cython routine for that, for about a 10x improvement.\\nIn [3]: s = pd.Series(pd.date_range('1900-1-1', periods=100000))\\n\\n# Master\\nIn [4]: %timeit s + pd.DateOffset(months=1)\\n1 loops, best of 3: 140 ms per loop\\n\\n# PR\\nIn [2]: %timeit s + pd.DateOffset(months=1)\\n100 loops, best of 3: 14.2 ms per loo\", 'comments': {'totalCount': 4, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"@jreback - pushed changes for your comments and added MonthEnd and MonthBegin\\nFor your comment about using this routine in the DateOffset are you talking about cythonizing the actual apply method too?  On that front, I see there is an offsets.pyx file, but doesn't look it's actually used?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'do we have an asv benchmark for this? if not pls add one. otherwise minor change. lmk\\nadd to the perf section (e.g. just add this issue on in the whatsnew next to #10744)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'Made those doc changes.  Yeah,  there is an asv for DateOffset\\n    before     after       ratio\\n  [8ea8968 ] [c1d53f9 ]\\n-  121.83ms    18.49ms      0.15 timeseries.timeseries_datetimeindex_offset_fast.time_timeseries_datetimeindex_offset_fast\\n-  121.50ms    21.70ms      0.18  timeseries.timeseries_series_offset_fast.time_timeseries_series_offset_fast'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@chris-b1 thanks! these pr's are awesome! keep em coming!\"}}]}}, {'createdAt': '2015-09-29T19:51:34Z', 'updatedAt': '2015-10-05T11:05:55Z', 'title': 'Fixed pandas.DataFrame().join() function.', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'soerendip', 'company': None}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11203', 'bodyText': 'Added on_right and on_left keywords.\\nNow a right.join(left,on=key) works as expected.\\nOld behavior is archived by right.join(left,left_on=key)\\nTested multi-index funtionallity.\\nimport pandas as pd\\nfrom copy import copy\\nprint pd.__file__\\nleft = pd.DataFrame({\\'A\\': [\\'A0\\', \\'A1\\', \\'A2\\', \\'A3\\',\\'A4\\'],\\n                   \\'key\\': [\\'K0\\', \\'K1\\', \\'K3\\', \\'K4\\',\\'K5\\']})\\nright = pd.DataFrame({\\'C\\': [\\'C0\\', \\'C1\\', \\'C2\\', \\'C3\\'],\\n                   \\'key\\': [\\'K0\\', \\'K2\\', \\'K1\\', \\'K3\\']},\\n                    index=[4,5,6,7])\\n\\nprint left.join(right,on=\\'key\\',how=\"outer\",rsuffix=\"_r\")\\n\\nOld result:\\n     A key    C key_r\\n0   A0  K0  NaN   NaN\\n1   A1  K1  NaN   NaN\\n2   A2  K3  NaN   NaN\\n3   A3  K4  NaN   NaN\\n4   A4  K5  NaN   NaN\\n4  NaN   4   C0    K0\\n4  NaN   5   C1    K2\\n4  NaN   6   C2    K1\\n4  NaN   7   C3    K3\\n\\nNew result:\\n     A key    C\\n0   A0  K0   C0\\n1   A1  K1   C2\\n2   A2  K3   C3\\n3   A3  K4  NaN\\n4   A4  K5  NaN\\n5  NaN  K2   C1', 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"I'm not sure the old behavior was a bug.\\nFrom the docs:\\n\\njoin takes an optional on argument which may be a column or multiple column names, which specifies that the passed DataFrame is to be aligned on that column in the DataFrame\\n\\nso I think left.join(right, on='key') is equivalent to pd.merge(left, right, left_on='key', right_index=True), not pd.merge(left, right, on='key').\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'soerendip'}, 'bodyText': 'The example that you showed is not equivalent.\\nleft.join(right, on=\\'key\\',rsuffix=\"_r\")\\n    A key    C key_r\\n0  A0  K0  NaN   NaN\\n1  A1  K1  NaN   NaN\\n2  A2  K3  NaN   NaN\\n3  A3  K4  NaN   NaN\\n4  A4  K5  NaN   NaN\\n\\nand\\npd.merge(left, right, left_on=\\'key\\', right_index=True)\\n\\nEmpty DataFrame\\nColumns: [key, A, key_x, C, key_y]\\nIndex: []'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"n [5]: left.join(right, on='key', rsuffix='_y', lsuffix='_x', how='outer')\\nOut[5]:\\n  key    A key_x    C key_y\\n0  K0   A0    K0  NaN   NaN\\n1  K1   A1    K1  NaN   NaN\\n2  K3   A2    K3  NaN   NaN\\n3  K4   A3    K4  NaN   NaN\\n4  K5   A4    K5  NaN   NaN\\n4   4  NaN   NaN   C0    K0\\n4   5  NaN   NaN   C1    K2\\n4   6  NaN   NaN   C2    K1\\n4   7  NaN   NaN   C3    K3\\n\\nIn [6]: pd.merge(left, right, left_on='key', right_index=True, how='outer')\\nOut[6]:\\n  key    A key_x    C key_y\\n0  K0   A0    K0  NaN   NaN\\n1  K1   A1    K1  NaN   NaN\\n2  K3   A2    K3  NaN   NaN\\n3  K4   A3    K4  NaN   NaN\\n4  K5   A4    K5  NaN   NaN\\n4   4  NaN   NaN   C0    K0\\n4   5  NaN   NaN   C1    K2\\n4   6  NaN   NaN   C2    K1\\n4   7  NaN   NaN   C3    K3\\nJust to step through what's going on here there's an align done before the merge/join (roughly left.align(right.set_index('key'))). Then that goes into the merge.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'soerendip'}, 'bodyText': 'Ok, now it has the same output.\\nWhy do you think  that should be like that?\\nWith the NaNs?\\nMaking join() more like merge() would be great, IMHO.\\nIt would make these functions more consistent (and join() would actually do what I want).\\nI regularly stumble over that issue, because to me it is quite unintuitive.\\nCan you explain to me why the current behavior is desirable?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"Agreed that it's unintuitive ;)\\nFrom what I can tell, left.join(right) is supposed to be shorthand for pd.merge(left, right, left_index=True, right_index=True). The on keyword seems a bit strange to me (maybe it was added later).\\nPersonally I never use .join, and avoid mentioning it when teaching. My recommendation is\\n\\nuse concat for merging/joining by index\\nuse .merge for database style joins, possibly on an index, possibly on columns.\\n\\nAnyways, we can't change the behavior now. It's working as documented (even though the documented behavior is strange in this case).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'not sure what you are trying to do here. .join works as expected.'}}]}}, {'createdAt': '2015-09-29T19:06:15Z', 'updatedAt': '2015-10-01T10:45:52Z', 'title': 'BUG: groupby list of keys with same length as index', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'rinoc', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11202', 'bodyText': 'Fixes issue #11185', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-27T15:46:20Z', 'updatedAt': '2015-10-04T17:01:35Z', 'title': 'API: read_excel signature', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11198', 'bodyText': \"Replace the **kwds in read_excel with the actual list of supported keyword args. This doesn't\\nchange any functionality, just nicer for interactive use.  Also a bit of clarification on the thousands\\narg in the docstring.\\nAdditionally, chunksize was a parameter in the ExcelFile.parse signature, but didn't do anything (xref #8011).  I removed this and raise NotImplementedError if passed, which is potentially breaking.\", 'comments': {'totalCount': 20, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - alright, I deprecated ExcelFile.parse, modified a bunch of tests to no longer use that.\\nI also reordered the Excel docs to fit the new note; I think it reads more logically now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - made the changes you noted.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'looks good. ping when green.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Is there a good reason to deprecate ExcelFile.parse?\\nThe docs gives an explicit use for it compared to read_excel: http://pandas.pydata.org/pandas-docs/stable/io.html#reading-excel-files'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jorisvandenbossche - there\\'s no super-compelling reason; the main idea was to match up with api of to_excel, i.e. the \"ExcelFileWrapper\" (ExcelFile, ExcelWriter) doesn\\'t have any pandas-specific functionality, instead you pass it into the io functions (read_excel, to_excel).\\nI did update the docs to cover that specific example. edit: although it may be hard to see in the diff - rendered below.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'OK, I see that you changed that explanation in the docs I was pointing to. But, what is the point of ExcelFile then? What advantage does it give above just providing the string name?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@jorisvandenbossche\\nI would say there isn't any usecase for exposing ExcelFile in the current impl. Before read_excel, sure it was the way you passed things around. But it is not necessary anymore, unless I am missing something.\\nSo should deprecate this as well.\\nI could see a use as an object holding the iterator if we do support chunksizing (e.g. kind of like the TableIterator/TextReader classes). But these are actually internal and not exposed (except thru the iteration itself).\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"What I mean is:\\nxls = pd.ExcelFile('path_to_file.xls')\\ndata['Sheet1'] = pd.read_excel(xls, 'Sheet1', index_col=None, na_values=['NA'])\\ndata['Sheet2'] = pd.read_excel(xls, 'Sheet2', index_col=1)\\n\\nIn the above, the ExcelFile is rather superfluous, and does not seem to add any value (you can just pass the string to both, which yields the same but is shorter). So if we want to deprecate parse, I would rather deprecate ExcelFile itself.\\nOne thing that is possible with Excelfile is to inspect the sheet names. So that is a reason to keep it I think. But if we keep it, I don't really see a reason to deprecate its parse method. It's just like you also have both read_hdf as HDFStore (but I know, HDFStore has more extra functionality).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"So right now ExcelFile does two useful things - first it exposes sheet_names (list of sheets in that file) which can be handy for interactive.\\nSecond, because xlrd loads the whole worbook into memory (as far as I can tell) - there can be a performance benefit.\\nIn [12]: df = pd.DataFrame({'a': np.arange(10000),\\n    ...:                    'b': np.arange(10000)})\\n\\nIn [13]: with pd.ExcelWriter('temp.xlsx') as f:\\n    ...:     df.to_excel(f, 'Sheet1')\\n    ...:     df.to_excel(f, 'Sheet2')\\n\\nIn [14]: %%time\\n    ...: with pd.ExcelFile('temp.xlsx') as f:\\n    ...:     pd.read_excel(f, 'Sheet1')\\n    ...:     pd.read_excel(f, 'Sheet2')\\nWall time: 862 ms\\n\\n\\nIn [16]: %%time\\n    ...: pd.read_excel('temp.xlsx', 'Sheet1')\\n    ...: pd.read_excel('temp.xlsx', 'Sheet2')\\nWall time: 1.55 s\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@chris-b1 oh, that is good 2 know (maybe add a note about that in the docs).\\nOk, I do see some utility in ExcelFile. but .parse seems kind of 'internal' to me. Its superfluous. So we can deprecate to remove from the public API (though it doesn't hurt anything).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - expanded the ExcelFile docs to try and better clarify the purpose and mention the performance consideration.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'I am still not fully convinced that deprecating parse is needed. OK, it does not have much added value, but given this already exists such a long time (and is explicitly included in the api.rst), it will only annoy users by removing it. And it is also not very costly to keep, since the one just calls the other.\\nThis is more subjective, but, when having such a ExcelFile object, it feels a bit more natural to do ExcelFile.parse than passing it to a function like  pd.read_excel(ExcelFile) (although ExcelFile.read have been better).\\nAnd for clarity, big +1 on the signature and doc changes!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"ok, @chris-b1 why don't you back out the deprecation of .parse. I don't think its a big deal.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'Sure, not a problem.  Latest changes backs out the deprecation - I left the testing changes in (primarily using read_excel over ExcelFile) although did add back a couple calls to ExcelFile.parse.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'some minor doc changes'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - made those doc changes.  The travis failure seems to be unrelated?  Has to do with HTML formatting'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'yeh that failure happens once in a while, no idea why'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@chris-b1 I was just thinking: to make it more clear in the docs that ExcelFile.parse is superfluous, we could eg also limit its docstring with a reference to read_excel (and then the template is not needed)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jorisvandenbossche - sure makes sense to me, see update.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 0d39ca1\\nthanks!'}}]}}, {'createdAt': '2015-09-27T13:11:46Z', 'updatedAt': '2015-09-27T14:28:49Z', 'title': 'CLN: GH11170, * import is removed from test_internals.py', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11197', 'bodyText': 'closes #11170', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ping on green'}}]}}, {'createdAt': '2015-09-26T13:25:31Z', 'updatedAt': '2015-10-01T11:20:29Z', 'title': 'PERF: compare freq strings (timeseries plotting perf)', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11194', 'bodyText': 'See overview in #11084', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@sinhrks You did think this is OK?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"Thanks for this. I think we should call freq = Period._maybe_convert_freq(freq).freqstr at the top of extract_ordinals for standardize the freq string. Because followings are valid constructions.\\nimport pandas as pd\\n\\nidx = pd.period_range('2011-01', periods=3, freq='A')\\nidx.asobject.values\\n# array([Period('2011', 'A-DEC'), Period('2012', 'A-DEC'), Period('2013', 'A-DEC')], dtype=object)\\n\\nidx.freqstr\\n# 'A-DEC'\\n\\n# freq = 'A'\\npd.PeriodIndex(idx.asobject.values, freq='A')\\n\\n# freq = <YearEnd: month=12>\\npd.PeriodIndex(idx.asobject.values, freq=idx.freq)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche if you can update'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"@sinhrks Thanks for the catch. It won't be needed for the plotting case I suppose, but indeed in general the example you give are valid. I should probably add some tests for these, as they were not caught by the test suite)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche which benchmark does this close?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"At least the plotting.plot_timeseries_period.time_plot_timeseries_period one (didn't test if it would improve for some of the others as well)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"this doesn't seem to fix that one?\\nand its using date_range not period_range (but same either way)\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"Did you test with this branch?\\nimport numpy as np\\nimport pandas as pd\\n\\nN = 2000\\nM = 5\\ndf = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975', periods=N))\\n\\nWith pandas 0.16.2:\\nIn [18]: %timeit df.plot()\\n1 loops, best of 3: 196 ms per loop\\n\\nWith 0.17.0rc:\\nIn [18]: %timeit df.plot()\\n1 loops, best of 3: 778 ms per loop\\n\\nWith this branch:\\nIn [18]: %timeit df.plot()\\n1 loops, best of 3: 235 ms per loop\\n\\nSo it did not fully bring it back to previous performance, but at least not the big slow down.\\nIt indeed uses date_range and not period_range, but the time series plotting machinery converts the datetimes internally to periods for plotting.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"In [1]: N = 2000\\n\\nIn [2]: M = 5\\n\\nIn [3]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975', periods=N))\\n\\nIn [4]: %timeit df.plot()\\n1 loops, best of 3: 451 ms per loop\\n\\nIn [5]: pd.__version__\\nOut[5]: '0.17.0rc1+119.g9707214'\\n\\nIn [1]: pd.__version__\\nOut[1]: u'0.17.0rc1'\\n\\nIn [2]: N = 2000\\n\\nIn [3]: M = 5\\n\\nIn [4]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975', periods=N))\\n\\nIn [5]: %timeit df.plot()\\n1 loops, best of 3: 438 ms per loop\\n\\nIn [7]: import matplotlib\\n\\nIn [8]: matplotlib.__version__\\nOut[8]: '1.4.3'\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"In [6]: import matplotlib\\n\\nIn [7]: matplotlib.__version__\\nOut[7]: '1.5.0rc1'\\n\\nIn [1]: M = 5\\n\\nIn [2]: N = 2000\\n\\nIn [3]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975', periods=N))\\n\\nIn [4]: %timeit df.plot()\\n1 loops, best of 3: 499 ms per loop\\n\\nIn [5]: 2015-10-01 06:50:53.161 python[25071:8630508] setCanCycle: is deprecated.  Please use setCollectionBehavior instead\\n2015-10-01 06:50:53.163 python[25071:8630508] setCanCycle: is deprecated.  Please use setCollectionBehavior instead\\n2015-10-01 06:50:53.164 python[25071:8630508] setCanCycle: is deprecated.  Please use setCollectionBehavior instead\\n2015-10-01 06:50:53.166 python[25071:8630508] setCanCycle: is deprecated.  Please use setCollectionBehavior instead\\n\\nIn [5]: pd.__version__\\nOut[5]: '0.17.0rc1+119.g9707214'\\n\\n@TomAugspurger can we get rid of this deprecated function?\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"@jreback strange .. I just rebuild both versions (rc1 and this branch) and tested again:\\nIn [1]: N = 2000\\n\\nIn [2]: M = 5\\n\\nIn [3]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975',\\n periods=N))\\n\\nIn [4]: %timeit df.plot()\\n1 loops, best of 3: 734 ms per loop\\n\\nIn [5]: pd.__version__\\nOut[6]: '0.17.0rc1'\\n\\nIn [1]: N = 2000\\n\\nIn [2]: M = 5\\n\\nIn [3]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975',\\n periods=N))\\n\\nIn [4]: %timeit df.plot()\\n1 loops, best of 3: 217 ms per loop\\n\\nIn [5]: pd.__version__\\nOut[5]: '0.17.0rc1+111.ga6379b6'\\n\\nDid you test it with this branch or with current master? As I notice you have another commit in the version string\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"ok cardinal sin of not rebuilding the extensions....i must have had an older version...\\nthanks!\\nIn [1]: M = 5\\n\\nIn [2]: N = 2000\\n\\nIn [3]: df = pd.DataFrame(np.random.randn(N, M), index=pd.date_range('1/1/1975', periods=N))\\n\\nIn [4]: pd.__version__\\nOut[4]: '0.17.0rc1+118.g8ecec2d'\\n\\nIn [5]: %timeit df.plot()\\n1 loops, best of 3: 167 ms per loop\"}}]}}, {'createdAt': '2015-09-25T22:57:53Z', 'updatedAt': '2015-09-25T23:51:09Z', 'title': 'COMPAT: platform_int fixes in groupby ops, #11189', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11191', 'bodyText': 'closes #11189', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-24T09:33:39Z', 'updatedAt': '2015-09-24T21:58:27Z', 'title': 'DOC: fix bunch of doc build warnings', 'mergedBy': {'login': 'jorisvandenbossche'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 9}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11184', 'bodyText': 'Fixes mainly some linking errors', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-24T08:54:12Z', 'updatedAt': '2015-09-25T12:11:12Z', 'title': 'DOC: XportReader not top-level API', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11183', 'bodyText': '@kshedden XportReader is not top-level API, so it was erroring in the doc build.\\nI now removed it from api.rst (the TextFileReader what you get back with chunksize for read_csv is also not included), but I can also adapt it to use .. currentmodule:: pandas.io.sas so it builds correctly if you want.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this is fine'}}]}}, {'createdAt': '2015-09-24T00:13:08Z', 'updatedAt': '2015-09-26T15:17:44Z', 'title': 'API: raise on header=bool in parsers', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 8}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11182', 'bodyText': \"closes #6113\\nPassing header=True|False to read_csv (and brethren), read_excel, and read_html will now  raise a TypeError, rather than being coerced to an int.\\nI had thought about converting False to None but thought that could break someone's code in a very subtle way if they were somehow depending on the old behavior.  But happy to change that if it seems better.\\nIf you want to push this in for 0.17 I can add a doc-note or this could easily wait.\", 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 if you add a release note (put in the API section) can add for 0.17.0'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback added a doc note and testing for the True case.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'tiny doc-change. ping when you push'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback thanks, made that change'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank you sir!'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Nice!'}}]}}, {'createdAt': '2015-09-23T22:56:20Z', 'updatedAt': '2015-09-26T13:15:50Z', 'title': 'improves groupby.get_group_index when shape is a long sequence', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri', 'company': None}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11180', 'bodyText': 'closes #10161\\nxref #10161 (comment)\\nIn [5]: df = DataFrame(np.random.randn(5000, 100).astype(str))\\n\\nIn [6]: %timeit df.duplicated()\\n1 loops, best of 3: 151 ms per loop\\n\\nIn [7]: %timeit df.T.duplicated()\\n1 loops, best of 3: 1.39 s per loop\\npart of this is because of taking the transpose (maybe cache locality). i.e. below performs better even though the shape is the same as df.T in above:\\nIn [8]: df = DataFrame(np.random.randn(100, 5000).astype(str))\\n\\nIn [9]: %timeit df.duplicated()\\n1 loops, best of 3: 965 ms per loop', 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'are there asv benches for this?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you add a doc-note in the performance section as well. thxs.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'samuelclark'}, 'bodyText': \"I tested this fix on the same dataframe and it looks like it solves the problem\\nIn [8]: df.info()\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 5000 entries, 0 to 4999\\nData columns (total 35 columns):\\n...\\ndtypes: float64(7), int64(12), object(16)\\nmemory usage: 1.4+ MB\\n\\n\\nIn [9]: %timeit -n 3 df.T.duplicated()\\n3 loops, best of 3: 549 ms per loop\\nThere is still a slight regression from 0.12.0 but it is minimal.  Thanks for fixing this.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': 'there already is a frame_duplicated asv benchmark.\\nadded the doc note.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@behzadnouri maybe add to that benchmark a case with the tranposed frame? (to catch this case with many columns)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 3fb802a\\nthanks!'}}]}}, {'createdAt': '2015-09-23T17:29:37Z', 'updatedAt': '2016-01-11T14:15:44Z', 'title': 'BUG: indexing with boolean-like Index, #11119', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'preddy5', 'company': 'UCL'}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11178', 'bodyText': 'closes #11119', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'preddy5'}, 'bodyText': '@jorisvandenbossche'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"Does this only work where you supply two items for a DataFrame, because it's checking for ndim=len(key)? Do you want to be checking the length of the axis?\\nIn [11]: df2=pd.concat([df, pd.Series([1,3,12,9],name=True)],axis=1)\\n\\nIn [12]: df2\\nOut[12]: \\n   False  True   True \\n0      6      3      1\\n1      1      9      3\\n2     13      8     12\\n3      8      2      9\\n\\n\\nIn [13]: df2[[False]]\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-13-b7f220d1c4ee> in <module>()\\n----> 1 df2[[False]]\\n\\n/Users/maximilian/Dropbox/workspace/pandas/pandas/core/frame.py in __getitem__(self, key)\\n   1906         if isinstance(key, (Series, np.ndarray, Index, list)):\\n   1907             # either boolean or fancy integer index\\n-> 1908             return self._getitem_array(key)\\n   1909         elif isinstance(key, DataFrame):\\n   1910             return self._getitem_frame(key)\\n\\n/Users/maximilian/Dropbox/workspace/pandas/pandas/core/frame.py in _getitem_array(self, key)\\n   1947                 else:\\n   1948                     raise ValueError('Item wrong length %d instead of %d.' %\\n-> 1949                                  (len(key), len(self.index)))\\n   1950             # check_bool_indexer will throw exception if Series key cannot\\n   1951             # be reindexed to match DataFrame rows\\n\\nValueError: Item wrong length 1 instead of 4.\\n\\nIn [17]: df2[[False, True, False]]\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-17-88caa367ce94> in <module>()\\n----> 1 df2[[False, True, False]]\\n\\n/Users/maximilian/Dropbox/workspace/pandas/pandas/core/frame.py in __getitem__(self, key)\\n   1906         if isinstance(key, (Series, np.ndarray, Index, list)):\\n   1907             # either boolean or fancy integer index\\n-> 1908             return self._getitem_array(key)\\n   1909         elif isinstance(key, DataFrame):\\n   1910             return self._getitem_frame(key)\\n\\n/Users/maximilian/Dropbox/workspace/pandas/pandas/core/frame.py in _getitem_array(self, key)\\n   1947                 else:\\n   1948                     raise ValueError('Item wrong length %d instead of %d.' %\\n-> 1949                                  (len(key), len(self.index)))\\n   1950             # check_bool_indexer will throw exception if Series key cannot\\n   1951             # be reindexed to match DataFrame rows\\n\\nValueError: Item wrong length 3 instead of 4.\\nWhile if the column names are strings:\\nIn [33]: df2_str[['False','True']]\\nOut[33]: \\n   False  True  True\\n0      6     3     1\\n1      1     9     3\\n2     13     8    12\\n3      8     2     9\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"What actually is needed is to infer this:\\nIn [22]: pd.lib.infer_dtype(Index([True,False]))\\nOut[22]: 'boolean'\\n\\nso right now we don't have a separate boolean Index type, its just object dtype.\\nSo this particular case only is relevant when you:\\n\\nhave a boolean indexer\\nhave a boolean Index on the same axis (so it must be object dtype), then you infer\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'preddy5'}, 'bodyText': '@jreback can you help me understand the error, I am unable to reproduce the error on my notebook and the error is only occurring for python2.7'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'its unrelated, a spurious failed. I restarted. In any event I will look at this prob next week. You are jumping thru lots of hoops here.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'preddy5'}, 'bodyText': '@jreback Could you review the PR. Thanks'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"I'm not sure if this change is actually a good idea. Suppose we have a 2x2 DataFrame with columns and rows [True, False]:\\nIn [3]: df = pd.DataFrame([[1, 2], [3, 4]], columns=[True, False], index=[True, False])\\n\\nIn [4]: df\\nOut[4]:\\n       True   False\\nTrue       1      2\\nFalse      3      4\\n\\nWhat should df[[False, True]] return? Standard indexing with [] is a mess of fallback rules, so usually it's best to first think about what .loc and .iloc can handle.\\nHere, note that both .loc and .iloc claim to support boolean arrays as indexers. Given that indexes can include booleans, this is clearly an ambiguous case in our current rules. The current behavior may be a bug, but all the indexers seem to currently use booleans arrays for subsetting rows, not looking up index values:\\nIn [5]: df[[True, False]]\\nOut[5]:\\n      True   False\\nTrue      1      2\\n\\nIn [9]: df.loc[[True, False]]\\nOut[9]:\\n      True   False\\nTrue      1      2\\n\\nSo my suggestion is that we should first consider deprecating using booleans that are not index values with .loc. Then, once this is entirely unambiguous, we can consider appropriate fallback rules for normal indexing.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"deprecating boolean indexers for .loc is a non-starter as a) this would break all back-compat, b) this would make .loc completely non-intuitive and unusable with a boolean array, which is a core feature.\\nSupporting label indexing is currently a buggy edge case. I'll review soon.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"I agree that this would be a backwards incompatibility break, but a pretty smooth deprecation cycle would be possible. I disagree that it is unintuitive for .loc to not work with boolean arrays, because .loc is explicitly for labeled based indexing. We've simply gotten away with it because using booleans for labels is pretty rare.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@shoyer I don't see any good reason to not allow .loc to accept boolean arrays. This would cause way more issues than it solves. In fact it is quite common to do this. Not sure why you are pushing this way. We want more consistency and unification, not less.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@pradyu1993 rebase this on master pls.\\nthis needs a fair amount more testing. (of other cases of boolean labels).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'closing. if you want to update according to comments. pls reopen'}}]}}, {'createdAt': '2015-09-23T15:02:30Z', 'updatedAt': '2015-09-24T01:15:01Z', 'title': 'Update 10min.rst', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'KarrieK', 'company': 'Data journalist '}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11177', 'bodyText': '', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-23T11:35:04Z', 'updatedAt': '2015-09-23T23:19:53Z', 'title': 'DOC: header=None in SAS docs', 'mergedBy': {'login': 'jorisvandenbossche'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11176', 'bodyText': \"Fixed a typo where I had header=False\\nSide note, it might be nice to change the actual behavior, not the first time it's tripped me up.  I guess there already is an issue, #6113\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Thanks!\\nAnd that is indeed a rather confusing feature. If we you want to push for #6113, certainly welcome!'}}]}}, {'createdAt': '2015-09-23T03:15:23Z', 'updatedAt': '2015-09-24T07:11:23Z', 'title': 'issue #11119', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'preddy5', 'company': 'UCL'}, 'files': {'totalCount': 7}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11175', 'bodyText': 'Closes #11119', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@pradyu1993 Thanks for the contribution. Can you add some tests for this behaviour?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Follow-up: #11178'}}]}}, {'createdAt': '2015-09-22T21:28:43Z', 'updatedAt': '2016-02-16T16:29:59Z', 'title': 'ENH: Restore original convert_objects and add _convert', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage', 'company': None}, 'files': {'totalCount': 22}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11173', 'bodyText': 'Restores the v0.16 behavior of convert_objects and moves the new\\nversion of _convert\\nAdds to_numeric for directly converting numeric data\\ncloses #11116\\ncloses #11133', 'comments': {'totalCount': 16, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'Still a few things to do:\\n\\n Update release notes to remove new docs and add deprecation note\\n Make sure copy is passed to _possibly_convert_objects\\n Add note about bugs fixed in convert_objects\\n Restore docs for convert_objects\\n Add tests for to_numeric\\n Add docs for to_numeric\\n Move converters to a new file, convert.py'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': \"The important changes are:\\nconvert in ObjectManager: https://github.com/pydata/pandas/pull/11173/files#diff-e705e723b2d6e7c0e2a0443f80916abfR1521\\nThis change requires this to 'route' to either the old or new style converter.\\nto_numeric: https://github.com/pydata/pandas/pull/11173/files#diff-40d1f19921aa9f2932951124fad07cc6R53\\nNew function.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@bashtage Really thanks a lot for this!\\n(I am still not fully convinced the deprecation is needed, but I will try to summarize my reasons tomorrow)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@bashtage\\n\\npls move the 2 converting functions either to a new module, core/convert.py.\\nlet's finish up the documentation changes.\\n\\npls let me know asap.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'Still no tests for to_numeric or docs for this method.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'looks good! ping when tests for pd.to_numeric'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche ok with this'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'to_numeric needs some fixes (and tests): lib is not yet imported, lib.maybe_convert_numeric expects an array so it bugs on a series as arg, and the final result needs to be boxed into a series again if the arg was a series.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'Yes - it definitely needs work.  It expects something with the buffer\\ninterface, so I need to ensure that this works as expected.\\nOn Sun, Sep 27, 2015 at 8:12 PM Joris Van den Bossche <\\nnotifications@github.com> wrote:\\n\\nto_numeric needs some fixes (and tests): lib is not yet imported,\\nlib.maybe_convert_numeric expects an array so it bugs on a series as arg,\\nand the final result needs to be boxed into a series again if the arg was a\\nseries.\\n—\\nReply to this email directly or view it on GitHub\\n#11173 (comment).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@bashtage did you have a chance to add some tests here?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'Should be ready when green I think'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'Can squash commits - wanted to make it easier to see changes at each stage since close to release'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': '@jreback Green - let me know if I missed anything.  Only tick left is to add some docs although this can probably wait since convert_objects is still available.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'All addressed'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': '@jreback It is green - or should be since all required tests passed.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@bashtage thanks for all of the work on this!'}}]}}, {'createdAt': '2015-09-22T20:31:53Z', 'updatedAt': '2015-11-14T23:16:58Z', 'title': 'ENH: make Series.ptp ignore NaN values (GH11163)', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr', 'company': None}, 'files': {'totalCount': 5}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11172', 'bodyText': 'Closes GH11163.\\nSeries.ptp will now ignore missing values when calculating the peak to peak difference.', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'actually what I would is define this using generic.py/_make_stat_function (e.g. how .min and such are defined). though this would only apply to Series (so might need to move a couple of things around to deal with this).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr'}, 'bodyText': 'Ah OK - no problem, I should be able to take a look at this and update in the next week or so.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you update'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr'}, 'bodyText': \"Apologies - the past few weeks have been much busier than I'd anticipated. I hope to get back to update this very soon.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'no prob.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@ajcr progress?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@ajcr pls update when you can'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr'}, 'bodyText': \"@jreback: my apologies for the delay once again, I've just got back and was  looking at this again just now.\\nAs I understand it, I need to do the following:\\n\\nmake a nanptp function in nanops.py (is implementing this as nanmax - nanmin sufficient?)\\nadd an entry for ptp to _make_stat_function in generic.py\\nmake sure that this doesn't become a method of DataFrame - i.e. retrict it to Series\\n\\nThe last part I'm not quite sure how to do best, because NDFrame is a parent class of both Series and DataFrame. Are there any other methods from NDFrame that are restricted to one or the other object in this way?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"you don't really need to do step 1, simply define it in _make_stat_function to do this (as its sort of a trivial function)\\nif you look at the end of core/series.py there is a like Series._add_numeric_operations(), (and in DataFrame), so you just need to tag certain of these functions to be added only on certain classes.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr'}, 'bodyText': \"@jreback: made some changes and things are now green - let me know if you'd like me to improve anything or make any other changes.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'ajcr'}, 'bodyText': 'No problem (sorry it took a while) - thanks for your guidance!'}}]}}, {'createdAt': '2015-09-22T18:48:51Z', 'updatedAt': '2015-10-20T17:49:06Z', 'title': 'Added density of a categorical #10949', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'sornars', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11171', 'bodyText': 'Not sure if this is the correct approach to the problem\\ncloses #10949.\\nAssuming it is I will go ahead and add some documentation for this enhancement. Let me know if you have any feedback.', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"Great!\\nI know @jreback suggested to multiply by 100, but is that right? Shouldn't we let the user convert to percent? (a more general point than this PR)\\nI'm not sure you want integer division there.\\nI would test for an actual value, rather than just the code that you wrote\"}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'sornars'}, 'bodyText': 'Happy to change this to a proportion, I agree that it seems a more sensible default.\\nWith regards to integer division, what is the preferred way to target Python 2 and 3 with float division, from __future__ import division?\\nWill update the tests in the next commit.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': \"I'm not sure on the pandas standard for the division - the maintainers will opine - either __future__ or coercing with float(x) would work.\\nAnd they'll also ask for a note in what's new & the docs.\\nCheers!\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'further, can you do a tiny bit of research to see what R calls this (they call the Categorical a Factor.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'sornars'}, 'bodyText': \"I haven't had much luck searching for an equivalent field in R but I have found that the SAS community calls this number the cardinality ratio: http://www.sascommunity.org/wiki/Cardinality_Ratio\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'Should we consider preempting or handling ZeroDivisionError?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'whoosh never realized we allow 0 categories!. Yes that should be a NaN'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a release note in 0.17.1 (enhancements)\\nadd to api.rst as well'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you update'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'sornars'}, 'bodyText': \"Apologies for the delay, I was trying to understand out the correct way to add documentation for this feature before other projects stole me away from this fix.\\nI've made an updated commit. There remains one failing test due to the recommendation to switch from tm.assert_numpy_array_equal(np.nan, s2.cat.density) to self.assertEqual(np.nan, s2.cat.density). Please advise on the best way to approach this failing test.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'use self.assertTrue(... is np.nan)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"closing as we closed the original issue #10949\\nlet's see an actual usecase (as I proposed the original issue, and don't really have one :)\"}}]}}, {'createdAt': '2015-09-22T16:25:27Z', 'updatedAt': '2015-09-24T01:20:51Z', 'title': 'BF: for version comparison first arg should be the LooseVersion', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'yarikoptic', 'company': 'Dartmouth College, @Debian, @DataLad, @PyMVPA, @fail2ban'}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11168', 'bodyText': 'also enhanced skip test msg a bit', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank you sir!'}}]}}, {'createdAt': '2015-09-22T06:05:44Z', 'updatedAt': '2015-09-22T07:46:46Z', 'title': 'DOC: Stating to install NumPY', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'awessels', 'company': None}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11165', 'bodyText': '', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': 'Running pandas at all requires numpy, as is already documented in the install instructions.'}}]}}, {'createdAt': '2015-09-21T12:22:23Z', 'updatedAt': '2015-09-21T12:22:28Z', 'title': 'DOC: minor doc formatting fixes', 'mergedBy': {'login': 'jorisvandenbossche'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11161', 'bodyText': 'Removes some warnings', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-21T00:17:25Z', 'updatedAt': '2015-11-07T03:42:03Z', 'title': 'PERF: Series.dropna with non-nan dtype blocks', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11159', 'bodyText': \"Minor improvement when dropna actually does nothing. If OK for 0.17, I'll add a release note.\\nimport numpy as np\\nimport pandas as pd\\n\\ns = pd.Series(np.random.randint(0, 1000, 50000000))\\n\\n# before\\n%timeit s.dropna()\\n#1 loops, best of 3: 1.58 s per loop\\n\\n# after\\n%timeit s.dropna()\\n#1 loops, best of 3: 568 ms per loop\", 'comments': {'totalCount': 4, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'sure, pls add a note in 0.17.1 whatsnew'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks looks good, can you rebase / add a release note'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Rebased, and added release note. Followings are asv results added in this PR.\\n    before     after       ratio\\n  [a89b96d4] [b2cb56a8]\\n    10.76ms    10.60ms      0.99  series_methods.series_dropna_datetime.time_series_dropna_datetime\\n-   12.07ms   726.61μs      0.06  series_methods.series_dropna_int64.time_series_dropna_int64'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-21T00:13:11Z', 'updatedAt': '2015-09-24T02:16:41Z', 'title': 'PERF: nested dict DataFrame construction', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11158', 'bodyText': 'Part of #11084\\nhttp://pydata.github.io/pandas/#frame_ctor.frame_ctor_nested_dict_int64.time_frame_ctor_nested_dict_int64\\n    before     after       ratio\\n   [d28fd70 ] [48c588  ]\\n-  325.06ms    72.25ms      0.22  frame_ctor.frame_ctor_nested_dict.time_frame_ctor_nested_dict\\n-  364.35ms   106.67ms      0.29  frame_ctor.frame_ctor_nested_dict_int64.time_frame_ctor_nested_dict_int64\\n\\nTwo changes to get back to the old performance:\\n\\nskip attempting to box Timedeltas/Timestamps if not a DatetimeIndex/TimedeltaIndex\\nonly convert index to object once', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ping when green (and tests are moved)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - moved the Period test as suggested, travis is green. Thanks.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks@'}}]}}, {'createdAt': '2015-09-20T21:13:13Z', 'updatedAt': '2015-09-20T23:19:08Z', 'title': 'DEPR: deprecate SparsePanel', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11157', 'bodyText': 'does not fit the current implementation; in theory a Panel could actually do this, but very little support (or issues related).', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-20T21:12:11Z', 'updatedAt': '2015-09-20T23:18:11Z', 'title': 'BLD: use python-dateutil in conda recipe', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11156', 'bodyText': '', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-20T16:39:58Z', 'updatedAt': '2015-09-21T02:35:57Z', 'title': 'BUG/API: GH11086 where freq is not inferred if both freq is None', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11155', 'bodyText': 'closes #11086, and a typo', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'cool, ping when green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank you sir!'}}]}}, {'createdAt': '2015-09-20T13:38:25Z', 'updatedAt': '2015-09-20T19:10:54Z', 'title': 'ENH: add merge indicator to DataFrame.merge', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 2}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11154', 'bodyText': 'Follow-up to #10054\\nxref #8790\\nAdds new indicator keyword to DataFrame.merge as well', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-19T23:01:47Z', 'updatedAt': '2015-09-26T00:27:01Z', 'title': '(WIP) BUG: DatetimeTZBlock.fillna raises TypeError', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 6}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11153', 'bodyText': 'import pandas as pd\\nidx = pd.DatetimeIndex([\\'2011-01-01\\', pd.NaT, \\'2011-01-03\\'], tz=\\'Asia/Tokyo\\')\\ns = pd.Series(idx)\\ns\\n#0   2011-01-01 00:00:00+09:00\\n#1                         NaT\\n#2   2011-01-03 00:00:00+09:00\\n# dtype: datetime64[ns, Asia/Tokyo]\\n\\n# NG, the result must be DatetimeTZBlock\\ns.fillna(pd.Timestamp(\\'2011-01-02\\', tz=\\'Asia/Tokyo\\'))\\n# TypeError: putmask() argument 1 must be numpy.ndarray, not DatetimeIndex\\n\\n# NG, it should be object dtype, as the same as when Series creation with mixed tz\\ns.fillna(pd.Timestamp(\\'2011-01-02\\'))\\n# TypeError: putmask() argument 1 must be numpy.ndarray, not DatetimeIndex\\n\\nAlso, found existing DatetimeBlock doesn\\'t handle tz properly. Closes #7095.\\nidx = pd.DatetimeIndex([\\'2011-01-01\\', pd.NaT, \\'2011-01-03\\'])\\ns = pd.Series(idx)\\n\\n# OK, result must be DatetimeBlock\\ns.fillna(pd.Timestamp(\\'2011-01-02\\'))\\n#0   2011-01-01\\n#1   2011-01-02\\n#2   2011-01-03\\n# dtype: datetime64[ns]\\n\\n# NG, it should be object dtype, as the same as when Series creation with mixed tz\\ns.fillna(pd.Timestamp(\\'2011-01-02\\', tz=\\'Asia/Tokyo\\'))\\n#0   2011-01-01 00:00:00\\n#1   2011-01-01 15:00:00\\n#2   2011-01-03 00:00:00\\n# dtype: datetime64[ns]\\n\\n# NG, unable to fill different dtypes\\ns.fillna(\\'AAA\\')\\n# ValueError: Error parsing datetime string \"AAA\" at position 0', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this is #7095, yes?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"Correct, this should fix #7095. I'll add test case for this.\\nAlso, I found current impl can't handle inplace option properly. inplace should be effective if target contains no NaN values in case of DatetimeTZBlock.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks @sinhrks nice catch'}}]}}, {'createdAt': '2015-09-19T22:57:21Z', 'updatedAt': '2015-09-20T19:03:47Z', 'title': 'PERF: improves performance in groupby.size', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11152', 'bodyText': \"on master:\\nIn [1]: n = 100000\\n\\nIn [2]: np.random.seed(2718281)\\n\\nIn [3]: dr = date_range('2015-09-19', periods=n, freq='T')\\n\\nIn [4]: ts = Series(np.random.choice(n, n), index=np.random.choice(dr, n))\\n\\nIn [5]: %timeit ts.resample('5T', how='size')\\n1 loops, best of 3: 538 ms per loop\\non branch:\\nIn [5]: %timeit ts.resample('5T', how='size')\\n10 loops, best of 3: 27.6 ms per loop\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-19T19:31:05Z', 'updatedAt': '2015-11-13T16:41:57Z', 'title': 'BUG: axis kw not propogating on pct_change', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Tux1', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11150', 'bodyText': 'axis option was not correctly set on fillna in pct_change', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Thanks. Can you add a test for this?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@Tux1 pls add a test for this'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Tux1'}, 'bodyText': 'Sorry. It is my first commit.\\nI made a simple test that shows the issue and my commit fixed it.\\nIs it okay ?\\nThanks'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls see the contributing docs: http://pandas.pydata.org/pandas-docs/stable/contributing.html\\ntests / fix should be on a single pr.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Tux1'}, 'bodyText': 'Sorry... Moreover the test was wrong.\\nI have made another test which is passed with my patch. That is committed in a single branch, this one.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a whatsnew note for 0.17.1 (bug fix), use this PR number'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@Tux1 can you rebase / update'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Tux1'}, 'bodyText': 'Sorry for several commits. I think that is okay now ? whatsnew edited, commit rebased'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'some comments, pls squash when finished'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls rebase on master / squash, see docs here'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Tux1'}, 'bodyText': 'Sorry, my repo is a mess... I will be better next time'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 334b076\\nthanks!'}}]}}, {'createdAt': '2015-09-19T18:45:31Z', 'updatedAt': '2016-01-15T01:02:47Z', 'title': 'API: multi-line, not inplace eval', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 5}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11149', 'bodyText': 'closes #9297\\ncloses #8664\\nThis is just a proof of concept at this point - the idea is to allow multiple assignments within an eval block, and also include an inplace argument (currently default True to match old behavior, although maybe that should be changed) for chaining.\\nIn [15]: df = pd.DataFrame({\\'a\\': np.linspace(0, 100), \\'b\\': range(50)})\\n\\nIn [16]: df.eval(\"\"\"\\n    ...: c = a * 2\\n    ...: d = c / 2.0\\n    ...: e = a + b + c\"\"\")\\n\\nIn [17]: df.head()\\nOut[17]: \\n        a  b          c         d          e\\n0  0.000000  0   0.000000  0.000000   0.000000\\n1  2.040816  1   4.081633  2.040816   7.122449\\n2  4.081633  2   8.163265  4.081633  14.244898\\n3  6.122449  3  12.244898  6.122449  21.367347\\n4  8.163265  4  16.326531  8.163265  28.489796\\n\\nIn [18]: df.eval(\"\"\"\\n    ...: f = e * 2\\n    ...: g = f * 1.5\"\"\", inplace=False).head()\\nOut[18]: \\n        a  b          c         d          e          f          g\\n0  0.000000  0   0.000000  0.000000   0.000000   0.000000   0.000000\\n1  2.040816  1   4.081633  2.040816   7.122449  14.244898  21.367347\\n2  4.081633  2   8.163265  4.081633  14.244898  28.489796  42.734694\\n3  6.122449  3  12.244898  6.122449  21.367347  42.734694  64.102041\\n4  8.163265  4  16.326531  8.163265  28.489796  56.979592  85.469388', 'comments': {'totalCount': 15, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 this looks reasonable. I think that we should change the inplace to False by default. That obviously is a breaking change. So what I think we could do is this:\\nmake inplace=None the default\\ndetect if an assignment is being made. if inplace is None, then show a deprecation warning.\\nthis implicity makes inplace=None -> inplace=False for non-assignments (which is what it currently does, and is quite reasonable). To get inplace then assignment you would have to be explict about passing inplace=True, which is what we would want.\\nyes?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'cc @shoyer'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': 'Yes, this looks great!\\nI agree that we should change the default behavior to inplace=True in a version or two. I think @jreback has the right idea on the migration path.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 going to mark this for 0.18.0 as putting the deprecation warning in-place (pun intended)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 want to update / add in deprecation warning as discussed'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - this is updated - added a whatsnew and deprecation warnings.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls review the eval docs for any updates (in enhancingperf.rst)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'does this handle #8664  ?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback\\nUpdated the enhancingperf docs, and added a fix/test for #8664'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm. couple of doc changes. squash & ping when green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback - updated and green'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'very tiny doc change. ping when pushed.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you run: git diff master | flake8 --diff and fix anything. going to shortly be enforcing PEP8.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'Updated for your doc comment and PEP8 - nothing major but did have to add a couple # noqa where a name was only used in an eval block - is that the right approach?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 99c2d93\\nthanks for the nice changes!\\nyeh the # noqa is fine (I took one out, but otherwise ok).'}}]}}, {'createdAt': '2015-09-19T00:31:15Z', 'updatedAt': '2015-10-16T22:07:42Z', 'title': 'BUG: astype(str) on datetimelike index #10442', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn', 'company': 'Onfido'}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11148', 'bodyText': 'closes #10442\\nfix for the bug: Convert datetimelike index to strings with astype(str)\\ngoing to send pull request for test', 'comments': {'totalCount': 21, 'edges': [{'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': 'I added all the tests..please let me know if I need to add anything else.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': 'sounds fine now, let me know if I need to add anythingelse'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a note in whatsnew for 0.17.1 (enhancements). squash as well.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'going to need tests for Series.astype(str) as well (put near tests in test_series or tseries/tests/test_timeseries.py wherever the existing tests for .astype are), add the issue as a comment in the tests.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback\\nthere is no v0.17.1.txt for 0.17.1 in whatsnew folder.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'rebase on master\\ni put it their 2 days ago'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@hamedhsn can you update'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback sorry, what do I need to update?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'comments above, needs a rebase as well'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback did that.\\nThanks'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'couple of comments. pls squash as well'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback sorry that I am asking..how can I squash the last few commits to one. I tried a few different approaches without a luck.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'http://pandas.pydata.org/pandas-docs/stable/contributing.html#contributing-your-changes-to-pandas'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback squashed all the commits into one.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'you need to make sure you force push too.\\ngit checkout thisbranch\\ngit rebase -i master\\n\\nsquash (put a s by each commit)\\ngit push yourremote thisbranch -f'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback did that. thanks'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback should be fine now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok @hamedhsn ping on green.'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': \"@jreback strange, not sure why test is failed. both sides are object type but it raises this error..\\nAssertionError: [index] Expected object to be of type class 'pandas.core.index.Index',\\nfound class 'pandas.core.series.Series' instead\\nI tried the test with both side same nothing to do with astype but still get the same error\\nresult = pd.Series(pd.date_range('2012-01-01', periods=3))\\nexpected = result\\ntm.assert_index_equal(result, expected)\"}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'hamedhsn'}, 'bodyText': '@jreback it is fine now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-18T23:07:27Z', 'updatedAt': '2015-09-20T19:03:42Z', 'title': 'PERF: improves performance in SeriesGroupBy.transform', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri', 'company': None}, 'files': {'totalCount': 2}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11147', 'bodyText': '-------------------------------------------------------------------------------\\nTest name                                    | head[ms] | base[ms] |  ratio   |\\n-------------------------------------------------------------------------------\\ngroupby_transform_series2                    |   6.3464 | 167.3844 |   0.0379 |\\ngroupby_transform_multi_key3                 |  99.4797 | 1078.2626 |   0.0923 |\\ngroupby_transform_multi_key1                 |  10.9547 |  98.5500 |   0.1112 |\\ngroupby_transform_multi_key2                 |   9.0899 |  67.2680 |   0.1351 |\\ngroupby_transform_series                     |   5.4360 |  30.5537 |   0.1779 |\\ngroupby_transform_multi_key4                 |  39.9816 | 187.1406 |   0.2136 |\\n-------------------------------------------------------------------------------\\nTest name                                    | head[ms] | base[ms] |  ratio   |\\n-------------------------------------------------------------------------------\\n\\nRatio < 1.0 means the target commit is faster then the baseline.\\nSeed used: 1234\\n\\nTarget [0470a10] : PERF: improves performance in SeriesGroupBy.transform\\nBase   [7b9fe14] : revers import .pandas_vb_common', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-18T22:53:15Z', 'updatedAt': '2015-09-20T20:40:27Z', 'title': 'PERF: infer_datetime_format without padding #11142', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11146', 'bodyText': 'Closes #11142', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"@sinhrks pushed changes for your notes.  The perf impact on _guess_datetime_format is negligible because it is only ever called once.  Nothing showed up in asv, here are a couple timings.\\nIn [1]: s, l = pd.date_range('2014-1-1', periods=10), pd.date_range('2014-1-1', periods=100000)\\nIn [2]: s = s.strftime('%Y-%m-%d')\\nIn [3]: l = l.strftime('%Y-%m-%d')\\n\\n# master\\nIn [4]: %timeit pd.to_datetime(l, infer_datetime_format=True)\\n10 loops, best of 3: 30.3 ms per loop\\n\\nIn [5]: %timeit pd.to_datetime(s, infer_datetime_format=True)\\n1000 loops, best of 3: 300 µs per loop\\n\\n# PR\\nIn [5]: %timeit pd.to_datetime(l, infer_datetime_format=True)\\n10 loops, best of 3: 30.3 ms per loop\\n\\nIn [6]: %timeit pd.to_datetime(s, infer_datetime_format=True)\\n1000 loops, best of 3: 308 µs per loop\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-18T19:55:26Z', 'updatedAt': '2016-11-03T12:38:24Z', 'title': 'COMPAT: Support for MPL 1.5', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger', 'company': '@ContinuumIO'}, 'files': {'totalCount': 8}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11145', 'bodyText': 'closes #11111  (nice issue number)\\nTesting with tox locally I get 1 failure using either matplotlib 1.4 or 1.5. I may have messed up my tox config though. Still need to cleanup a bunch of stuff, I just threw in changes to get each test passing as I went.\\nrelease notes et. al coming tonight or tomorrow. Are we making any changes to the build matrix to run a matplotlib 1.5?', 'comments': {'totalCount': 8, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"u can add to the channel list (I think I showed this above)\\nit will thrn pick up the latest version if it's not specified exactly\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"is this not tested on travis? I don't see the icon\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"Travis was flaky yesterday morning. I just pushed some changes, and it's picked up now.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': '@jreback travis is green. Looks like https://travis-ci.org/pydata/pandas/jobs/81429896 and https://travis-ci.org/pydata/pandas/jobs/81429897 both ran with matplotlib RC1.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"make the 3.4 regular build not specify a matplotlib (in requirements-3.4.run). All of the slow tests are running (e.g. 2.7) just fine. But I don't think the fast ones actually are run by travis (in theory they are on 3.3, but that is not build ATM for 1.5.0rc1).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"@jreback done. We'll want to followup and pin it at 1.5 once that's released. I'm assuming the conda-forge channel will continue to track master and we won't want changes there breaking our builds.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks @TomAugspurger'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'tacaswell'}, 'bodyText': \"@TomAugspurger I don't think we plan to update conda forge from master, just tags.\"}}]}}, {'createdAt': '2015-09-18T17:29:52Z', 'updatedAt': '2017-11-28T11:37:59Z', 'title': 'Openpyxl22', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11144', 'bodyText': \"closes #10125\\nI've added preliminary tests for openpyxl >= 2.2 styles. Unfortunately, I don't know how to get a whole test class to be skipped so tests will only run with either the Openpyxl20 or Openpyxl22 test class disabled, depending upon which version is installed. I hope this can be fixed fairly easily. Some code somewhere is still calling openpyxl.styles.Style() which needs changing. The aggregate Style object is an irrelevancy in client code.\", 'comments': {'totalCount': 24, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"I don't know how to get a whole test class to be skipped\\n\\nYou can refer this which can be used as a decorator for test class.\\n\\nhttps://github.com/pydata/pandas/blob/master/pandas/util/testing.py#L183\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'Include this: Themanwithoutaplan@e4e02a8\\nfor testing on travis'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks @TomAugspurger @chris-b1\\ncan u guys give this a try\\nthxs'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"It's worth noting that this implementation is significantly slower than simply streaming the data to a worksheet. This is probably related to the cell abstraction.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"I tried a handful of examples, all of which seemed to work fine.\\nMaybe it's a separate PR, and I don't think it's a new issue, but could performance be improved like you suggested?  It gets pretty slow for even medium-size frames.  The cell abstraction itself is actually pretty light-weight, but maybe needs to be refactored a bit for whatever openpyxl works best with?\\nIn [1]: df = pd.DataFrame({'a': np.linspace(0, 100, 20000),\\n                           'b': range(20000),\\n                           'c': pd.date_range('1900-1-1', periods=20000)})\\n\\nIn [3]: %timeit df.to_excel('xlwswriter.xlsx', engine='xlsxwriter')\\n1 loops, best of 3: 2.42 s per loop\\n\\nIn [4]: %timeit df.to_excel('openpy.xlsx', engine='openpyxl')\\n1 loops, best of 3: 10.4 s per loop\\n\\nLooks like a lot of time spent on the styles?\\nIn [9]: %prun df.to_excel('openpy.xlsx', engine='openpyxl')\\n\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n1380478 1.068   0.000   1.579   0.000 base.py:31(__set__)\\n2880368 1.041   0.000   1.327   0.000 style.py:98(__iter__)\\n940344  0.783   0.000   1.856   0.000 base.py:45(__set__)\\n80003   0.604   0.000   3.694   0.000 lxml_worksheet.py:64(write_cell)\\n120012  0.570   0.000   1.422   0.000 style.py:108(__eq__)\\n480085/360067   0.569   0.000   1.049   0.000 hashable.py:55(key)\\n4600889/3520831 0.559   0.000   1.858   0.000 {getattr}\\n2441079 0.412   0.000   0.412   0.000 {isinstance}\\n1580529 0.394   0.000   0.394   0.000 base.py:20(__set__)\\n1   0.351   0.351   8.493   8.493 excel.py:1186(write_cells)\\n420198  0.324   0.000   1.205   0.000 base.py:143(__set__)\\n60000/40000 0.301   0.000   1.653   0.000 functools.py:105(wrapper)\\n1   0.267   0.267   6.991   6.991 lxml_worksheet.py:42(write_rows)\\n20000   0.267   0.000   0.267   0.000 datetime.py:72(time_to_days)\\n120019  0.266   0.000   0.770   0.000 style.py:114(__hash__)\\n60009   0.262   0.000   1.478   0.000 style.py:46(__init__)\\n80001   0.219   0.000   0.357   0.000 format.py:1770(_format_regular_rows)\\n80012   0.217   0.000   0.640   0.000 excel.py:1032(_convert_to_side)\\n60008   0.216   0.000   4.077   0.000 styleable.py:81(style_id)\\n1080203 0.212   0.000   0.364   0.000 hashable.py:59(<genexpr>)\\n160024  0.202   0.000   3.745   0.000 indexed_list.py:45(add)\\n80003   0.178   0.000   0.178   0.000 cell.py:111(__init__)\\n20000   0.174   0.000   0.245   0.000 jdcal.py:203(jd2gcal)\\n120000  0.171   0.000   0.236   0.000 threading.py:147(acquire)\\n20003   0.164   0.000   1.061   0.000 fonts.py:77(__init__)\\n160024  0.158   0.000   2.425   0.000 indexed_list.py:40(append)\\n180010  0.154   0.000   0.154   0.000 {method 'element' of 'lxml.etree._IncrementalFileWriter' objects}\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"Yeah, styles are a pain if they're being applied individually to cells but that shouldn't be the case for that dataset. They get decomposed to their constituents which get hashed to remove duplicates but they shouldn't be called when just setting datetimes.\\nExcel worksheets are row-oriented which is why this is append() is the standard call for bulk inserts. We've harmonised the API across the implementations but this means you'd have to pass in WriteOnlyCells among values for anything that needs styling to use the streaming mode.\\nThese are times on my machine using append() with openpyxl 2.3-b2\\nxlsxwriter 6.116043s\\nopenpyxl 18.04805s\\nopenpyxl direct 5.844062s\\n\\nIt would be slightly slower if it was keeping cells in memory.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"what is openpyxl direct ?\\ncan you add a whatsnew note (e.g. saying which versions people should use). Futher let's update the install.rst and io.rst with the recomendations (e.g. a warning box in the Excel section of io.rst, you can just list the versions in install.rst)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"openpyxl direct is how the snippet does it, based on the code you showed me at PyCon: just pass rows of data (so a Dataframe just needs expanding and transposing).\\nI'd personally advise against any version of openpyxl < 2.2. I hope to add support for NumPy types in the soon to be released 2.3.\\n2.0 & 2.1 make sense if you're editing a worksheet in place because they avoid the side-effects when working with styles. Otherwise the guards, used to avoid side-effects when working with styles, exact a huge penalty on performance.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': 'I\\'d also really appreciate a standalone version of the \"cells\" generator to work with and test in openpyxl so that the responsibility of the API is with openpyxl.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@Themanwithoutaplan you can put whatever you think is best for users. I think we have something pointing to use your 1.x Series, now you can just say use > 2.2 for styles.\\nnot sure what a 'standalone version of the cells' generator is?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': 'Just for testing purposes: whatever gets passed into the write_cells method.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'This is the formatter, which is relatively standalone.  IIRC xlsxwriter could also benefit from cells being yielded in a row-oriented matter too so it probably makes sense to change.\\nhttps://github.com/pydata/pandas/blob/master/pandas/core/format.py#L1615'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': 'Thanks. Can I assume the workbook will be saved as soon as data has been added? ie. Switch to using to write-only (formerly optimized-write) mode?\\nIn the current in-memory implementation we\\'re actually still using a dictionary to hold the cells but I plan to move this using some kind of matrix structure. This could have several memory benefits and would also facilitate aggregate functions (deleting / adding rows or columns) and might also speed things up, but is essentially an implementation detail write-only could benefit from a co-routine.\\nI\\'ve also tried looking at the xlrd reader code, because openpyxl\\'s read support is more extensive but I gave up. Would be happy to work on something using read-only mode for fast, sheet-by-sheet access but I think that going from rows (here the implementation really matters) to typed columns is definitely \"tricky\".'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@Themanwithoutaplan you can certainly optimize / change internals in a future version. Further I think both reading is by definition read-only (e.g. you can assume that).\\nTrying to get this version out-the-door. Let me know when you can make those doc changes.\\nAlso pls squash a bit.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"I'm working on the docs now. What should I be squashing?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'the commits. Ideally just 1-2 or so.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"I think I'll need to look that up. I'm not very familiar with git, if it's a feature there, and I tend to write a lot of commits. Does this affect when the CI work? I thought that would run once per push.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"http://pandas.pydata.org/pandas-docs/stable/contributing.html#contributing-your-changes-to-pandas\\nyou just\\ngit rebase -i master\\nthen change pick to s\\nI will do it if you don't\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"@jreback I tried squash but wasn't sure what it was doing.\\nImprovements on style handling can be done later. They're slow because the same style (cell with border) is being created over and over again. This can be avoided by having the style locally and just binding it when required: xcell.border = CellWithBorders. This will allow for future improvements like named styles which will just assign the name of a style to a cell and thus avoiding even the need to hash the style. But avoiding style object creation should be the biggest win.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"I hope the changes are okay.\\nI looked at using a dict to cache styles (this is something openpyxl does internally as well) but cell.style isn't hashable. Something like that will have to be added in the future.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': \"I've added a naive style caching strategy. This speeds things up quite a bit (25 %) in openpyxl 2.2 and even more (33 %) with openpyxl 2.3.\\nBTW. I had trouble running tests against openpyxl 2.3-b2 because LooseVersion doesn't like it: AttributeError: 'unicode' object has no attribute 'version'.\\nI thought I was following convention? I'd like to be able to run the tests against our betas.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@Themanwithoutaplan ok, going to merge when this passes: https://travis-ci.org/jreback/pandas/builds/81571705\\njust squashed yours + minor doc edits.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'Themanwithoutaplan'}, 'bodyText': '@jreback thanks very much and sorry for any trouble.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via c6bcc99\\nthanks for the fixes @Themanwithoutaplan'}}]}}, {'createdAt': '2015-09-18T02:10:10Z', 'updatedAt': '2015-10-16T23:33:49Z', 'title': 'BUG: Issue in the gbq module when authenticating on remote servers #8489', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea', 'company': None}, 'files': {'totalCount': 4}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11141', 'bodyText': 'Resolve issue where gbq authentication on remote servers fails silently.', 'comments': {'totalCount': 7, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Travis is green'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea can you update, I think should just raise an informative message'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"@jreback\\nI'm still working on this PR. I removed my last commit, which automatically closed the pull request. It should open when I submit a new commit.\\nI want to provide users with a simple way to create the required bigquery_credentials.dat file. Can I add a new function gbq.authorize() to do this?\\nThen I can raise an informative error message if the credentials are missing or invalid which tells the user to first call the gbq.authorize() function. When users call the gbq.authorize() function, the following message will appear: 'Please visit the following url to obtain an authorization code:' Users then will visit the web page to get the authorization code, and then paste it in.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea .authorize sounds reasonable'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Add  gbq.authorize() method'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'All tests passed in my local environment. Waiting on travis.\\nnosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_generate_bq_schema_deprecated ... ok\\n\\n----------------------------------------------------------------------\\nRan 53 tests in 357.736s\\n\\nOK'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Ready for review. Travis is green.'}}]}}, {'createdAt': '2015-09-18T01:56:09Z', 'updatedAt': '2015-10-09T13:21:28Z', 'title': 'BUG: remove midrule in latex output with header=False', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'loicseguin', 'company': None}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11140', 'bodyText': 'This bug fix addresses issue #7124. If to_latex is called with option header=False, the output should not contain a \\\\midrule which is used to separate the column headers from the data.', 'comments': {'totalCount': 5, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@TomAugspurger ?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a whatsnew note for 0.17.1'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': 'Yeah this looks good (looked at this code in a while though). @loicseguin if you could rebase and add a release note, thanks.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'loicseguin'}, 'bodyText': \"Done. Let me know if there's anything else I should add.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 924c419\\nthanks!'}}]}}, {'createdAt': '2015-09-17T19:55:56Z', 'updatedAt': '2015-09-18T09:14:24Z', 'title': \"TST: Verify fix for buffer overflow in read_csv with engine='c'\", 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'evanpw', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11138', 'bodyText': 'Tests for the bug in GH #9735, which was previously fixed in GH #10023.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-17T14:35:38Z', 'updatedAt': '2015-09-17T15:36:08Z', 'title': 'DEPR: Series.is_timeseries', 'mergedBy': {'login': 'sinhrks'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11135', 'bodyText': 'Follow-up for #10890. Deprecate Series.is_timeseries.', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm\\njust check that we are not actually using this anywhere (tests or otherwise)'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Based on the search result, it only exists in series.py and test_series.py. Both fixed.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'gr8 merge when ready'}}]}}, {'createdAt': '2015-09-17T01:34:38Z', 'updatedAt': '2015-09-17T22:31:12Z', 'title': 'BUG: nested construction with timedelta #11129', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 5}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11134', 'bodyText': \"Closes #11129\\nPer discussion in issue this adds similar hash equality to _Timedelta / datetime.timedelta\\nthat exists for _Timestamp / datetime.datime.\\nI tried inlining _Timedelta._ensure_components and it actually hurt performance a bit, so I left it\\nas a plain def but did build a seperate _has_ns check for hashing.\\nAdds a little overhead to hashing:\\nIn [1]: tds = pd.timedelta_range('1 day', periods=100000)\\n\\n# PR\\nIn [2]: %timeit [hash(td) for td in tds]\\n1 loops, best of 3: 810 ms per loop\\n\\n# Master\\nIn [2]: %timeit [hash(td) for td in tds]\\n1 loops, best of 3: 765 ms per loop\", 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'try making it a cpdef'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'hmm, no reason ever to do hash like that in a loop though..'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"changing to a cpdef didn't seem to matter, assuming I'm profiling the right thing (I think component access would take a call to _ensure_components())?\\nIn [2]: %timeit tds.microseconds\\n#cpdef\\n1 loops, best of 3: 776 ms per loop\\n#def\\n1 loops, best of 3: 775 ms per loop\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm. pls change _has_ns to be a method of _Timedelta'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jreback made that change, Travis green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank u sir!'}}]}}, {'createdAt': '2015-09-17T01:34:35Z', 'updatedAt': '2015-09-22T21:33:35Z', 'title': 'API: convert_objects, xref #11116, instead of warning, raise a ValueError', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11133', 'bodyText': 'closes #11116\\nThis is a lot more noisy that the current impl. It pretty much raises on the old-style input if it had coerce in it. This is correct as the actual behavior of coerce has changed (in that it now it much more strict).\\nerrror on old-style input\\n\\nraise a ValueError for df.convert_objects(\\'coerce\\')\\nraise a ValueError for df.convert_objects(convert_dates=\\'coerce\\') (and convert_numeric,convert_timedelta)\\n\\nIn [1]: data = \"\"\"foo,bar\\n   ...: 2015-09-14,True\\n   ...: 2015-09-15,\\n   ...: \"\"\"\\n\\nIn [2]: df = pd.read_csv(StringIO(data),sep=\\',\\')\\n\\nIn [3]: df\\nOut[3]: \\n          foo   bar\\n0  2015-09-14  True\\n1  2015-09-15   NaN\\n\\nIn [4]: df.convert_objects(\\'coerce\\')\\nValueError: The use of \\'coerce\\' as an input is deprecated. Instead set coerce=True.\\n\\nIn [5]: df.convert_objects(coerce=True)\\nValueError: coerce=True was provided, with no options for conversions.excatly one of \\'datetime\\', \\'numeric\\' or \\'timedelta\\' must be True when when coerce=True.\\n\\nIn [8]: df.convert_objects(convert_dates=\\'coerce\\')\\n/Users/jreback/miniconda/bin/ipython:1: FutureWarning: the \\'convert_dates\\' keyword is deprecated, use \\'datetime\\' instead\\nValueError: The use of \\'coerce\\' as an input is deprecated. Instead set coerce=True.', 'comments': {'totalCount': 22, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'cc AmbroseKeith\\ncc @bashtage\\n@jorisvandenbossche @TomAugspurger'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"I am not really in favor of this, as this really completely breaks people's code.\\nAnother option would be to not do any mapping of the old keyword arguments to the new ones. So leave the old keywords with deprecation warnings but their original behaviour, and the new keywords with the new behaviour.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"it's not the deprecation mappings that are a problem. it's that the coerce conversion by definition is forced. using it on s frame is just not a good idea at all.\\nin prior versions it worked because it was really never a forced conversion.\\nusers code was simply wrong and should break\\nI don't see any way around this nor should their be\\nthey were relying on broken behavior\\nthis is a loud break showing that what you were doing was wrong\\nI suppose s better error message would help\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"Yes, the fact that previously the conversion was not really 'forced' (as if it only converted if at least one could be converted), and now it is 'forced', this is what it is making a breaking change. Peoples code was not wrong, they were just using the function how it worked.\\nNot doing the mapping of the old to the new keyword arguments would be a way to introduce the new behaviour (under the new kwargs) without breaking people's code (but of course, the old behaviour should still be supported in the code base in this case, which could make it more complex)\\nI agree that this PR is in fact (in some cases) better than current master, as it raises an error notifying the user he/she should look at it, instead of just changing the result and breaking possible further processing.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"I don't see any reason to support old code like that\\nit just prolongs the change period\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"Actually, what you are doing here is partly the same as I am saying: not doing the mapping of old to new kwargs (for the 'coerce' case). But the question is then what to do with the old kwargs: raising an error (this PR), or keeping the old behaviour but deprecating (allowing a more smooth transition).\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"@jreback I don't say that is the better or easy approach, but the reason to do this is pretty clear: keep current user code working with 0.17.0 and giving them some time to adapt.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Further, not doing the mapping would also allow us to further clean up some inconsistencies in the current API (which would otherwise be more breaking changes):\\n\\nnumeric=True also implies coerce=True (while this is not the case for datetime and timedelta), so converting numeric strings without coercing is not possible (actually, it is possible with astype, but not with convert_objects)\\ndatetime=True does not parse strings to datetimes, while datetime=True, coerce=True does parse strings. While coerce should be about converting inconvertibles to NaN or nto.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"as I said I think we could give a better error message (kind of like instructions on how his changed)\\nI'll put that up a bit later\\nbut this is a pretty narrow case - breaking is sometimes necessary\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"I think that we should just change this whole approach - this is too error prone\\nlet's deprecate this entire function - not adding anything (so I guess revert to the prior behavior) - or just outright raise an error\\nadd pd.to_numeric(errors='raise'|'ignore'|'coerce)\\neg same signature as pd.to_datetime/timedelta\\nthat way the pattern is that the user needs to apply this to individual columns -\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'another option is to make a new method\\n.convert with the new behavior\\nand leave .convert_objects alone (but deprecate it fully)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': \"using it on s frame is just not a good idea at all.\\n\\nI agree with this.  Better practice is to use\\ndf['a'] = df['a'].astype('float')\\n\\nor\\ndf['a'] = pd.to_datetime(df['a'])\\n\\nso the the minimal conversions is used (or an error is raised).\\n\\nlet's deprecate this entire function - not adding anything (so I guess revert to the prior behavior) - or just outright raise an error\\n\\nI think this is probably the right approach, plus adding to_numeric which will handle the numeric uses of convert_objects\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@bashtage you want to take a crack at this?\\n\\nrestore original .convert_objects (though you prob want to use your new structure, maybe have a compat parm (into the _possibly_convert_objects)\\ndeprecate .convert_objects\\nchange whatsnew\\npd.to_numeric'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@bashtage are you going to be able to address this?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"+1 to adding a pd.to_numeric\\nI would not add a new convert method. If we want this functionality, we can do it within convert_objects (instead of deprecating the method in favor of a new, we can deprecate the keyword arguments in favor of new arguments, within the same method. In the end, the result can be the same, but we don't need a new method on DataFrame).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': \"I can get to it this week.\\nIMO the old behavior of convert_objects is at best poor and in the long run it would be better to remove it.  For example, it didn't even respect the coerce flag, and has wierd dependencies between types (since it tries one, moves on to the next,etc, and so it not robust to lots of reasonable refactorings).\\nI think there are two ways forward:\\n\\nadd to_numeric to there is a to_datetime and to_timestamp which completes the set.  Start deprecation of convert_objects\\nMove new version to convert and start deprecation.\\n\\nIn principle adding to_numeric and convert aren't orthogonal, so both could be done.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@bashtage thanks!\\n\\nI think let's just add pd.to_numeric\\nas far as the original .convert_objects. I agree the original behavior was just misleading. I think it might simply be best to just use kind of what this PR does. E.g. map the original arguments, if no coerce is specified, then it should work, if not, refuse to guess and just raise. Further I think let's simply deprecate the entire function (.convert_objects).\\n\\nYes this will break code, but it will loudly do it and I think only a very very small subset of users will experience this. We cannot cleanly do this as the fundamental behavior is changed. I suppose could revert to the original behavior, but simplest/easiest from this point is just raise.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"Agree on pd.to_numeric, this is useful in any case!\\nOn convert_objects, if we want to deprecate it, I would revert it to how it was originally (0.16.2). I don't really see a point in breaking people's code to make something more consistent in a feature we are going to deprecate anyway. So why not just deprecating it then and pointing them to pd.to_numeric/datetime. I would say it is either change how convert_objects (to make it better, in order to keep it) or either deprecate it.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"ok, let's rollback the entire .convert_objects change, and just deprecate it entirely.\\nIf you want to leave the logic you have internally for _possibly_convert_objects thats fine as well (though should be the same exact external API), so not sure what is easier.\\nand add pd.to_numeric (which is essentially your new logic).\\n@bashtage change the documentation to advertise to_numeric/to_datetime/to_timedelta, rather than .convert_objects.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'bashtage'}, 'bodyText': 'I have been working on the following:\\n\\nRevert the public convert_objects with the small bug fixed.\\nTurn the current convert_objects to a private _convert.  This function is used internally in a handful of places and works correctly.\\nAdd to_numeric\\n\\nOne question: where should to_numeric live?  generic.py?  The other to_ live in time series specific modules.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@bashtage awesome!\\npd.to_numeric could live in pandas/tools/util.py (not much there) ATM (then import in __init__ into the .pd API).\\nprob should reorg this at some point, though that is purely internal.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'closing in favor of #11173'}}]}}, {'createdAt': '2015-09-16T23:46:48Z', 'updatedAt': '2015-09-17T00:28:58Z', 'title': 'ERR: make sure raising TypeError on invalid nanops reductions xref #10472', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11131', 'bodyText': 'xref #dask/dask#693', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-16T21:02:57Z', 'updatedAt': '2015-09-16T23:01:46Z', 'title': 'BLD: install build deps when building', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 26}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11127', 'bodyText': 'closes #11122\\ndependency resolution in the pandas channel is a bit challenging as the current script tries to install all deps up front before building. But statsmodels requires pandas, so a current version is installed (e.g. the rc), and several of the deps are changed.\\nSo fixed, but installing only the build deps, compiling, then install in the run deps.', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-16T18:03:12Z', 'updatedAt': '2015-09-16T23:04:13Z', 'title': 'BUG: Fix computation of invalid NaN indexes for interpolate.', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'lmjohns3', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11124', 'bodyText': \"Fixes issue #11115.\\nThis change fixes up a couple edge cases for the computation of invalid NaN indexes for interpolation limits.\\nI've also added a test explicitly for the reported bug (and similar behaviors at the opposite end of the series) in #11115.\\n/cc @jreback\", 'comments': {'totalCount': 4, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'in the whatsnew add this issue on (where the limit_direction change exists, just add this issue number on)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'lmjohns3'}, 'bodyText': 'Done!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"gr8. don't worry about eh failing builds on travis. fixing shortly\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-16T04:27:04Z', 'updatedAt': '2015-09-24T23:37:55Z', 'title': 'ENH: Add ability to create datasets using the gbq module', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea', 'company': None}, 'files': {'totalCount': 5}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11121', 'bodyText': \"Removed the bq command line module from test_gbq.py, as it doesn't support python 3. In order to do this, I had to create functions for create_dataset(), delete_dataset() and dataset_exists().\\nThis change is required for #11110\\nAt the same time, I also implemented the following list functions: list_dataset() and list_table().\", 'comments': {'totalCount': 33, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'The build failed on travis. I noticed an unusual error in the build log.\\nhttps://travis-ci.org/pydata/pandas/jobs/80567794\\nTraceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\n  File \"pandas/__init__.py\", line 16, in <module>\\n    \"extensions first.\".format(module))\\nImportError: C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run \\'python setup.py build_ext --inplace\\' to build the C extensions first.\\nnosetests --exe -A slow and not network and not disabled pandas --doctest-tests --with-xunit --xunit-file=/tmp/nosetests.xml\\nE\\n======================================================================\\nERROR: Failure: ImportError (C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run \\'python setup.py build_ext --inplace\\' to build the C extensions first.)\\n----------------------------------------------------------------------\\nTraceback (most recent call last):\\n  File \"/home/travis/miniconda/envs/pandas/lib/python2.7/site-packages/nose/loader.py\", line 418, in loadTestsFromName\\n    addr.filename, addr.module)\\n  File \"/home/travis/miniconda/envs/pandas/lib/python2.7/site-packages/nose/importer.py\", line 47, in importFromPath\\n    return self.importFromDir(dir_path, fqname)\\n  File \"/home/travis/miniconda/envs/pandas/lib/python2.7/site-packages/nose/importer.py\", line 94, in importFromDir\\n    mod = load_module(part_fqname, fh, filename, desc)\\n  File \"/home/travis/build/pydata/pandas/pandas/__init__.py\", line 16, in <module>\\n    \"extensions first.\".format(module))\\nImportError: C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run \\'python setup.py build_ext --inplace\\' to build the C extensions first.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': '@parthea I restarted the build. Maybe it was a one-off failure.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Ready for review. All tests passed in my local environment,\\nnosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_bq_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_bq_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_bq_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_bq_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_bq_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_bq_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_bq_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_bq_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\n\\n----------------------------------------------------------------------\\nRan 53 tests in 368.008s'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea travis is fixed. go ahead and rebase'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': '@jreback\\nCould this make it into 0.17.0 ? It would helpful for my other PR #11110'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'what does create_dataset do? what is the difference between this and a table?\\nlist_table is ok.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'why is this needed for #11110  ?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"A table belongs to a dataset. In order to create a table, you must either have an existing dataset or create a new dataset.\\nTables\\nhttps://cloud.google.com/bigquery/docs/reference/v2/tables\\nDatasets\\nhttps://cloud.google.com/bigquery/docs/reference/v2/datasets\\nFrom https://cloud.google.com/bigquery/what-is-bigquery#tables,\\nDatasets allow you to organize and control access to your tables. Because tables are contained in datasets, you'll need to create at least one dataset before loading data into BigQuery.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"why is this needed for #11110 ?\\n\\nThe bq command line module doesn't support python 3. The  bq command line module is currently used to create and delete datasets so unit tests will be broken in python 3 builds without the ability to create and delete datasets.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you simply do this via a web interface? we are adding all of these functions which should be done externally to pandas.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"separately I think we need to make the API's more explicit, e.g.\\ncreate_table\\ndelete_table\\ntable_exists\\n\\nshould these be\\ntable_create\\ntable_delete\\ntable_exists\\n\\n?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'can you simply do this via a web interface?\\n\\nYes, it can be done through web interface. From the integration testing point of view, it would be easier to be able to create datasets programatically. Each time the test starts, there is a newly created dataset used for testing. The dataset is deleted programatically after the test.\\nOne potential change could be to use the create and delete functions for unit testing purposes only without exposing the functionality globally.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'table_create\\ntable_delete\\ntable_exists\\n\\nYes, I agree this is much better. I will commit a new version.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"why don't we just advertise the GbqConnector as the main way to do things (and then you have methods you can call directly on it), e.g. create(), delete(), exists().\\nI would actually rename this to Table, then no problem with having an Dataset object as well.\\ne.g.\\nt = Table(project_id=....)\\nt.create()\\nt.delete()\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"sounds good! I'll commit a new version soon.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Ready for review. All tests passed.\\nnosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\n\\n----------------------------------------------------------------------\\nRan 53 tests in 458.258s\\n\\nOK'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Travis is green'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"I am not familiar with gbq, but about the API: if you have a gbq.Table class, I would expect that this maps to a certain table in a gbq Dataset. But if that is the case, something like table.exists('my_table') is a bit strange.\\nAnd given that a table gets created like gbq.Table(projectid, 'my_dataset'), this seems more like it maps to a Dataset, although you also have a Dataset class which does not map to a Dataset but only holds a project id.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"@jorisvandenbossche\\nI agree. I'm going to propose another naming convention. Assuming the following structure on Google BigQuery :\\n\\nA table belongs to a dataset.\\nA dataset  belongs to a project id.\\n\\nCurrent implementation for Dataset class\\ndataset = gbq.Dataset(projectid)\\nThe Dataset class has the following instance methods:  create, delete, list, exists\\n\\nWhere create(datasetid) creates the specified dataset under the project projectid.\\nWhere delete(datasetid) deletes the specified dataset under the project projectid.\\nWhere list() lists all datasets under the project projectid\\nWhere exists(dataset_id) checks to see if the dataset exists under project projectid\\n\\nProposed implementation for Project, Dataset, Table class\\nProject class\\nproject = gbq.Project(projectid)\\nThe Project class has the following instance methods: dataset_list\\n\\nWhere dataset_list() lists all the datasets under project projectid\\n\\nDataset class\\ndataset = gbq.Dataset(project, datasetid)\\nThe Dataset class has the following instance methods: table_list,create, delete, exists\\n\\nWhere table_list() lists all tables under the dataset datasetid\\nWhere create() creates the specified dataset under the dataset datasetid.\\nWhere delete() deletes the specified dataset under the dataset datasetid.\\nWhere exists() checks to see if the dataset exists under dataset datasetid\\n\\nTable class\\ntable = gbq.Table(dataset, tableid)\\nThe Table class has the following instance methods: create, delete, exists\\n\\nWhere create() creates the specified table under the table tableid.\\nWhere delete() deletes the specified table under the table tableid.\\nWhere exists() checks to see if the table exists under table tableid\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"let's not add even more API here, keep it as minimal as possible.Dataset/Table are enough. and just have a Dataset.tables() method.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@parthea Is your assessment that this would be used a lot by users? (creating the DataSet\\nprogrammatically) Or is it mainly because we need this ourselves for the tests?\\nBecause otherwise, we could maybe also start with just having these as private methods for internal use as a first step.\\nFurther, creating the Dataset and Table objects to only be able to create or delete them, seems a bit overkill to me thinking a bit more about it.\\ndataset = gbq.Dataset(projectid, datasetid)\\ndataset.create()\\n\\nvs\\ngbq.create_dataset(projectid, datasetid)\\n\\nObject oriented APIs can be nice, but in some case just sticking to some functions can be simpler and more straightforward.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"@jorisvandenbossche\\n@jreback\\n\\nwe could maybe also start with just having these as private methods for internal use as a first step.\\n\\nThis need for this enhancement was driven around integration testing. While I was working on this enhancement, I thought it would be good to expose the create and delete functions for tables and datasets in case others require it. I believe the value of this enhancement is with integration testing so I am ok with using private functions to create and delete tables / datasets and see if there is demand to expose the functionality.\\nI think it would be more flexible for future development if I keep the Table and Dataset classes, but don't expose them. I could add the dataset.tables() instance method as requested. Listing datasets will also be useful for integration testing during the integration testing setup and teardown. (ie use project.datasets() to list datasets ).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@parthea ok, let's keep them private for now (maybe make them _Table, _Dataset to recognize this, and take them out of the API.rst. All for integration testing :)\\nbut since we are so close to actually releasing, just want to finish this up.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': '@jreback I am currently travelling. A colleague will commit the final changes for this PR (should be today)\\n@aaront'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': '@jreback let me know if anything else is needed'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea ok, small API change and a doc check. ping when green. (also confirm that this passes locally for you ).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': \"@jreback made the requested changes except for the generator, I don't want to rock the boat too much at the moment :)\\nThe indentation in api.rst looks OK on my end.\\nMy only concern at this point is that some public API was removed: create_table, delete_table, and table_exists are no longer part of the module. If this is OK, then all is go.\\nI've just made a commit for some fixups for some broken tests, as well.\\nRunning all the gbq tests locally yields:\\n> nosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\n\\n----------------------------------------------------------------------\\nRan 53 tests in 429.681s\\n\\nOK\\n\\nPing me if there's anything else needed.\\ncc: @parthea\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@aaront @parthea\\n\\nMy only concern at this point is that some public API was removed: create_table, delete_table, and table_exists are no longer part of the module. If this is OK, then all is go.\\n\\nThese only existed in master. They have not been released, so this is ok.\\nJust make the small change in the naming of generate_schema and I think good to go.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': \"@jreback all 53 tests pass locally again.\\nPing me if you'd like me to squash these commits. Will ping when travis is green.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': 'Got your comment about deprecating & removing after I had made the rename of generate_bq_schema. Will remove this from the docs and properly deprecate.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'gr8. pls squash as well.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': 'All tests passing locally after squash. Will ping when Travis is done.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via a5276cf\\nthanks!\\nI tweaked the deprecation a bit.'}}]}}, {'createdAt': '2015-09-16T01:58:17Z', 'updatedAt': '2015-09-22T14:16:59Z', 'title': 'BF: do not assume that expanduser would replace all ~', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'yarikoptic', 'company': 'Dartmouth College, @Debian, @DataLad, @PyMVPA, @fail2ban'}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11120', 'bodyText': 'since it must not e.g. if there is ~ in some directory name along the path (awkward but possible/allowed).\\nBetter to check either new path is absolute.  Removed 1 test which had no value', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-15T16:32:37Z', 'updatedAt': '2015-09-15T23:25:24Z', 'title': 'COMPAT: compat for python 3.5, #11097', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 11}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11114', 'bodyText': 'closes #11097\\n\\nupdate install / compat docs for 3.5\\nuse visit_Call based on the version of python\\nskip if html5lib is not installed in test_data\\nbug in nose causes deprecation warning in some pytables tests\\nremove superfluous socket timeout', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-15T15:32:08Z', 'updatedAt': '2015-09-27T14:19:46Z', 'title': 'COMPAT: Add Google BigQuery support for python 3 #11094', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11110', 'bodyText': 'closes #11094\\nAdds gbq support for python 3.4', 'comments': {'totalCount': 16, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Python 3.5 unit tests are failing, waiting on #11097\\nSee https://travis-ci.org/parthea/pandas/builds/80424117'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea yes, 3.5 are ok'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"Some gbq unit tests are failing on Python 3.4\\nI think I need to remove the bq command line module from test_gbq.py, as it doesn't support python 3. I'm going to create a new PR for adding ability to create and delete datasets using create_dataset() and delete_dataset() and at the same time remove the bq command line tool.\\nERROR: Python 3 is not supported by the Google Cloud SDK.  Please use a Python 2.x version that is 2.6 or greater.\\n\\nIf you have a compatible Python interpreter installed, you can use it by setting the CLOUDSDK_PYTHON environment variable to point to it.\\nERROR: Python 3 is not supported by the Google Cloud SDK.  Please use a Python 2.x version that is 2.6 or greater.\\n\\nIf you have a compatible Python interpreter installed, you can use it by setting the CLOUDSDK_PYTHON environment variable to point to it.\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ERROR\\nERROR: Python 3 is not supported by the Google Cloud SDK.  Please use a Python 2.x version that is 2.6 or greater.\\n\\nIf you have a compatible Python interpreter installed, you can use it by setting the CLOUDSDK_PYTHON environment variable to point to it.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@parthea gr8. let me know when ready for review.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'whatsnew note as well.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'add a whatsnew entry (in the gbq sub-section that you did before).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Can I wait for #11121 to be merged?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok, rebase this, ping when green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@aaront @parthea can you update this?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"@aaront You are welcome to work on this if you have time, but don't feel obligated. I plan to commit a new version tomorrow evening.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': \"@parthea @jreback Yeah, I'll rebase and run the tests.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': '@parthea @jreback  Rebased & ran tests in 3.4, 3.5, and 2.7. All passing.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'aaront'}, 'bodyText': 'Travis is green'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you certify that all tests are passing on py2 and py3? (as we skip the actual tests on travis).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Ready for review. All tests pass locally.\\nPython 2.7 test results\\nnosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\npandas.io.tests.test_gbq.test_generate_bq_schema_deprecated ... ok\\n\\n----------------------------------------------------------------------\\nRan 54 tests in 346.397s\\n\\nOK\\n\\nPython 3.4 test results\\nnosetests test_gbq.py -v\\ntest_should_be_able_to_get_a_bigquery_service (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_results_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_schema_from_query (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_get_valid_credentials (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_should_be_able_to_make_a_connector (pandas.io.tests.test_gbq.TestGBQConnectorIntegration) ... ok\\ntest_bad_project_id (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_bad_table_name (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_column_order_plus_index (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_download_dataset_larger_than_200k_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_index_column (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_malformed_query (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_arbitrary_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_empty_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_false_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_null_timestamp (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_timestamp_unix_epoch (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_true_boolean (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_floats (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_integers (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_should_properly_handle_valid_strings (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_unicode_string_conversion_and_normalization (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_zero_rows (pandas.io.tests.test_gbq.TestReadGBQIntegration) ... ok\\ntest_read_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_booleans_as_python_booleans (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_floats_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_integers_as_python_floats (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_strings_as_python_strings (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_should_return_bigquery_timestamps_as_numpy_datetime (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_that_parse_data_works_properly (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_should_fail_if_invalid_table_name_passed (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_to_gbq_with_no_project_id_given_should_fail (pandas.io.tests.test_gbq.TestReadGBQUnitTests) ... ok\\ntest_create_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_create_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_dataset_exists (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_delete_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_generate_schema (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_google_upload_errors_should_raise_exception (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_dataset (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_list_table_zero_results (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_table_does_not_exist (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_append (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_fail (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\ntest_upload_data_if_table_exists_replace (pandas.io.tests.test_gbq.TestToGBQIntegration) ... ok\\npandas.io.tests.test_gbq.test_requirements ... ok\\npandas.io.tests.test_gbq.test_generate_bq_schema_deprecated ... ok\\n\\n----------------------------------------------------------------------\\nRan 54 tests in 346.396s'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks @parthea @aaront'}}]}}, {'createdAt': '2015-09-15T13:55:12Z', 'updatedAt': '2015-09-15T13:58:56Z', 'title': 'DOC: fix ref to template for plot accessor', 'mergedBy': {'login': 'jorisvandenbossche'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11104', 'bodyText': '', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-15T12:38:20Z', 'updatedAt': '2015-10-03T22:24:44Z', 'title': 'ENH: Data formatting with unicode length', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 11}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11102', 'bodyText': 'Closes #2612. Added display.unicode.east_asian_width options, which calculate text width considering East Asian Width. Enabling this option affects to a performance as width must be calculated per characters.\\nCurrent results (captured)\\n\\n\\n Basic impl and test\\n Series / DataFrame truncation\\n Perf test\\n Doc / Release note', 'comments': {'totalCount': 23, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"How much slower is this than the default behavior? My guess is that anyone who uses these characters will want this.\\nUsually we're not printing enough data to the screen for performance on this sort of stuff to matter.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': 'east_asian_len calculation can be reduced to a 3-element set membership testing. Ever so slightly faster but probably too micro to matter.  I think ambiguous characters need special handling (?).'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"OK, this PR should work all cases which I'm aware of. Appreciated if anyone provide further test cases if any concerns.\\n@shoyer Yes, east-asian prefer this to be default True. But it is almost 2 times slower in below case.\\n\\nDataFrame contains 10000 data, 100 rows * 100 columns, each item contains 10 Unicode chars\\nDisplay options are default:\\n\\npd.options.display.max_rows: 60\\npd.options.display.max_columns: 20\\n\\n\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nchars = list(u'あいうえおかきくけこさしすせそたちつてとなにぬねの')\\n\\ndef rand_jp(x):\\n    return ''.join(np.random.choice(chars) for _ in range(x))\\n\\ndf = pd.DataFrame(np.empty((100, 100)))\\ndf = df.applymap(lambda x: rand_jp(10))\\n\\n%timeit unicode(df)\\n# 10 loops, best of 3: 177 ms per loop\\n\\n# Enable Unicode handling\\npd.options.display.unicode.east_asian_width = True\\n\\n%timeit unicode(df)\\n# 1 loops, best of 3: 381 ms per loop\\n\\nThe affect is almost the same as all ascii (same condition except for characters are all ascii):\\n\\nDefault: 10 loops, best of 3: 131 ms per loop\\nEnable Unicode handling: 1 loops, best of 3: 302 ms per loop\\n\\n@kawochen I may not properly understand, but reducing East Asian Width category will not affect to the performance because dict lookup is O(1).\\n\\nI think ambiguous characters need special handling (?).\\n\\nDo you have any idea about affected characters and special handling logic? My concern is these characters can not be aligned properly even if we tried so.\\nCC:  @ayapi\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': 'If I understand correctly, because pandas does not actually print every element of large dataframes, so printing a larger DataFrame would be the same speed?\\nHow does this patch effect the speed of printing Unicode text if it only contains ASCII characters?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': '@shoyer Correct. Larger data can be printed almost the same speed. Perf is not affected by data size once it exceeds max_columns and max_rows.\\nResult for all ascii are described in above. Almost 2 times longer, because the logic is the same. It can be short-passed if we can distinguish whether input has 2 bytes char or not including symbols. Is it possible using any built-in?.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': 'I would probably try tweaking the string length check and seeing if a simple variant can give you a speed up. But generally this is pretty reasonable already. Few dataframes will show this many strings.\\nOn Sat, Sep 26, 2015 at 8:16 PM, Sinhrks notifications@github.com wrote:\\n\\n@shoyer Correct. Larger data can be printed almost the same speed. Perf is not affected by data size once it exceeds max_columns and max_rows.\\nResult for all ascii are described in above. Almost 2 times longer, because the logic is the same. It can be short-passed if we can distinguish whether input has 2 bytes char or not including symbols. Is it possible using any built-in?.\\nReply to this email directly or view it on GitHub:\\n#11102 (comment)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"@sinhrks Yes the optimization I mentioned is so minor I don't know why I brought it up.  It makes east_asian_len take about 30~40% less time to run, but I don't think this is where time is spent anyways.  Regarding ambiguous characters I am not sure what can be done (perhaps config/options)?  Also wanted to mention in passing that printing ⟼ (neutral) would still not print as one would hope.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"@jreback I've refactored a little based on your comments. I've created TextAdjustment class which has a set of east-asian depending function rather than defining separate pad function. Because separate definitions may causes unexpected results in future (e.g. using pad for east-asian with normal len ). If looks OK, I'll squash.\\n@kawochen OK, I leave current east_asian_len.\\nAlso, please provide the list of ambiguous characters and its width. As long as I understand, these ambiguous character widths are not integral multiple, thus these cannot be aligned by padding with white spaces. I don't think we can fix it because it is unicode spec.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'looks nice!\\ncan u run a perf check on relevant benchmarks to assert its about the same as current?\\nadd a release note and can do for 0.17.0'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': '@sinhrks  ambiguous characters are either wide or narrow. see here for list http://unicode.org/reports/tr11-2/'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"@kawochen Ok. Backed to first discussion, it can't be support it without clarifying the logic. I can't find the logic per characters from your link.... Maybe I misunderstood sonething? Pls provide actual code works as your expectation.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"@sinhrks There is no per character logic.  All ambiguous characters are narrow on my terminal, but that's just my settings.  Users should know whether it should be treated as wide or narrow, which depends on where the characters are being printed, so making it configurable might make sense.  I don't think that can be figured out from within Python.  You can try printing the characters in the list.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Thanks. Maybe I could understand a little. From the link:\\n\\nWhen mapping Unicode to East Asian legacy character encodings\\n\\nWide Unicode characters always map to fullwidth characters.\\nNarrow (and neutral) Unicode characters always map to halfwidth characters.\\nHalfwidth Unicode characters always map to halfwidth characters.\\nAmbiguous Unicode characters always map to fullwidth characters.\\n\\nWhen mapping Unicode to non-East Asian legacy character encodings\\n\\nWide Unicode characters do not map to non-East Asian legacy character encodings.\\nNarrow (and neutral) Unicode characters always map to regular (narrow) characters.\\nHalfwidth Unicode characters do not map.\\nAmbiguous Unicode characters always map to regular (narrow) characters.\\n\\n\\nWhen mapping Unicode to East Asian legacy character encodings: Ambiguous should be handled ad full-width (length=2). It can be covered by the impl added by the PR (display.unicode.east_asian_width=True) .\\nWhen mapping Unicode to non-East Asian legacy character encodings:  Because full width are not mapped (cannot appear), all characters including ambiguous can be regarded as half-width (length=1). It should be corresponding to the current default display.unicode.east_asian_width=False.\\nWhat the situation requires a separate option only for Ambiguous?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"@sinhrks when we are not mapping to legacy encodings. For example in my terminal Chinese characters are twice as wide as ambiguous characters.  Whether it should be 1 or 2 depends on where and how the data will be displayed. In the unlikely case when we wanted it to look pretty in Arial then we'd print tabs to align stuff.  In mono space fonts it's easier so we can use spaces.  In modern terminals ambiguous characters are usually narrow.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': '@kawochen Can you describe with screenshots and code which I can confirm on my terminal?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': \"In [2]: print('中文\\\\n\\\\u00A1\\\\u00A1ab')\\nIn [4]: east_asian_width('\\\\u00A1')\\nOut[4]: 'A'\\n\\n\\n\\nSo if I have all of those in a DataFrame, I might have too much padding.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Understood, how about the name unicode.ambiguous_as_wide with default False (length=1) if it is popular in current terminal. If specified True, its length is regarded as 2.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Updated to add release note and unicode.ambiguous_as_wide. \"Ambiguous\" characters cannot be aligned properly on Sphinx (Jupyter).\\nterminal\\nLatter case cannot be aligned because of the mismatch between terminal and pandas option.\\n\\nSphinx/Jupyter\\nGoing to add note to say \"This should be aligned properly in terminal which uses monospaced font.\"\\n\\nI\\'ll update perf comparison tomorrow.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks ok some minor doc fixes. merge when ready.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Thanks, updated doc and asv result attached. Results looks random.\\nAll benchmarks:\\n\\n    before     after       ratio\\n  [5049b5  ] [53ac28  ]\\n    19.50ms    22.42ms      1.15  frame_methods.frame_repr_tall.time_frame_repr_talld\\n     1.30ms     1.43ms      1.10  groupby.series_value_counts.time_value_counts_int64\\n     3.41μs     3.74μs      1.10  indexing.indexing_frame_get_value.time_indexing_frame_get_value'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"@jreback Could you merge this when you prepare RC?\\nAll: I'm willing to fix if anything is pointed out during RC.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks (and @kawochen ) this is fantastic. quite a bit of work and lots of tests yeh!'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': \"Found doc layout can differ by environment. I'll update notes to describe it.\\n1st looks OK, 2nd NG\\n\\n1st looks NG, 2nd OK\"}}]}}, {'createdAt': '2015-09-15T02:29:11Z', 'updatedAt': '2015-10-11T15:43:28Z', 'title': 'ENH Add user defined function support to read_gbq', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin', 'company': None}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11099', 'bodyText': \"Added support for BigQuery's new user defined function (UDF) feature. See https://cloud.google.com/bigquery/user-defined-functions#api for details.\", 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this is not very intuitive in the API. can you show an example use case?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin'}, 'bodyText': \"Google has added support for JS based user defined functions. They essentially perform a map operation on rows fed into them. You could use one, for example, to parse a useragent out to browser/os/version/etc. They're stored as .js files in Google Cloud Storage, and in order to use them, the URI for the source must be included in the query data. All of the parsing happens in the BigQuery engine. This pull enables a user to include a list of source URIs for any UDFs used in their queries.\\nThe web client has added a control to add these URIs, so this shouldn't be a completely foreign thing to BQ users.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'what you added does not appear to actually do anything or be passed any where. how can one tell if this doing anything?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin'}, 'bodyText': \"When a list of URIs is passed to read_gbq, the userDefinedFunctionResources object is added to the job data as a list of resourceUri objects per the spec in the BQ API docs. If the user doesn't include any URIs, defaulting to the empty list bypasses adding this data to the job data.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin'}, 'bodyText': 'Here is an example of a UDF source file. This one implements a function uaDecode based on UAParser.js taking a single column user_agent and parsing it to return a single column browser.\\nTo use it you include the URI in the job config and submit a query like this\\n    select\\n        browser\\n    from\\n        uaDecode(select userAgent from logs)\\nEDIT: The BQ specific parts are the very top and very bottom. The middle is just UAParser.js'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I have no problem with the functionality\\nbut he API\\nwould rather have u pass maybe a parm, call it query that is directly passed in\\neg the user would format it (in this case would be a dict of list of dicts)\\nvery JSONy'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin'}, 'bodyText': \"Are you suggesting the user should create the entire userDefinedFunctionResources object and pass something like this to read_gbq?\\n[{'resourceUri':'uri1'},\\n{'resourceUri':'uri2'},\\n{'resourceUri':'uri3'},]\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'yep, I would simply have the user format these.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'cgrin'}, 'bodyText': \"I suppose that would enable the inline udf functionality at the same time, but will lead to repeated code for the user. What about changing the parameter name to udf_resources and adding inlineCode objects when the string doesn't begin with gs://? The read_gbq method is already building the job_data json object, so it's not unprecedented to obscure the details of the BQ api.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I would like to see some more use on this before we starting adding, IMHO, very specific keyword functionaility. This is easily added by the user.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'I am currently using user defined functions(UDF) in pandas 0.17.0. I just enter the UDF as part of the BigQuery query string. Its possible that users can just use the read_gbq() function as is.\\nI used the following link as a reference to construct my query string:\\nhttp://stackoverflow.com/questions/32208958/cannot-pass-input-field-of-repeated-record-type-into-bigquery-udf'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"I think this is better left to a cookbook (or documentation recipe). @cgrin if you'd like to do that, or propose a more generic API for this, pls reopen\"}}]}}, {'createdAt': '2015-09-14T23:54:06Z', 'updatedAt': '2015-09-17T17:34:49Z', 'title': 'DOC: Fixed outdated doc-string, added missing default values, added missing optional parameters for some io classes', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11098', 'bodyText': 'Some doc-strings are outdated, in a sense that there are missing newly added optional parameters and default values.', 'comments': {'totalCount': 5, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@terrytangyuan The changes look good, but, the data.py is now maintained at https://github.com/pydata/pandas-datareader. So maybe you can do a PR for that file there'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"@jorisvandenbossche Nice catch! Thanks for checking. I've made the changes. I'll submit another PR to make changes on the new file.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jorisvandenbossche On green now. Can you merge?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'Thanks @kawochen Fixed and on green now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I guess this is fine. @terrytangyuan prob strip off the stuff for pandas-datareader and submit there as well.'}}]}}, {'createdAt': '2015-09-14T20:27:25Z', 'updatedAt': '2015-09-21T15:33:55Z', 'title': 'ERR/API: Raise NotImplementedError when method is not implemented (issue: 7692)', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11095', 'bodyText': 'Fixed #7692', 'comments': {'totalCount': 12, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback Does this need a whatsnew note or test? Would the test to check whether this raises NotImplementedError?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'tests'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback  thanks could you check the update again?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'Thanks. Done @jreback'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'On green. Could you merge? @jreback'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@terrytangyuan just a single ping is enough.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"@jreback I apologize for the repetitive ping and thanks for the feedback. I've made the changes.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a whatsnew note (Bug fixes) for 0.17.0'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback  Done. On green now. Thanks.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'Thanks for the review @jreback . Fixed and on green now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@kawochen @jreback  Thanks. Revised. On green now.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 597aa42\\nthanks!'}}]}}, {'createdAt': '2015-09-14T15:25:50Z', 'updatedAt': '2015-09-16T23:02:45Z', 'title': 'TST: More thorough test to check correctness of to_html_unicode', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11093', 'bodyText': 'The current test for test_to_html_unicode only checks for whether the function works but does not check for its correctness.', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'On green. Can this be merged?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'looks good. will merge soon.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-14T12:21:30Z', 'updatedAt': '2015-09-14T13:10:51Z', 'title': 'DOC: Update \"enhancing perf\" doc based on #10953', 'mergedBy': {'login': 'sinhrks'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11092', 'bodyText': 'Updated \"enhancing performance\" doc based on #10953.\\nCC @sklam', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm\\nmerge when ready'}}]}}, {'createdAt': '2015-09-13T20:15:02Z', 'updatedAt': '2015-09-14T20:23:13Z', 'title': 'TST: Fix skipped unit tests in test_ga. Install python-gflags using p…', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11091', 'bodyText': 'closes #11090\\nThis commit should resolve the following error in the travis build log.\\n-------------------------------------------------------------\\n#15 nose.failure.Failure.runTest: need httplib2 and auth libs\\n-------------------------------------------------------------\\n\\nValid Google credentials are required to run the ga unit tests.', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': 'Here is the output from my local test session:\\ntest_getdata (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_iterator (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_remove_token_store (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_segment (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_v2_advanced_segment_format (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_v2_dynamic_segment_format (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_v3_advanced_segment_common_format (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_v3_advanced_segment_weird_format (pandas.io.tests.test_ga.TestGoogle) ... ok\\ntest_v3_advanced_segment_with_underscore_format (pandas.io.tests.test_ga.TestGoogle) ... ok\\n\\n----------------------------------------------------------------------\\nRan 9 tests in 3.370s\\n\\nOK'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'parthea'}, 'bodyText': \"I've updated the tests as per the comments. The import error is gone in my latest commit. See job https://travis-ci.org/parthea/pandas/jobs/80171740\\nThe new reason for skipped tests is authentication error as the tests require valid google credentials.\\n#16 pandas.io.tests.test_ga.TestGoogle.test_getdata: authentication error\\n#17 pandas.io.tests.test_ga.TestGoogle.test_iterator: authentication error\\n#18 pandas.io.tests.test_ga.TestGoogle.test_segment: authentication error\\n\\nI've confirmed that the tests pass with valid credentials.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-13T15:27:25Z', 'updatedAt': '2015-09-13T18:27:40Z', 'title': 'TST: make sure to close stata readers', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11088', 'bodyText': '', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'should fix the Resource Warning sometimes happening on Travis'}}]}}, {'createdAt': '2015-09-13T14:17:11Z', 'updatedAt': '2015-09-13T21:43:33Z', 'title': 'DOC: Comparison with SAS', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11087', 'bodyText': \"closes #11075\\nxref #4052\\nThere doesn't seem to be a straightforward way to syntax highlight the SAS code, so it will format as generic code blocks.\", 'comments': {'totalCount': 11, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"lots of lexers here: http://pygments.org/docs/lexers/#lexers-for-the-r-s-languages\\nisn't it actual S syntax?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'No, SAS has its own syntax.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you render and post this page'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can also add to the highlites section in v0.17.0 (and add to the release.rst the same line).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': \"Here's the rendered output - for some reason in-line code segments don't get rendered correctly locally for me (on all doc pages).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 looks pretty good.\\nI need a more full look, but @jorisvandenbossche ?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@chris-b1 Really cool!!\\njust added some minor comments'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': '@jorisvandenbossche - pushed changes for your comments, thanks for taking a look.  Rendered output above is also updated.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@chris-b1 just a couple of minor edits / suggestions.\\nreally nice!\\nping when ready for merge.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1'}, 'bodyText': 'Thanks!  @jreback pushed changes for your notes, should be good to merge.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'awesome. pls have a look once they are built and do a follow-up if needed!'}}]}}, {'createdAt': '2015-09-13T00:28:26Z', 'updatedAt': '2019-02-14T05:23:07Z', 'title': 'BUG: Fixed bug in groupby.std changing target column when as_index=False', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11085', 'bodyText': 'Fixed #10355', 'comments': {'totalCount': 14, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"this is not going to be oerformant\\nis there a reason you didn't follow the soln from the issue?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'I think the code structure changed somewhat. I couldn\\'t figure out how to\\nuse it.\\nOn Sep 12, 2015 8:54 PM, \"Jeff Reback\" notifications@github.com wrote:\\n\\nthis is not going to be oerformant\\nis there a reason you didn\\'t follow the soln from the issue?\\n—\\nReply to this email directly or view it on GitHub\\n#11085 (comment).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"They are refactored to _cython_functions I guess. I tried to do similar thing using cython but failed. Maybe this is good for now and I'll figure out how to do it in a separate PERF PR some time later?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'no that will just change it again'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback any updated soln for the soln mentioned in issue? I tried similar way and failed, maybe I missed something.  Thanks.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'you can just do the .var, then the np.sqrt only applies to the action columns.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback  Could you take a look at the new push? Did I get what you meant?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"no u can't use the lamba\\nthat goes to a python path\\ncompute var then apply the sqrt\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'How do I apply it only to action columns without lambda?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'Since the result of applying .var would be a DataFrame and it lost its original groups, which is why the original implementation causes this bug.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this needs a similar approach to what I suggest (and like what .first does)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'So I removed the original definition of std() and the diff is as follows:\\n@@ -828,6 +828,8 @@ class GroupBy(PandasObject):\\n                               numeric_only=False, _convert=True)\\n     last = _groupby_function(\\'last\\', \\'last\\', _last_compat, numeric_only=False,\\n                              _convert=True)\\n+    def std(self):\\n+        return self._cython_agg_general(\\'std\\')\\n\\n     def ohlc(self):\\n         \"\"\"\\n@@ -1485,6 +1487,8 @@ class BaseGrouper(object):\\n             \\'f\\': lambda func, a, b, c, d: func(a, b, c, d, 1)\\n         },\\n         \\'last\\': \\'group_last\\',\\n+        \\'std\\': {\\'name\\' : \\'group_var\\',\\n+                \\'f\\' : lambda func, a: np.sqrt(func(a))}\\n     }\\n\\nStill couldn\\'t get it working. (tried other similar things too) The code structure of groupby.py must have changed a lot since the issue has been posted. I think this PR at least fixed the bug. I won\\'t be the best person to fix the perf issues.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"then let's close this for now. this is non performant.\"}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'xieyuheng'}, 'bodyText': '#25315'}}]}}, {'createdAt': '2015-09-12T23:25:09Z', 'updatedAt': '2015-09-13T00:23:54Z', 'title': 'DOC: fix plot submethods whatsnew example', 'mergedBy': {'login': 'shoyer'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer', 'company': '@google '}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11083', 'bodyText': 'This was not building properly.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'shoyer'}, 'bodyText': \"OK, not going to wait for Travis on this one because Travis doesn't even bother to build the docs.\"}}]}}, {'createdAt': '2015-09-12T22:46:57Z', 'updatedAt': '2015-09-12T23:36:12Z', 'title': 'CI: support *.pip for installations', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 7}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11081', 'bodyText': '', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-12T18:31:35Z', 'updatedAt': '2015-09-14T22:11:28Z', 'title': 'BUG: Fix Series nunique groupby with object dtype', 'mergedBy': {'login': 'cpcloud'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud', 'company': '@twosigma'}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11079', 'bodyText': 'closes #11077', 'comments': {'totalCount': 11, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'cc @behzadnouri\\nor you want to use do a factorize?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': 'diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py\\nindex f34fd6e..39be706 100644\\n--- a/pandas/core/groupby.py\\n+++ b/pandas/core/groupby.py\\n@@ -2565,13 +2565,21 @@ class SeriesGroupBy(GroupBy):\\n         ids, _, _ = self.grouper.group_info\\n         val = self.obj.get_values()\\n\\n-        sorter = np.lexsort((val, ids))\\n+        try:\\n+            sorter = np.lexsort((val, ids))\\n+        except TypeError:\\n+            val, _ = algos.factorize(val, sort=False)\\n+            sorter = np.lexsort((val, ids))\\n+            isnull = lambda a: a == -1\\n+        else:\\n+            isnull = com.isnull\\n+\\n         ids, val = ids[sorter], val[sorter]\\n\\n         # group boundries are where group ids change\\n         # unique observations are where sorted values change\\n-        idx = com._ensure_int64(np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]])\\n-        inc = com._ensure_int64(np.r_[1, val[1:] != val[:-1]])\\n+        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\\n+        inc = np.r_[1, val[1:] != val[:-1]]\\n\\n         # 1st item of each group is a new unique observation\\n         mask = isnull(val)'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud'}, 'bodyText': \"@behzadnouri If you submit that as a PR to my fork, I'll happily merge it in\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': 'the error says TypeError: merge sort not available for item 0 even though numpy does have merge sort for objects (both np.argsort and np.sort). so it may be a bug on numpy side. so, it is better to implement try catch in case future numpy releases do implement lexsort for this case\\nonce, values are factorized, original values are not needed and it is more efficient to work with integer factors, as long as isnull function is adjusted accordingly.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud'}, 'bodyText': '@jreback getting a somewhat strange travis failure here that seems unrelated to my change: https://travis-ci.org/cpcloud/pandas/jobs/80036566'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud'}, 'bodyText': 'any ideas what that might be?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'hmm someone else reported that as well\\ntry restarting it and see'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud'}, 'bodyText': 'this is passing on my travis ci fork'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'couple comments, squash and merge ok'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@cpcloud can you update (e.g. revert those 2 lines). ok to merge on green after that.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ty sir!'}}]}}, {'createdAt': '2015-09-12T18:18:15Z', 'updatedAt': '2015-09-12T18:20:20Z', 'title': 'updateing my fork', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle', 'company': None}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11078', 'bodyText': '', 'comments': {'totalCount': 0, 'edges': []}}, {'createdAt': '2015-09-12T16:09:22Z', 'updatedAt': '2015-09-15T13:29:18Z', 'title': 'ENH Add check for inferred compression before `get_filepath_or_buffer`', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover', 'company': '@civisanalytics '}, 'files': {'totalCount': 5}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11074', 'bodyText': 'When reading CSVs, if compression=\\'infer\\', check the input before calling get_filepath_or_buffer in the _read function. This way we can catch compresion extensions on S3 files. Partially resolves issue #11070 .\\nChecking for the file extension in the _read function should make the checks inside the parsers redundant. When I tried to remove them, however, I discovered that there\\'s tests which assume the parsers can take an \"infer\" compression, so I left their checks.\\nI also discovered that the URL-reading code has a test which reads a URL ending in \"gz\" but which appears not to be gzip encoded, so this PR attempts to preserve its verdict in that case.', 'comments': {'totalCount': 11, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls change tests which are incorrect as well\\niow this should cause them to fail'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"I made this PR so that it didn't break any tests. Are the parsers ever accessed outside of the _read function? Do they need to be able to infer the compression type on their own? If not, I can remove that code from the parsers and change the tests.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'the infer param can be moved higher up in the stack (eg in the get_filepath_or_buffer) - makes the readers simpler in that respect'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"Found it. I actually didn't need to change any tests. Now the only check for file extensions happens in the _read function, instead of separately inside each of the two parsers. By the time the parsers get called, any compression inference has already taken place.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'gr8'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Added a test using the new files in s3://pandas-test/.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Should I do anything else for this PR?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you add a whatsnew note for this'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls rebase. ping when green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"@jreback , green! I found had to tweak get_filepath_or_buffer with an extra check for 'infer' compression.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-12T13:51:09Z', 'updatedAt': '2015-09-14T22:32:55Z', 'title': 'ENH Enable streaming from S3', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover', 'company': '@civisanalytics '}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11073', 'bodyText': \"File reading from AWS S3: Modify the get_filepath_or_buffer function such that it only opens the connection to S3, rather than reading the entire file at once. This allows partial reads (e.g. through the nrows argument) or chunked reading (e.g. through the chunksize argument) without needing to download the entire file first.\\nI wasn't sure what the best place was to put the OnceThroughKey. (Suggestions for better names welcome.) I don't like putting an entire class inside a function like that, but this keeps the boto dependency contained.\\nThe readline function, and modifying next such that it returns lines, was necessary to allow the Python engine to read uncompressed CSVs.\\nThe Python 2 standard library's gzip module needs a seek and tell function on its inputs, so I reverted to the old behavior there.\\nPartially addresses #11070 .\", 'comments': {'totalCount': 20, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"New commit addresses your comments. I'll squash once the review is complete.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'need to change class name references\\nadd a test using chunk size'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"Will do. BTW -- reading with nrows or chunksize worked before, too. The difference now is that we can do it without having to download the entire file. I don't know how to write a unit test for that -- I tested it myself by looking at whether it's 100 ms or 20 s to read part of a large file. In this case, I think it's okay just to check for functionality. The way the code is written, the only way we'd lose partial reads without breaking tests is for downstream code to suck in the entire file before feeding it to the parser. (This happens right now for bz2 files without the changes in #11072 .)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"in that case why don't u make an asv benchmark and read say 100 lines or something\\nthrn we would have a perf regression if this was changed (don't use a really long file but should take maybe 1s) to download the entire thing\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'I think it would need about 1 MM rows, maybe more, to be able to reliably separate performance regression from variance in network performance. I\\'d also need a public S3 bucket which could host such a thing, which I don\\'t have. Do you know what the \"s3://nyqpug\" bucket is? For the unit tests, it would be helpful to be able to host gzip and bzip2 files there as well.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I have a pandas bucket where can put stuff\\npush the files up (to your branch and give me a pointer and I will put them up)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"Thanks! I'll ping you when that's done.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Test using chunksize added.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': '@jreback , I created compressed versions of the \"tips.csv\" table for unit tests and large tables of random data for performance regression tests. All are in the commit at stephen-hoover@36b5d3a .'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"all uploaded here (its a public bucket) https://s3.amazonaws.com/pandas-test/\\nthough let's make the big ones only in the perf tests and/or slow\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Thanks! Could you put the file from \"s3://nyqpug/tips.csv\" in that bucket as well? The tests will be simpler if everything\\'s in the same place.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'done'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Are the files set publicly readable? I can see the contents of the bucket, but when I try to read a file, I get a \"Forbidden\" error. That happens with the aws CLI as well as with read_csv.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'should be fixed now. annoying that I had to do that for each file :< individually'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"Thanks. The S3 tests are more extensive now. I'll start working on performance tests, but I won't be able to have those until tomorrow afternoon.\\nI'll want to either rebase this PR on a merged #11072 or vice-versa. After that PR merges, the (Python 3) C parser will be able to read bz2 files from S3, and I can change the new tests to reflect that.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"With #11072 merged in, I've updated the tests to reflect the fact that the Python 3 C parser can now read bz2 files from S3. I've also added a set of benchmarks for reading from S3 in the asv_bench/benchmarks/io_bench.py file. The benchmarks read 10 rows of the 100,000 row x 50 column table of random noise in the pandas-test bucket. On the current (9ef8534) master branch, benchmarks are\\nPython 2:\\n               ============= ======== ========\\n               --                  engine     \\n               ------------- -----------------\\n                compression   python     c    \\n               ============= ======== ========\\n                    None      27.90s   27.46s \\n                    gzip      13.20s   13.13s \\n                    bz2       20.43s    n/a   \\n               ============= ======== ========\\n\\nPython 3:\\n               ============= ======== ========\\n               --                  engine     \\n               ------------- -----------------\\n                compression   python     c    \\n               ============= ======== ========\\n                    None      27.16s   27.34s \\n                    gzip      14.42s   13.82s \\n                    bz2       11.69s   11.77s \\n               ============= ======== ========\\n\\nand on this PR, the benchmarks are\\nPython 2:\\n               ============= ========== ==========\\n               --                    engine       \\n               ------------- ---------------------\\n                compression    python       c     \\n               ============= ========== ==========\\n                    None      208.45ms   486.83ms \\n                    gzip       12.79s     13.10s  \\n                    bz2        20.39s      n/a    \\n               ============= ========== ==========\\n\\nPython 3:\\n               ============= ========== ==========\\n               --                    engine       \\n               ------------- ---------------------\\n                compression    python       c     \\n               ============= ========== ==========\\n                    None      207.24ms   484.90ms \\n                    gzip      249.83ms   363.71ms \\n                    bz2       608.92ms   592.51ms \\n               ============= ========== ==========\\n\\nNote that in the > 1 s benchmarks, all of the time is taken in downloading the entire file from S3. Times will vary with network speed. It should be obvious if a future change forces read_csv to ingest the entire file from S3 again.\\nASV is a really keen tool. I'm happy I found it.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok, this seems reasonable. pls add a note in whatsnew. ping when all green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': '@jreback , green!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you rebase?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank you sir!'}}]}}, {'createdAt': '2015-09-12T13:32:12Z', 'updatedAt': '2015-09-14T20:12:22Z', 'title': 'ENH Enable bzip2 streaming for Python 3', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover', 'company': '@civisanalytics '}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11072', 'bodyText': 'This is the one modification related to issue #11070 which affects non-S3 interactions with read_csv. The Python 3 standard library has an improved capability for handling bz2 compression, so a simple change will let read_csv stream bz2-compressed files.', 'comments': {'totalCount': 8, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'tests!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"I added a test for reading from an open file with the C parser. It fails on the master branch and passes here. How's that?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'do you have exactly the same deps'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Yes, exactly the same dependencies. This PR works because the standard library bz2 module was upgraded to accept file pointers in 3.3.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok, this looks good. pls add a note in whatsnew for 0.17.0 (just released the rc1 yesterday, but this is ok). reference both the original issue and this PR number I think.\\nsquash & ping when green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': \"Note added. It doesn't look like anything else references a PR; should I leave that reference in?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': '@jreback , tests are green!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-12T13:29:25Z', 'updatedAt': '2015-09-14T20:09:26Z', 'title': \"ENH Recognize 's3n' and 's3a' as an S3 address\", 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover', 'company': '@civisanalytics '}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11071', 'bodyText': 'This PR allows read_csv to recognize that \"s3n://\" designates a valid AWS S3 address. Partially addresses issue #11070 .', 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'Fixed. I also realized (reading the links I added for issue #11070 ) that \"s3a\" is a valid S3 designator as well, so I added that too.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': 'The one test failure appears unrelated to the code change in this PR:\\nFAIL: test_numeric_column_names (pandas.io.tests.test_stata.TestStata)\\n----------------------------------------------------------------------\\nTraceback (most recent call last):\\n  File \"/home/travis/build/pydata/pandas/pandas/io/tests/test_stata.py\", line 480, in test_numeric_column_names\\n    original.to_stata(path)\\n  File \"/home/travis/miniconda/envs/pandas/lib/python3.3/contextlib.py\", line 55, in __exit__\\n    next(self.gen)\\n  File \"/home/travis/build/pydata/pandas/pandas/util/testing.py\", line 2021, in assert_produces_warning\\n    % extra_warnings)\\nAssertionError: Caused unexpected warning(s): [\\'ResourceWarning\\'].'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"no it's just hitting the actual error there\\nyou have something failing\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'add a whatsnew note in 0.17.0 (enhancements section)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'stephen-hoover'}, 'bodyText': '@jreback , green here too!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-12T10:14:21Z', 'updatedAt': '2016-01-08T23:59:46Z', 'title': 'DOC/CI: include api docs on travis', 'mergedBy': None, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 11}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11069', 'bodyText': 'Continued in #12002\\n\\nCloses #6100\\nCloses #3800\\nJust to check how this takes on travis', 'comments': {'totalCount': 13, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'might simply want to create a doc-build (e.g. this issue #3800)'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@jreback yes, I know, but I was first just experimenting a bit to get it working (not very familiar with travis/bash syntax details). Once I have something working the idea was indeed to put it in a separate travis build.\\nThe problem I have now, is that for some reason make.py does not output its progress to the standard output of travis (which is the case if you run it locally in a terminal). And then, because of this, travis thinks nothing is happening anymore and aborts the build ..\\nDo you know a solution for this?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'http://docs.travis-ci.com/user/build-timeouts/\\nmight work'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"is make.py writing to stderr? then could just redirect\\nmaybe it's buffering?\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'https://coderwall.com/p/wws2uq/have-travis-ci-test-your-sphinx-docs\\n?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'https://github.com/Syntaf/travis-sphinx'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'make.py calls sphinx-build, which AFAIU outputs to both stdout as stderr? And also, I would think travis outputs both stdout as stderr in the logs? But it is not really clear to me how it is specified the output of the doc build is logged in a file that is then later shown, and why for tests it is outputted directly.\\nThe travis_wait is probably not a solution, as this just raises the timeout from 10 to 20 minutes, but I think the doc build will even take longer.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"aha, hadn't notices that there is also a separate run_build_docs.sh script next to build_docs.sh, and that is of course capturing that output ..\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@jreback With the last commit the full docs build OK, although it took 37 minutes. So I put it in a separate travis build, and include the full api docs (there is still a temporary commit to let the others fail).\\nCan you have a look at the travis ci changes? (it also included somes hacks to numpydoc to prevent some warnings, they are not very nice hacks, but the warnings had to be eliminated to get it running on travis (otherwise the log was too big ..)).'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche all looks cool to me then. I know the doc-build takes a while but so what its in the  lower section.\\nnote - the builds run in the order that they are defined in the top section (no matter if they are in th ebottom section or not). So you prob want to have the doc-build run last? (I think you have it that way). That way other workers can finish the other jobs. we can have 5 simultaneously i think.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'The problem with this approach is that the time it takes to build on Travis seems rather variable (maybe depending on its load?). In any case, sometimes it succeeded to finish it in ca 45 min, but other times it goes up to 50min and then travis aborts it.\\nBut now trying if parallel build speeds it up'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche closing, but reopen if updating\\nin theory this looks like a great idea....but yeh it takes quite some time to build the API docs'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Continued in #12002'}}]}}, {'createdAt': '2015-09-12T09:05:48Z', 'updatedAt': '2015-09-12T10:45:07Z', 'title': 'DOC: Re-organize whatsnew', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11068', 'bodyText': 'Reorganized release note for 0.17. There are some descriptions which looks the same level, but one in a highlights and the other is not.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok!'}}]}}, {'createdAt': '2015-09-11T22:33:26Z', 'updatedAt': '2015-09-11T23:05:56Z', 'title': 'DOC: Missing Excel index images', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'chris-b1', 'company': None}, 'files': {'totalCount': 2}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11067', 'bodyText': \"Adds missing static files that didn't get pushed in #10967\\n@jreback\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thank you sir!'}}]}}, {'createdAt': '2015-09-11T16:38:44Z', 'updatedAt': '2015-11-15T21:09:40Z', 'title': 'BUG: matplotlib inset-axis #10407', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'd1manson', 'company': '@landtechnologies '}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11065', 'bodyText': 'This is the issue: #10407\\nI just added a bunch of try-catch AttributeError around the offending code - it seemed to do the trick for me.', 'comments': {'totalCount': 7, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this would need a test'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Thanks, this is also a side effect of #9740.\\nHowever, example in #10407 looks work on current master and matplotlib 1.4.3, maybe fixed by #10879. Can you show another example which causes a problem? Of course it is very nice to add an explicit tests for this.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@d1manson can you update according to comments'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@d1manson pls update'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'patclaffey'}, 'bodyText': \"Here is some information from pd.show_versions()\\npython: 3.4.0.final.0\\npandas: 0.17.0\\nmatplotlib: 1.4.3\\nI want to use add_axes instead of subplots.  The problem with subplots is that all subplot are the same height.  The advantage of add_axes is that subplots can be created easily with different heights.  For my use case I need  the first subplot to be smaller in height than subsequent subplots.\\nHere is a simple test case to reproduce the problem on in pandas 17.0:\\ndf_test = pd.DataFrame([1,2,3,4]) # create DataFrame\\nfig_test = plt.figure() # create the figure instance\\n#add_axes : Add an axes at position rect [left, bottom, width, height]\\n#where all quantities are in fractions of figure width and height.\\naxes_a =  fig_test.add_axes([0.1, 0.1, 0.8, 0.4])\\ndf_test.plot(ax = axes_a, title = 'Lower')\\naxes_b =  fig_test.add_axes([0.1, 0.6, 0.8, 0.2])\\ndf_test.plot(ax = axes_b, title = 'Higher')\\nThis causes error AttributeError: 'Axes' object has no attribute 'rowNum'\\nI really need this functionality - looks like I need to downgrade pandas.\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'I think this one is trying to solve the same underlying issue as what I tried to solve with #11561, which is now merged.\\n@patclaffey normally your example should work in the master version of pandas now.'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"@d1manson the approach I took in my fix was to just not call this function (_handle_shared_axes) when not having a SubplotAxes, instead of catching the AttributeError. Sorry I didn't see this before, but thanks for working on it!\"}}]}}, {'createdAt': '2015-09-11T14:59:20Z', 'updatedAt': '2017-01-30T20:55:09Z', 'title': 'Adds the option display.escape_notebook_repr_html', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'iiSeymour', 'company': 'Oxford Nanopore Technologies'}, 'files': {'totalCount': 2}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11063', 'bodyText': 'Adds the option display.escape_notebook_repr_html for controlling HTML escaping when rendering DataFrames in IPython notebooks. Unset, this argument does not change the default behaviour.\\nFixes #11062', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'open to these types of things, but:\\n\\nuse a display.html name space for these types of options. (or maybe display.notebook, but would have to rename some of the existing options possibly)\\nI think we need a more general soln.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls update'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'closing, but if you would like to continue working on this pls reopen'}}]}}, {'createdAt': '2015-09-11T09:20:20Z', 'updatedAt': '2015-09-11T11:14:09Z', 'title': 'DOC: clean up 0.17 whatsnew', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11059', 'bodyText': 'Mainly some rearranging (eg the datetime tz was somewhere in the middle of the API changes, moved it to in front of the new features)', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thxs'}}]}}, {'createdAt': '2015-09-11T05:26:28Z', 'updatedAt': '2015-09-13T20:27:43Z', 'title': 'DOC: Updated doc-string using new doc-string design for DataFrameFormatter', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 2}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11057', 'bodyText': 'DataFrameFormatter does not have this parameter: force_unicode_docstring. This change uses the doc-string re-design in #11011', 'comments': {'totalCount': 16, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'what is this supposed to address?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'Redundant parameter in docstring'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'So the original docstring for DataFrameFormatter is not correct(one or two more parameters in docstring but not in the function declaration). This PR fixed that. Can you merge?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'post the original and the new'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'common_docstring = \"\"\"\\n    Parameters\\n    ----------\\n    buf : StringIO-like, optional\\n        buffer to write to\\n    columns : sequence, optional\\n        the subset of columns to write; default None writes all columns\\n    col_space : int, optional\\n        the minimum width of each column\\n    header : bool, optional\\n        whether to print column labels, default True\\n    index : bool, optional\\n        whether to print index (row) labels, default True\\n    na_rep : string, optional\\n        string representation of NAN to use, default \\'NaN\\'\\n    formatters : list or dict of one-parameter functions, optional\\n        formatter functions to apply to columns\\' elements by position or name,\\n        default None. The result of each function must be a unicode string.\\n        List must be of length equal to the number of columns.\\n    float_format : one-parameter function, optional\\n        formatter function to apply to columns\\' elements if they are floats,\\n        default None. The result of this function must be a unicode string.\\n    sparsify : bool, optional\\n        Set to False for a DataFrame with a hierarchical index to print every\\n        multiindex key at each row, default True\\n    index_names : bool, optional\\n        Prints the names of the indexes, default True\"\"\"\\n\\njustify_docstring  = \"\"\"\\n    justify : {\\'left\\', \\'right\\'}, default None\\n        Left or right-justify the column labels. If None uses the option from\\n        the print configuration (controlled by set_option), \\'right\\' out\\n        of the box.\"\"\"\\n\\nforce_unicode_docstring = \"\"\"\\n    force_unicode : bool, default False\\n        Always return a unicode result. Deprecated in v0.10.0 as string\\n        formatting is now rendered to unicode by default.\"\"\"\\n\\nreturn_docstring = \"\"\"\\n\\n    Returns\\n    -------\\n    formatted : string (or unicode, depending on data and options)\"\"\"\\n\\nThe original is the sum of all the above\\ndocstring_to_string = common_docstring + justify_docstring + force_unicode_docstring + return_docstring\\n\\nThe new one is\\ncommon_docstring + justify_docstring + return_docstring\\n\\nor the original one excluding force_unicode_docstring'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'i am excluding it since this function/class does not have force_unicode_docstring to be passed in'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I can read the code\\nI want to see what it looks like under 0.16.2 thrn after this change is applied'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'I am not sure what you mean but just to better clarify this: This PR only excludes/removes one parameter from the docstring of DataFrameFormatter since DataFrameFormatter does not have force_unicode_docstring as a parameter.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'post what the doc string looks like before and after the change'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"The original:\\n    Parameters\\n    ----------\\n    buf : StringIO-like, optional\\n        buffer to write to\\n    columns : sequence, optional\\n        the subset of columns to write; default None writes all columns\\n    col_space : int, optional\\n        the minimum width of each column\\n    header : bool, optional\\n        whether to print column labels, default True\\n    index : bool, optional\\n        whether to print index (row) labels, default True\\n    na_rep : string, optional\\n        string representation of NAN to use, default 'NaN'\\n    formatters : list or dict of one-parameter functions, optional\\n        formatter functions to apply to columns' elements by position or name,\\n        default None. The result of each function must be a unicode string.\\n        List must be of length equal to the number of columns.\\n    float_format : one-parameter function, optional\\n        formatter function to apply to columns' elements if they are floats,\\n        default None. The result of this function must be a unicode string.\\n    sparsify : bool, optional\\n        Set to False for a DataFrame with a hierarchical index to print every\\n        multiindex key at each row, default True\\n    index_names : bool, optional\\n        Prints the names of the indexes, default True\\n    justify : {'left', 'right'}, default None\\n        Left or right-justify the column labels. If None uses the option from\\n        the print configuration (controlled by set_option), 'right' out\\n        of the box.\\n    force_unicode : bool, default False\\n        Always return a unicode result. Deprecated in v0.10.0 as string\\n        formatting is now rendered to unicode by default.\\n\\n    Returns\\n    -------\\n    formatted : string (or unicode, depending on data and options)\\n\\nThe new one:\\n    Parameters\\n    ----------\\n    buf : StringIO-like, optional\\n        buffer to write to\\n    columns : sequence, optional\\n        the subset of columns to write; default None writes all columns\\n    col_space : int, optional\\n        the minimum width of each column\\n    header : bool, optional\\n        whether to print column labels, default True\\n    index : bool, optional\\n        whether to print index (row) labels, default True\\n    na_rep : string, optional\\n        string representation of NAN to use, default 'NaN'\\n    formatters : list or dict of one-parameter functions, optional\\n        formatter functions to apply to columns' elements by position or name,\\n        default None. The result of each function must be a unicode string.\\n        List must be of length equal to the number of columns.\\n    float_format : one-parameter function, optional\\n        formatter function to apply to columns' elements if they are floats,\\n        default None. The result of this function must be a unicode string.\\n    sparsify : bool, optional\\n        Set to False for a DataFrame with a hierarchical index to print every\\n        multiindex key at each row, default True\\n    index_names : bool, optional\\n        Prints the names of the indexes, default True\\n    justify : {'left', 'right'}, default None\\n        Left or right-justify the column labels. If None uses the option from\\n        the print configuration (controlled by set_option), 'right' out\\n        of the box.\\n\\n    Returns\\n    -------\\n    formatted : string (or unicode, depending on data and options)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'hmm, I think we can take that force_unicode out entirely, it was removed in 0.14.0'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"Okay I'll take it out.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"No wonder i don't see any usage of force_unicode when PR #11011 for to_html, to_latex, etc. I'll make the changes on those doc-strings too.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'perfect ping when green'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback  On green now. Thanks.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-11T02:15:01Z', 'updatedAt': '2015-09-11T14:15:01Z', 'title': 'BUG: Fixed bug in groupby(axis=1) with filter() throws IndexError, #11041', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11055', 'bodyText': 'Fixed #11041', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'On green now. Can you merge?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via ba5106e\\nthanks\\nnote that we always to compare the results with an expected, and not just the index as this is a more general comparison.'}}]}}, {'createdAt': '2015-09-11T01:55:38Z', 'updatedAt': '2015-09-11T04:46:12Z', 'title': 'DOC: Deleted redundant/repetitive import statement', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11054', 'bodyText': '', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"Hmm, I couldn't figure out why this simple change couldn't pass Travis.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'This will be done in #11055 at the same time.'}}]}}, {'createdAt': '2015-09-10T12:59:11Z', 'updatedAt': '2015-10-17T14:46:11Z', 'title': 'Add capability to handle Path/LocalPath objects', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep', 'company': None}, 'files': {'totalCount': 7}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11051', 'bodyText': 'fixes #11033', 'comments': {'totalCount': 11, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok, looks reasonable\\nthe pathlib test should pass on 3.4 and skip on all others (pls confirm via the travis log)\\nhowever, pls add py to the requirements-2.7.txt so conda will install it (so it will test)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': 'Docs would be great as well, in the docstring and in the doc/source/io.rst.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you add pathlib to one of the py2 builds\\npls add a whatsnew note for 0.17.1'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep'}, 'bodyText': 'all done!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep'}, 'bodyText': 'anaconda can’t install pathlib. wat do?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'add to ci/requirements-2.7.pip'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls squash. otherwise lgtm.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep'}, 'bodyText': 'squashed 👍'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep'}, 'bodyText': 'done, sorry for the holdup'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 82f0033\\nthanks!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'flying-sheep'}, 'bodyText': '🎆 yeah!'}}]}}, {'createdAt': '2015-09-10T11:42:38Z', 'updatedAt': '2015-09-10T22:12:02Z', 'title': 'BUG: GH10645 and GH10692 where operation on large Index would error', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11049', 'bodyText': 'closes #10645\\ncloses #10692\\nreplaces #10675\\nDo I need @slow for these tests?', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'no need for slow these are < 1s total.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'minor correction, ping on green'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@kawochen thank you sir!'}}]}}, {'createdAt': '2015-09-10T10:52:17Z', 'updatedAt': '2015-09-10T20:48:47Z', 'title': 'DOC/CLN: typo and redundant code', 'mergedBy': {'login': 'hayd'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11048', 'bodyText': '', 'comments': {'totalCount': 2, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ping on green.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'kawochen'}, 'bodyText': '@jreback green'}}]}}, {'createdAt': '2015-09-10T08:50:49Z', 'updatedAt': '2015-09-10T10:57:48Z', 'title': 'DOC: correct quoting constants order', 'mergedBy': {'login': 'jorisvandenbossche'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11046', 'bodyText': 'Small correction, QUOTE_NONE and QUOTE_NONNUMERIC have to be switched:\\nIn [1]: import csv\\n\\nIn [2]: csv.QUOTE_NONNUMERIC\\nOut[2]: 2\\n\\nIn [3]: csv.QUOTE_NONE\\nOut[3]: 3', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm'}}]}}, {'createdAt': '2015-09-09T23:38:43Z', 'updatedAt': '2016-04-25T14:53:30Z', 'title': 'PERF: improves performance in GroupBy.cumcount', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri', 'company': None}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11039', 'bodyText': \"closes #12839\\n    -------------------------------------------------------------------------------\\n    Test name                                    | head[ms] | base[ms] |  ratio   |\\n    -------------------------------------------------------------------------------\\n    groupby_ngroups_10000_cumcount               |   3.9790 |  74.9559 |   0.0531 |\\n    groupby_ngroups_100_cumcount                 |   0.6043 |   0.9940 |   0.6079 |\\n    -------------------------------------------------------------------------------\\n    Test name                                    | head[ms] | base[ms] |  ratio   |\\n    -------------------------------------------------------------------------------\\n\\n    Ratio < 1.0 means the target commit is faster then the baseline.\\n    Seed used: 1234\\n\\n    Target [2a1c935] : PERF: improves performance in GroupBy.cumcount\\n    Base   [5b1f3b6] : reverts 'from .pandas_vb_common import *'\", 'comments': {'totalCount': 34, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'this would prob close #7569\\nand partial on #5755\\npls add tests and/or advise.\\nty'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': 'well i am not adding a new feature or closing a bug; so the tests already there should be fine'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'hayd'}, 'bodyText': 'I think this crept in via #7910 / @mortada adding support for passing lists to nth.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@behzadnouri can you do a whatsnew note'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Are we sure we want to change this? This has always been like that I think? (but indeed, inconsistent between SeriesGroupBy and DataFrameGroupBy ..)\\n@hayd this behaviour is already in 0.14.1 (while the list enhancement was only added in 0.15.0)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'I think this should be changed as @behzadnouri suggest. Code is MUCH simpler. and everything would be consistent. I think what nth does now is a little odd (and IIRC is differently if you .apply it rather than directly use .nth)'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"This is really a bug-fix, but since is a fundamental change (e.g. a position based index vs the original index) being returned it ought to be highlited to the user. This just needs a simple Previous Behavior / New Behavior section with a code-block from the previous and actual from the new.\\nIn [1]: df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\\nIn [2]: g = df.groupby('A')\\n\\nv0.16.2\\nIn [6]: g.B.nth(0)\\nOut[6]: \\n0   NaN\\n2     6\\nName: B, dtype: float64\\n\\nIn [7]: g.B.nth(1)\\nOut[7]: \\n1    4\\nName: B, dtype: float64\\n\\nThis PR\\nIn [4]: g.B.nth(0)\\nOut[4]: \\nA\\n1   NaN\\n5     6\\nName: B, dtype: float64\\n\\nIn [5]:  g.B.nth(1)\\nOut[5]: \\nA\\n1    4\\nName: B, dtype: float64\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'hayd'}, 'bodyText': 'nth was supposed to be of the \"filtering\" type rather than aggregating type. Hence the v0.16.2 behaviour.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@hayd right! I forgot about that. so what should we do with all of this, the prior code had quite a number of 'hacks' to support that exact behavior.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'moving to next version. This needs to not change the API. The result index can be set differently at the end.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': \"api is the function signature which does not change.\\nif you mean the behaviour, it is inconsistent between series and data-frame, and even within series if there are nulls; and seems to be a bug, not by design. so at some point you need to make series behave like frames or the other way around.\\n>>> df\\n   A  B\\n0  a  0\\n1  b  1\\n>>> df.groupby('A').nth(0)\\n   B\\nA   \\na  0\\nb  1\\n>>> df.groupby('A')['B'].nth(0)\\n0    0\\n1    1\\nName: B, dtype: int64\\n\\n>>> ts\\n0     0\\n1   NaN\\ndtype: float64\\n>>> ts.groupby(Series(['a', 'b'])).nth(0, dropna=True)\\na   NaN\\nb   NaN\\ndtype: float64\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'hayd'}, 'bodyText': 'Series should behave like DataFrame (and filter rather than agg). I was sure these worked the same on Series or it did behind a flag (which was not able to be the default due to old behaviour).'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"@hayd I think we are not using the same terminology :-)\\nCurrently, nth DataFrame behaviour is aggregating/reducing (so setting the grouping keys as the index), while Series behaviour is filtering (keeping the original index labels) (so the other way around than I understand from your comment)\\nOriginally nth on a DataFrame was filtering when you added it (#6569), but it was changed to reducing even before it was included in a release by @jreback  (#7044). Back then I asked specifically about this (#7044 (comment)), but @jreback gave some reasons to make it reducing. Only, in that PR, I think it was an oversight of us that it only made DataFrame reducing, and left Series behaviour as filtering.\\nAlso in the docs, nth is mentioned in the reducing methods explicitely: the 'note box at the end of this section: http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation\\nBy the introduction of the possibility to pass lists to get multiple values, it complicated this a bit, as a real aggregating method will only return one value per group. Further, with the introduction of dropna kwarg, you cannot really call it filtering anymore when using that, since it will not return original rows always.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'IIRC (and has been a while), we want to make nth(0,dropna=True) == .first() and nth(0,dropna=True) == .last()'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': '@jreback and that is true for a DataFrame, but not Series (so this PR makes that analogy more consistent)'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'Now, for a solution. I think we just have to make a choice ..\\n\\nLeave it as is (DataFrame as reducing, Series as filtering, it has been like this since 0.14)\\nHave all as reducing (so change Series)\\nHave all as filtering (and change DataFrame behaviour)\\n\\nIf we change something, I would go for 2), as the filtering behaviour is easy to get by using as_index=False (while for the other way around you have to do set_index()) + then it is more consistent with first/last'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ahh, ok this was a consistency-fix partially then. I would agree with 2).'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': \"Some other inconsistencies:\\n\\n\\nas_index=False is not working when using dropna:\\nIn [38]: df = pd.DataFrame({'a':list('ABABAB'), 'b':[1,2,3,4,5,6]})\\n\\nIn [39]: df2 = df.copy()\\n\\nIn [40]: df2.iloc[1,1] = np.nan\\n\\nIn [41]: df2.groupby('a').nth(0, dropna='any')\\nOut[41]:\\nb\\na\\nA  1\\nB  4\\n\\nIn [42]: df2.groupby('a', as_index=False).nth(0, dropna='any')\\nOut[42]:\\nb\\na\\nA  1\\nB  4\\n\\nIn [43]: df2.groupby('a', as_index=False).nth(0)\\nOut[43]:\\na   b\\n0  A   1\\n1  B NaN\\n\\n\\n\\nthe resulting index when using as_index=False is different between nth and first:\\nIn [45]: df.index = df.index + 10\\n\\nIn [46]: df.groupby('a').nth(0)\\nOut[46]:\\nb\\na\\nA  1\\nB  2\\n\\nIn [47]: df.groupby('a').first()\\nOut[47]:\\nb\\na\\nA  1\\nB  2\\n\\nIn [48]: df.groupby('a', as_index=False).first()\\nOut[48]:\\na  b\\n0  A  1\\n1  B  2\\n\\nIn [50]: df.groupby('a', as_index=False).nth(0)\\nOut[50]:\\na  b\\n10  A  1\\n11  B  2\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'these are already noted in #5755 (though not so explicity)'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'jorisvandenbossche'}, 'bodyText': 'But here it is possibly on purpose, as for nth this is a way to get filtering behaviour (keep original index), while reducing methods give you just a default 0, 1, .., n index when using as_index=False as you never can have the original index. That is the problem with nth being some mixture of a reducing and filtering method.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@behzadnouri can you rebase / update\\nfurther need to clarify any API changes / fixes here'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'hayd'}, 'bodyText': \"@jorisvandenbossche I don't understand why the same argument couldn't apply to .head, isn't .nth is the same in whether it's filtering/reducing. :( Or is this claim due to dropping NaN, the really slow path inside nth.\\nIME the nth, index behaviour is rarely what you want, you just want the nth line regardless of NaN. Maybe we should have an iloc method/accessor on groupby that would be more explicit (and always fast) and do just that.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"@hayd if you wouldn't mind putting up some examples of what you think we should be doing for the various filter/reducers: .head/nth/first/iloc (and I agree .iloc would be a nice clear accessor, though obviously wouldn't skip nan, we did in fact had`.irow``, but deprecated recently)\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'hayd'}, 'bodyText': 'yeah, iirc it was deprecated cos it was slow... it seems that the nth path (which skips nans) is similarly slow. Do people use this feature/is it useful? I guess if we had iloc, as the fast path, we (I) can just ignore nth as an R-throwback. :p'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok, so who wants to update, propose something here 2) seemed to be the most useful way to go, IOW, fix Series to be reducing (as DataFrame is now), and easy to get filtering with as_index=False\\ncc @behzadnouri\\n@hayd\\n@jorisvandenbossche'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'can you rebase/update'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': '@jreback rebased'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@behzadnouri ok thanks. Can you put a whatsnew sub-section that shows the changes as a user would care about them, IOW an example (you can take from tests) showing what used to happend and the (correct) new way.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': '@jreback added an example to whatsnew showing the changes'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': '@jreback moved to api changes section'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri'}, 'bodyText': '@jreback made the changes'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@jorisvandenbossche @TomAugspurger comments?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"Looks good on a quick skim.\\n@behzadnouri you seem to also fixed a bug where groupby.nth was ignoring the sort keyword\\n# master\\nIn [5]: df.groupby('c', sort=True).nth(1)\\nOut[5]:\\n          a         b\\nc\\n0 -0.029029  0.565333\\n1  0.186213  1.110464\\n2  0.982333 -0.544459\\n3 -0.626740 -0.541241\\n\\nIn [6]: df.groupby('c', sort=False).nth(1)\\nOut[6]:\\n          a         b\\nc\\n0 -0.029029  0.565333\\n1  0.186213  1.110464\\n2  0.982333 -0.544459\\n3 -0.626740 -0.541241\\nvs.\\n# your branch\\nIn [1]: df = pd.DataFrame(np.random.randn(100, 2), columns=['a', 'b'])\\n\\nIn [2]: df['c'] = np.random.randint(0, 4, 100)\\n\\nIn [3]: df.groupby('c', sort=True).nth(1)\\nOut[3]:\\n          a         b\\nc\\n0 -1.168300 -2.224763\\n1 -0.562298 -1.262734\\n2 -0.439613  0.236592\\n3 -0.499235 -0.808404\\n\\nIn [4]: df.groupby('c', sort=False).nth(1)\\nOut[4]:\\n          a         b\\nc\\n1 -0.562298 -1.262734\\n2 -0.439613  0.236592\\n3 -0.499235 -0.808404\\n0 -1.168300 -2.224763\\nWould be nice to have  a release note for that to (could maybe do on merge?).\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks @behzadnouri\\n@TomAugspurger your suggestions incorporated as well!'}}]}}, {'createdAt': '2015-09-09T20:14:32Z', 'updatedAt': '2015-09-09T21:37:01Z', 'title': 'DOC: Fix misspelling of max_colwidth in documentation.', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'gaulinmp', 'company': 'University of Utah'}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11037', 'bodyText': 'One location referencing display.max_colwidth was incorrectly spelled as display.max_columnwidth.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-09T17:05:23Z', 'updatedAt': '2015-09-09T21:09:17Z', 'title': 'BUG: Fix Timestamp __ne__ comparison issue', 'mergedBy': None, 'authorAssociation': 'MEMBER', 'author': {'login': 'cpcloud', 'company': '@twosigma'}, 'files': {'totalCount': 2}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11036', 'bodyText': 'closes #11034', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via ccaf040\\n@cpcloud I add some more tests for completeness.\\nty'}}]}}, {'createdAt': '2015-09-09T01:51:42Z', 'updatedAt': '2015-09-09T12:40:45Z', 'title': \"BUG: Fixed bug in len(DataFrame.groupby) causing IndexError when there's a NaN-only column (issue11016)\", 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11031', 'bodyText': 'This is the new PR for Fixed issue #11016', 'comments': {'totalCount': 6, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"generally we don't make new PR's. just push -f the old ones. ok for now. but pls have a look at:\\nhttp://pandas.pydata.org/pandas-docs/stable/contributing.html\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback Got it.  I will take a closer look at it.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': 'So when I run nosetests pandas/tests/test_groupby.py, it gave me a lot of errors. This is on master branch from recent pydata/pandas. I think the errors are from recent commits rather than this PR.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'you need to rebuild the c extensions\\nafter u pull from master do this'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 066783c\\nthanks'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'marcelm'}, 'bodyText': 'Thanks for fixing!'}}]}}, {'createdAt': '2015-09-09T01:18:09Z', 'updatedAt': '2015-09-09T01:52:44Z', 'title': \"BUG: Fixed bug in len(DataFrame.groupby) causing IndexError when there's a NaN-only column (issue11016)\", 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan', 'company': 'Ant Financial'}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11030', 'bodyText': 'Fixed #11016', 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': '@jreback Could you help me with this conflict?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"don't put the whatsnew note at the very end of bug fixes. stick it where their is blank space. at the end always causes conflicts, much much more rare when its not.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'terrytangyuan'}, 'bodyText': \"Thanks. I had trouble rebasing it. It has conflict that I failed to fix. I've created a new PR. Sorry for inconvenience.\"}}]}}, {'createdAt': '2015-09-08T20:34:19Z', 'updatedAt': '2015-10-27T20:25:50Z', 'title': 'Fix for DataFrame.hist() with by- and weights-keyword', 'mergedBy': None, 'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle', 'company': None}, 'files': {'totalCount': 1}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11028', 'bodyText': \"closes #9540\\nfor example:\\nimport pandas as pd\\nd = {'one' : ['A', 'A', 'B', 'C'],\\n     'two' : [4., 3., 2., 1.],\\n     'three' : [10., 8., 5., 7.]}     \\ndf = pd.DataFrame(d)\\ndf.hist('two', by='one', weights='three', bins=range(0, 10))\\ndoes not seem to break anything, but this is my first meddling in the pandas library, so a review would be nice\", 'comments': {'totalCount': 9, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"Just gave a quick look through here. This is a good idea that we should support.\\nWe'll need tests for this. Put them in pandas/tests/test_graphics.py. Make sure to cover cases where\\n\\nby is None or a column\\nweights is an array of weights or a string column\\nvalues has missing data and / or weights has missing data\\n\\nThe plotting stuff is being refactored a bunch currently, so I've tagged this for the 0.18. You might want to hold off on making more changes, but in the meantime you can write tests for this.\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'FYI the checking is quite similar to how weights are checked for DataFrame.sample, so would want to make this a common function (could be a private function on a dataframe)'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle'}, 'bodyText': 'Thanks jreback, I will try my best to add some tests. I will also check what can be done with synching dropna with the weighs.\\nI was thinking in the lines of\\ndrow rows in weighs, that are na in group\\ndrop rows na in group\\nfillna in weighs with zero, so they do not count to anything\\nwould this be ok?'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle'}, 'bodyText': 'Or more logical to drop all rows that are NA in group or in weights?'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': \"I'd say your lest method. Something like data.dropna(subset=['group', 'weight'])\"}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@TomAugspurger can you review'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle'}, 'bodyText': 'Sorry I have not had time to continue with the tests, my limited git knowledge made it tough interacting with the repo. I planned to look how my patch worked with the new release'}}, {'node': {'authorAssociation': 'NONE', 'author': {'login': 'Twizzledrizzle'}, 'bodyText': 'Added a new pull request here: #11441\\nfor the new version, sorry for not using git correctly :('}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'TomAugspurger'}, 'bodyText': 'Superseded by #11441'}}]}}, {'createdAt': '2015-09-08T17:22:31Z', 'updatedAt': '2015-09-08T17:28:01Z', 'title': 'Fix common typo in documentation', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'swails', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11027', 'bodyText': 'documenation -> documentation', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-08T13:51:54Z', 'updatedAt': '2015-09-08T20:35:25Z', 'title': 'DOC: Add GroupBy.count', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 1}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11025', 'bodyText': 'Related to #11013. Define GroupBy.count for API doc and added docstring.', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks!'}}]}}, {'createdAt': '2015-09-08T13:07:49Z', 'updatedAt': '2015-09-09T08:47:50Z', 'title': 'BUG: DatetimeIndex.freq can defer by ufunc', 'mergedBy': {'login': 'sinhrks'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 2}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11024', 'bodyText': \"Follow up of #10638. DatetimeIndex.freq also can be changed when other arg is an ndarray.\\nAs the same as #10638, freq is re-infered only when it exists. Otherwise, leave it as None even if the result can be on specific freq. This is based on other functions behavior as below:\\nidx = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'])\\nidx\\n# DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)\\nidx.take([0, 2])\\n# DatetimeIndex(['2011-01-01', '2011-01-03'], dtype='datetime64[ns]', freq=None)\\n\\nidx = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], freq='D')\\nidx.take([0, 2])\\n# DatetimeIndex(['2011-01-01', '2011-01-03'], dtype='datetime64[ns]', freq='2D')\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'lgtm. merge on green.'}}]}}, {'createdAt': '2015-09-07T22:43:56Z', 'updatedAt': '2015-09-08T00:30:05Z', 'title': 'PERF: use NaT comparisons in int64/datetimelikes #11010', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback', 'company': None}, 'files': {'totalCount': 4}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11023', 'bodyText': \"closes #11010\\nIn [10]: %timeit gr['3rd'].count()\\n100 loops, best of 3: 15.1 ms per loop\\n\\nIn [11]: %timeit gr['3rd'].min()\\n100 loops, best of 3: 8.4 ms per loop\\n\\nIn [12]: %timeit gr['3rd'].max()\\n100 loops, best of 3: 8.47 ms per loop\\n\\nIn [13]: %timeit gr['3rd'].first()\\n100 loops, best of 3: 10.5 ms per loop\\n\\nIn [14]: %timeit gr['3rd'].last()\\n100 loops, best of 3: 10.3 ms per loop\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'cc @larvian\\ncc @behzadnouri'}}]}}, {'createdAt': '2015-09-07T18:33:05Z', 'updatedAt': '2015-09-19T00:38:09Z', 'title': 'BUG: Exception when setting a major- or minor-axis slice of a Panel with RHS a DataFrame', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'evanpw', 'company': None}, 'files': {'totalCount': 3}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11021', 'bodyText': \"Fixes GH #11014\\nThis isn't actually a recent regression. The bug was introduced in 0.15.0 with this commit: 30246a7; it was just made more common (i.e., it happens on panels without mixed dtype) in 0.16.2 by this commit: 5c39467.\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@evanpw gr8 thanks!'}}]}}, {'createdAt': '2015-09-07T15:44:46Z', 'updatedAt': '2015-09-07T20:23:10Z', 'title': 'BUG: Unable to infer negative freq', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 8}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11018', 'bodyText': \"Fixed 2 issues:\\n\\ndate_range can't handle negative calendar-based freq (like A, Q and M).\\n\\nimport pandas as pd\\n\\n# OK\\npd.date_range('2011-01-01', freq='-1D', periods=3)\\n# DatetimeIndex(['2011-01-01', '2010-12-31', '2010-12-30'], dtype='datetime64[ns]', freq='-1D')\\n\\n# NG\\npd.date_range('2011-01-01', freq='-1M', periods=3)\\n# DatetimeIndex([], dtype='datetime64[ns]', freq='-1M')\\n\\n\\nUnable to infer negative freq.\\n\\npd.DatetimeIndex(['2011-01-05', '2011-01-03', '2011-01-01'], freq='infer')\\n# DatetimeIndex(['2011-01-05', '2011-01-03', '2011-01-01'], dtype='datetime64[ns]', freq=None)\\n\\npd.TimedeltaIndex(['-1 days', '-3 days', '-5 days'], freq='infer')\\n# TimedeltaIndex(['-1 days', '-3 days', '-5 days'], dtype='timedelta64[ns]', freq=None)\", 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'thanks @sinhrks'}}]}}, {'createdAt': '2015-09-07T14:46:47Z', 'updatedAt': '2015-09-11T14:36:06Z', 'title': 'BUG: Index dtype may not be applied properly', 'mergedBy': None, 'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks', 'company': None}, 'files': {'totalCount': 3}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11017', 'bodyText': 'Fixed 2 problems:\\n\\nSpecified dtype is not applied to other iterables.\\n\\nimport numpy as np\\nimport pandas as pd\\n\\npd.Index([1, 2, 3], dtype=int)\\n# Index([1, 2, 3], dtype=\\'object\\')\\n\\n\\nSpecifying category to ndarray-like results in TypeError\\n\\npd.Index(np.array([1, 2, 3]), dtype=\\'category\\')\\n# TypeError: data type \"category\" not understood', 'comments': {'totalCount': 10, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls run perf check on this -'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks can you see if this affects perf?'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Following is a asv result. Will consider a better path.\\n    before     after       ratio\\n  [789e07d9] [ce89f9d1]\\n    33.94μs    44.57μs      1.31  ctors.index_from_series_ctor.time_index_from_series_ctor\\n   121.86μs   173.21μs      1.42  frame_methods.frame_get_dtype_counts.time_frame_get_dtype_co\\n    16.45μs    27.07μs      1.65  index_object.index_float64_construct.time_index_float64_construct'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"this is prob just from the imports (e.g. a Float64Index) doesn't care about Datetimeindex, so the import adds the extra time (or the check for the import anyhow)\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': '@jreback Thanks, suggested changes improve perf a little. I assume other slowness is caused by categorical condition moved to the top.\\nAll benchmarks:\\n\\n    before     after       ratio\\n  [4d4a2e33] [c3eaeb07]\\n    33.85μs    41.24μs      1.22  ctors.index_from_series_ctor.time_index_from_series_ctor\\n   129.48μs   160.86μs      1.24  frame_methods.frame_get_dtype_counts.time_frame_get_dtype_counts\\n    16.36μs    22.88μs      1.40  index_object.index_float64_construct.time_index_float64_construct'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'master (this is index_object.index_float64_construct.time_index_float64_construct) benchmark\\nIn [1]: arr = np.arange(1000000.0)\\nIn [2]: %timeit Index(arr)\\nThe slowest run took 12.23 times longer than the fastest. This could mean that an intermediate result is being cached \\n100000 loops, best of 3: 15.7 Âµs per loop\\n\\nthis branch\\nIn [2]: %timeit Index(arr)\\nThe slowest run took 7.49 times longer than the fastest. This could mean that an intermediate result is being cached \\n10000 loops, best of 3: 19.8 Âµs per loop\\n\\nfix jreback@29c0325\\nIn [2]: %timeit Index(arr)\\nThe slowest run took 11.74 times longer than the fastest. This could mean that an intermediate result is being cached \\n100000 loops, best of 3: 13.8 Âµs per loop\\n\\nwill merge in a bit, thanks @sinhrks'}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Wow, great. Thanks!'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': \"merged via ead3ca8 (my change in another commit)\\nthanks!\\nI don't believe we had an issue assosicated, correct?\"}}, {'node': {'authorAssociation': 'MEMBER', 'author': {'login': 'sinhrks'}, 'bodyText': 'Though this was based on gitter chat, #5196 refers to the same issue. Closed.'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': '@sinhrks awesome thanks!'}}]}}, {'createdAt': '2015-09-07T04:21:55Z', 'updatedAt': '2015-12-08T03:52:34Z', 'title': 'BUG: .ix with PeriodIndex is fixed', 'mergedBy': None, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty', 'company': 'Sixty Capital'}, 'files': {'totalCount': 2}, 'state': 'CLOSED', 'resourcePath': '/pandas-dev/pandas/pull/11015', 'bodyText': \"Solves #4125\\nIt doesn't feel like an elegant solution, but I think it works.\\nI think when Period gets its own dtype, these will all go away?\", 'comments': {'totalCount': 3, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'pls add a whatsnew note'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'merged via 879f5e9\\nthanks!\\n(I changed this slightly in another commit). is_array_period_like does inference on the object and thus could be slow potentially'}}, {'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'max-sixty'}, 'bodyText': 'Cheers @jreback!'}}]}}, {'createdAt': '2015-09-05T22:48:36Z', 'updatedAt': '2015-09-08T01:37:28Z', 'title': 'CLN: removes cython implementation of groupby count', 'mergedBy': {'login': 'jreback'}, 'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'behzadnouri', 'company': None}, 'files': {'totalCount': 7}, 'state': 'MERGED', 'resourcePath': '/pandas-dev/pandas/pull/11013', 'bodyText': '', 'comments': {'totalCount': 1, 'edges': [{'node': {'authorAssociation': 'CONTRIBUTOR', 'author': {'login': 'jreback'}, 'bodyText': 'ok thanks'}}]}}]}}}\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Query limit remaining:  4952\n",
      "\n",
      "{\n",
      "  search(first:100, query:\"repo:pandas-dev/pandas created:2015-09-30..2015-10-31 type:pr\", type:ISSUE) {\n",
      "    nodes {\n",
      "      ... on PullRequest {\n",
      "        createdAt\n",
      "        updatedAt\n",
      "        title\n",
      "        mergedBy {\n",
      "          login\n",
      "        }\n",
      "        authorAssociation\n",
      "        author {\n",
      "          login\n",
      "          ... on User {\n",
      "            company\n",
      "          }\n",
      "        }\n",
      "        files {\n",
      "          totalCount\n",
      "        }\n",
      "        state\n",
      "        resourcePath\n",
      "        bodyText\n",
      "        comments(first: 50) {\n",
      "          totalCount\n",
      "          edges {\n",
      "            node {\n",
      "              authorAssociation\n",
      "              author{\n",
      "                login\n",
      "              }\n",
      "              bodyText\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "{'data': None, 'errors': [{'message': 'Something went wrong while executing your query. This may be the result of a timeout, or it could be a GitHub bug. Please include `F1E4:15AE:1DE1C5:2D5CF8:5D66EF7F` when reporting this issue.'}]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-53fa66277885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauto_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas-dev\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-ba01f44c0d64>\u001b[0m in \u001b[0;36mauto_query\u001b[0;34m(repository_owner, repository_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mquery_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_response\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# DEBUG STATEMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mpr_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetter_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Check query limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-80b88228182c>\u001b[0m in \u001b[0;36mbetter_df\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mtake\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mraw\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mturn\u001b[0m \u001b[0mit\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpretty\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "auto_query(\"pandas-dev\", \"pandas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
